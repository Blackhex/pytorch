<testsuites>
  <testsuite name="pytest" errors="0" failures="25" skipped="3308" tests="5030" time="623.213" timestamp="2023-03-27T12:56:10.014306" hostname="BartonTest">
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_H_cpu_complex128" time="0.054" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_H_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_T_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_T_cpu_float64" time="0.070" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad___getitem___cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad___getitem___cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad___radd___cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad___radd___cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad___rdiv___cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad___rdiv___cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad___rmatmul___cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad___rmatmul___cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad___rmod___cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad___rmul___cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad___rmul___cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad___rpow___cpu_complex128" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad___rpow___cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad___rsub___cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad___rsub___cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad__native_batch_norm_legit_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad__segment_reduce_lengths_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad__segment_reduce_offsets_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad__softmax_backward_data_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad__upsample_bilinear2d_aa_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_abs_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_abs_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_acos_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_acos_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_acosh_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_acosh_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_add_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_add_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_addbmm_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_addbmm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_addcdiv_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_addcdiv_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_addcmul_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_addcmul_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_addmm_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_addmm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_addmm_decomposed_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_addmm_decomposed_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_addmv_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_addmv_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_addr_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_addr_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_all_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_all_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_allclose_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_allclose_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_amax_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_amin_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_aminmax_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_angle_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_angle_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_any_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_any_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_arange_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_argmax_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_argmin_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_argsort_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_argwhere_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_argwhere_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_as_strided_cpu_complex128" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Numerous errors">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Numerous errors</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_as_strided_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Numerous errors">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Numerous errors</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_as_strided_partial_views_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_as_strided_partial_views_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_as_strided_scatter_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_as_strided_scatter_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_asin_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_asin_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_asinh_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_asinh_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_atan2_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_atan_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_atan_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_atanh_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_atanh_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_atleast_1d_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_atleast_1d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_atleast_2d_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_atleast_2d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_atleast_3d_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_atleast_3d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_baddbmm_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_baddbmm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_bernoulli_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_bfloat16_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_bfloat16_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_block_diag_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_block_diag_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_bmm_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_bmm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_bool_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_bool_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_broadcast_tensors_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_broadcast_tensors_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_broadcast_to_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_broadcast_to_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_bucketize_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_byte_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_byte_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_cartesian_prod_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_cartesian_prod_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_cat_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="TODO(whc) fix pre-existing bug with cat for newly added opinfo for empty+nonempty">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: TODO(whc) fix pre-existing bug with cat for newly added opinfo for empty+nonempty</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_cat_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="TODO(whc) fix pre-existing bug with cat for newly added opinfo for empty+nonempty">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: TODO(whc) fix pre-existing bug with cat for newly added opinfo for empty+nonempty</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_cauchy_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_cdist_cpu_float64" time="0.323" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_cdouble_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_cdouble_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_ceil_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_cfloat_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_cfloat_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_chalf_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_chalf_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_char_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_char_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_cholesky_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_cholesky_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_cholesky_inverse_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_cholesky_inverse_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_cholesky_solve_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_cholesky_solve_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_chunk_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_chunk_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_clamp_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_clamp_max_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_clamp_min_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_clone_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_clone_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_column_stack_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_column_stack_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_combinations_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_combinations_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_complex_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_conj_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_conj_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_conj_physical_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_conj_physical_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_constant_pad_nd_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_constant_pad_nd_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_contiguous_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_contiguous_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_copysign_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_corrcoef_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_corrcoef_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_cos_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_cos_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_cosh_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_cosh_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_count_nonzero_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_count_nonzero_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_cov_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_cov_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_cross_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_cross_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_cummax_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_cummin_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_cumprod_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_cumprod_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_cumsum_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_cumsum_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_cumulative_trapezoid_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_cumulative_trapezoid_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_deg2rad_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_diag_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_diag_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_diag_embed_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_diag_embed_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_diagflat_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_diagflat_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_diagonal_copy_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_diagonal_copy_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_diagonal_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_diagonal_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_diagonal_scatter_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_diagonal_scatter_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_diff_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_diff_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_digamma_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_dist_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_dist_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_div_floor_rounding_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_div_no_rounding_mode_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_div_no_rounding_mode_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_div_trunc_rounding_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_dot_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_dot_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_double_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_double_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_dsplit_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_dsplit_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_dstack_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_dstack_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_einsum_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_einsum_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_empty_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_empty_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_empty_like_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_empty_like_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_empty_permuted_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_empty_permuted_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_eq_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_eq_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_equal_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_equal_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_erf_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_erfc_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_erfinv_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_exp2_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_exp2_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_exp_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_exp_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_expand_as_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_expand_as_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_expand_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_expand_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_expm1_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_exponential_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_eye_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_eye_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fft_fft2_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fft_fft2_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fft_fft_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fft_fft_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fft_fftn_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fft_fftn_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fft_fftshift_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fft_fftshift_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fft_hfft2_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fft_hfft2_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fft_hfft_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fft_hfft_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fft_hfftn_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fft_hfftn_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fft_ifft2_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fft_ifft2_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fft_ifft_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fft_ifft_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fft_ifftn_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fft_ifftn_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fft_ifftshift_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fft_ifftshift_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fft_ihfft2_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fft_ihfft_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fft_ihfftn_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fft_irfft2_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fft_irfft2_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fft_irfft_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fft_irfft_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fft_irfftn_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fft_irfftn_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fft_rfft2_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fft_rfft_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fft_rfftn_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fill_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fill_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_flatten_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_flatten_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_flip_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_flip_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fliplr_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fliplr_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_flipud_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_flipud_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_float_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_float_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_float_power_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_float_power_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_floor_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_floor_divide_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fmax_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fmin_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_fmod_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_frac_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_frexp_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_full_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_full_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_full_like_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_full_like_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_gather_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_gather_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_ge_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_geometric_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_geqrf_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_geqrf_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_gradient_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_gradient_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_grid_sampler_2d_cpu_float64" time="0.004" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_gt_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_half_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_half_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_heaviside_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_histc_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_histogram_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_histogramdd_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_hsplit_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_hsplit_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_hstack_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_hstack_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_hypot_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_i0_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_igamma_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_igammac_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_imag_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_index_add_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_index_add_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_index_copy_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_index_copy_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_index_fill_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_index_fill_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_index_put_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_index_put_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_index_reduce_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_index_select_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_index_select_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_inner_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_inner_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_int_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_int_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_isclose_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_isclose_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_isfinite_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_isfinite_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_isin_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_isinf_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_isinf_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_isnan_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_isnan_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_isneginf_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_isposinf_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_isreal_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_isreal_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_istft_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_jiterator_2inputs_2outputs_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_jiterator_2inputs_2outputs_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_jiterator_4inputs_with_extra_args_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_jiterator_4inputs_with_extra_args_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_jiterator_binary_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_jiterator_binary_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_jiterator_binary_return_by_ref_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_jiterator_binary_return_by_ref_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_jiterator_unary_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_jiterator_unary_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_kron_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_kron_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_kthvalue_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_ldexp_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_ldexp_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_le_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_lerp_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_lerp_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_lgamma_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_cholesky_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_cholesky_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_cholesky_ex_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_cholesky_ex_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_cond_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_cond_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_cross_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_cross_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_det_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_det_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_det_singular_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_det_singular_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_diagonal_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_diagonal_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_eig_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_eig_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_eigh_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_eigh_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_eigvals_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_eigvals_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_eigvalsh_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_eigvalsh_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_householder_product_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_householder_product_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_inv_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_inv_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_inv_ex_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_inv_ex_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_ldl_factor_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_ldl_factor_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_ldl_factor_ex_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_ldl_factor_ex_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_ldl_solve_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_ldl_solve_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_lstsq_cpu_complex128" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_lstsq_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_lstsq_grad_oriented_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_lstsq_grad_oriented_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_lu_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_lu_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_lu_factor_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_lu_factor_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_lu_factor_ex_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_lu_factor_ex_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_lu_solve_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_lu_solve_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_matrix_norm_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_matrix_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_matrix_power_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_matrix_power_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_matrix_rank_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_matrix_rank_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_matrix_rank_hermitian_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_matrix_rank_hermitian_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_multi_dot_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_multi_dot_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_norm_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_norm_subgradients_at_zero_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_norm_subgradients_at_zero_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_pinv_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_pinv_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_pinv_hermitian_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_pinv_hermitian_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_pinv_singular_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_pinv_singular_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_qr_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_qr_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_slogdet_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_slogdet_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_solve_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_solve_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_solve_ex_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_solve_ex_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_solve_triangular_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_solve_triangular_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_svd_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_svd_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_svdvals_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_svdvals_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_tensorinv_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_tensorinv_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_tensorsolve_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_tensorsolve_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_vander_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_vander_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_vecdot_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_vecdot_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_vector_norm_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linalg_vector_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linspace_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_linspace_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_log10_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_log10_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_log1p_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_log1p_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_log2_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_log2_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_log_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_log_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_log_normal_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_log_softmax_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_log_softmax_with_dtype_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_log_softmax_with_dtype_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_logaddexp2_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_logaddexp_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_logaddexp_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_logcumsumexp_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_logcumsumexp_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_logdet_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_logdet_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_logical_and_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_logical_and_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_logical_not_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_logical_not_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_logical_or_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_logical_or_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_logical_xor_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_logical_xor_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_logit_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_logspace_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_logspace_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_logsumexp_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_long_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_long_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_lt_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_lu_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_lu_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_lu_solve_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_lu_solve_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_lu_unpack_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_lu_unpack_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_mH_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_mH_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_mT_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_mT_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_masked_amax_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_masked_amin_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_masked_argmax_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_masked_argmin_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_masked_cumprod_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_masked_cumprod_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_masked_cumsum_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_masked_cumsum_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_masked_fill_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_masked_fill_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_masked_log_softmax_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_masked_logaddexp_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_masked_logsumexp_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_masked_mean_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_masked_mean_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_masked_median_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_masked_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_masked_normalize_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_masked_normalize_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_masked_prod_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_masked_prod_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_masked_scatter_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_masked_scatter_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_masked_select_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_masked_select_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_masked_softmax_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_masked_softmin_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_masked_std_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_masked_std_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_masked_sum_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_masked_sum_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_masked_var_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_masked_var_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_matmul_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_matmul_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_matrix_exp_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_matrix_exp_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_max_binary_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_max_pool2d_with_indices_backward_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_max_reduction_no_dim_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_max_reduction_with_dim_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_maximum_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_mean_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_mean_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_median_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_meshgrid_list_of_tensors_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_meshgrid_list_of_tensors_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_meshgrid_variadic_tensors_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_meshgrid_variadic_tensors_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_min_binary_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_min_reduction_no_dim_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_min_reduction_with_dim_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_minimum_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_mm_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_mm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_mode_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_movedim_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_movedim_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_msort_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_mul_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_mul_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_multinomial_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_mv_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_mv_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_mvlgamma_mvlgamma_p_1_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_mvlgamma_mvlgamma_p_3_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_mvlgamma_mvlgamma_p_5_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nan_to_num_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nanmean_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nanmedian_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nanquantile_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nansum_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_narrow_copy_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_narrow_copy_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_narrow_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_narrow_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_native_batch_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_native_dropout_backward_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_native_layer_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_ne_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_ne_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_neg_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_neg_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_new_empty_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_new_empty_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_new_empty_strided_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_new_empty_strided_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_new_full_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_new_full_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_new_ones_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_new_ones_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_new_zeros_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_new_zeros_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nextafter_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_adaptive_avg_pool1d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_adaptive_avg_pool2d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_adaptive_avg_pool3d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_adaptive_max_pool1d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_adaptive_max_pool2d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_adaptive_max_pool3d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_alpha_dropout_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_avg_pool1d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_avg_pool2d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_avg_pool3d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_batch_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_bilinear_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_binary_cross_entropy_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_binary_cross_entropy_with_logits_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_celu_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_conv1d_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_conv1d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_conv2d_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_conv2d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_conv_transpose1d_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_conv_transpose1d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_conv_transpose2d_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_conv_transpose2d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_conv_transpose3d_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_conv_transpose3d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_cosine_embedding_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_cosine_similarity_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_cross_entropy_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_ctc_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_dropout2d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_dropout3d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_dropout_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_elu_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_embedding_bag_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_embedding_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_feature_alpha_dropout_with_train_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_feature_alpha_dropout_without_train_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_feature_alpha_dropout_without_train_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_fractional_max_pool2d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_fractional_max_pool3d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_gaussian_nll_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_gelu_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_glu_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_grid_sample_cpu_float64" time="0.004" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_group_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_hardshrink_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_hardsigmoid_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_hardswish_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_hardtanh_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_hinge_embedding_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_huber_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_instance_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_interpolate_area_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_interpolate_bicubic_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_interpolate_bilinear_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_interpolate_linear_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_interpolate_nearest_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_interpolate_trilinear_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_kl_div_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_l1_loss_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_l1_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_layer_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_leaky_relu_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_linear_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_linear_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_local_response_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_logsigmoid_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_margin_ranking_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_max_pool1d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_max_pool2d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_max_pool3d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_max_unpool1d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_max_unpool1d_grad_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_max_unpool2d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_max_unpool2d_grad_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_max_unpool3d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_max_unpool3d_grad_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_mish_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_mse_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_multi_margin_loss_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_multilabel_margin_loss_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_multilabel_soft_margin_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_nll_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_normalize_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_normalize_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_pad_circular_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_pad_circular_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_pad_constant_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_pad_constant_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_pad_reflect_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_pad_reflect_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_pad_replicate_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_pad_replicate_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_pairwise_distance_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_pairwise_distance_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_pdist_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_pixel_shuffle_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_pixel_shuffle_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_pixel_unshuffle_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_pixel_unshuffle_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_poisson_nll_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_prelu_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_relu6_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_relu_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_rrelu_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_scaled_dot_product_attention_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_selu_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_silu_complex_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_silu_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_smooth_l1_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_soft_margin_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_softmin_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_softmin_with_dtype_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_softmin_with_dtype_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_softplus_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_softshrink_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_softsign_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_softsign_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_tanhshrink_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_tanhshrink_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_threshold_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_triplet_margin_loss_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_triplet_margin_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_triplet_margin_with_distance_loss_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_triplet_margin_with_distance_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_unfold_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_unfold_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_upsample_bilinear_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nn_functional_upsample_nearest_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nonzero_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_nonzero_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_norm_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_norm_fro_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_norm_fro_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_norm_inf_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_norm_inf_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_norm_nuc_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_norm_nuc_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_normal_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Gradients are incorrect!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Gradients are incorrect!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_normal_in_place_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_normal_in_place_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_normal_number_mean_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Gradients are incorrect!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Gradients are incorrect!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_ones_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_ones_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_ones_like_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_ones_like_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_ormqr_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_ormqr_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_outer_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_outer_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_pca_lowrank_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_permute_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_permute_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_pinverse_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_pinverse_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_polar_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_polygamma_polygamma_n_0_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_polygamma_polygamma_n_1_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_polygamma_polygamma_n_2_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_polygamma_polygamma_n_3_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_polygamma_polygamma_n_4_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_positive_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_positive_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_pow_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_pow_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_prod_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_prod_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_put_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_put_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_qr_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_qr_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_quantile_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_rad2deg_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_rand_like_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_rand_like_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_randint_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_randint_like_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_randn_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_randn_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_randn_like_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_randn_like_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_ravel_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_ravel_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_real_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_real_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_reciprocal_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_reciprocal_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_remainder_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_renorm_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_renorm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_repeat_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_repeat_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_repeat_interleave_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_repeat_interleave_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_reshape_as_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_reshape_as_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_reshape_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_reshape_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_resize__cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_resize__cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_resize_as__cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_resize_as__cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_resolve_conj_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_resolve_conj_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_resolve_neg_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_resolve_neg_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_roll_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_roll_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_rot90_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_rot90_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_round_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_round_decimals_0_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_round_decimals_3_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_round_decimals_neg_3_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_rsqrt_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_rsqrt_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_rsub_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_rsub_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_scalar_tensor_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_scalar_tensor_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_scatter_add_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_scatter_add_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_scatter_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_scatter_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_scatter_reduce_amax_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_scatter_reduce_amin_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_scatter_reduce_mean_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_scatter_reduce_prod_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_scatter_reduce_sum_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_searchsorted_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_select_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_select_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_select_scatter_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_sgn_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_sgn_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_short_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_short_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_sigmoid_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_sigmoid_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_sign_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_signal_windows_bartlett_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_signal_windows_blackman_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_signal_windows_cosine_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_signal_windows_exponential_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_signal_windows_gaussian_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_signal_windows_general_cosine_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_signal_windows_general_hamming_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_signal_windows_hamming_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_signal_windows_hann_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_signal_windows_kaiser_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_signal_windows_nuttall_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_signbit_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_sin_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_sin_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_sinc_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_sinc_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_sinh_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_sinh_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_slice_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_slice_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_slice_scatter_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_softmax_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_softmax_with_dtype_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_softmax_with_dtype_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_sort_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_sparse_mm_reduce_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_sparse_sampled_addmm_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch is built without MKL support">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch is built without MKL support</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_sparse_sampled_addmm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch is built without MKL support">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch is built without MKL support</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_special_airy_ai_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_special_bessel_j0_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_special_bessel_j1_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_special_bessel_y0_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_special_bessel_y1_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_special_chebyshev_polynomial_t_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_special_chebyshev_polynomial_u_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_special_chebyshev_polynomial_v_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipping - testing takes an unreasonably long time, #79528">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipping - testing takes an unreasonably long time, #79528</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_special_chebyshev_polynomial_w_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipping - testing takes an unreasonably long time, #79528">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipping - testing takes an unreasonably long time, #79528</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_special_entr_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_special_erfcx_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_special_hermite_polynomial_h_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_special_hermite_polynomial_he_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_special_i0e_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_special_i1_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_special_i1e_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_special_laguerre_polynomial_l_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_special_legendre_polynomial_p_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipping - testing takes an unreasonably long time, #79528">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipping - testing takes an unreasonably long time, #79528</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_special_log_ndtr_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_special_modified_bessel_i0_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_special_modified_bessel_i1_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_special_modified_bessel_k0_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_special_modified_bessel_k1_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_special_ndtr_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_special_ndtri_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_special_polygamma_special_polygamma_n_0_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_special_scaled_modified_bessel_k0_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_special_scaled_modified_bessel_k1_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_special_shifted_chebyshev_polynomial_t_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipping - testing takes an unreasonably long time, #79528">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipping - testing takes an unreasonably long time, #79528</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_special_shifted_chebyshev_polynomial_u_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipping - testing takes an unreasonably long time, #79528">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipping - testing takes an unreasonably long time, #79528</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_special_shifted_chebyshev_polynomial_v_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipping - testing takes an unreasonably long time, #79528">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipping - testing takes an unreasonably long time, #79528</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_special_shifted_chebyshev_polynomial_w_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipping - testing takes an unreasonably long time, #79528">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipping - testing takes an unreasonably long time, #79528</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_special_spherical_bessel_j0_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_special_xlog1py_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_special_zeta_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_split_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_split_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_split_list_args_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_split_list_args_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_split_with_sizes_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_split_with_sizes_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_sqrt_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_sqrt_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_square_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_square_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_squeeze_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_squeeze_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_squeeze_multiple_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_squeeze_multiple_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_stack_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_stack_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_std_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_std_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_std_mean_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_std_mean_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_std_mean_unbiased_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_std_mean_unbiased_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_std_unbiased_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_std_unbiased_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_stft_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_stft_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_sub_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_sub_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_sum_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_sum_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_sum_to_size_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_sum_to_size_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_svd_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_svd_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_svd_lowrank_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_t_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_t_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_take_along_dim_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_take_along_dim_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_take_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_take_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_tan_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_tan_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_tanh_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_tanh_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_tensor_split_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_tensor_split_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_tensordot_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_tensordot_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_tile_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_tile_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_to_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_to_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_to_sparse_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_to_sparse_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_topk_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_trace_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_trace_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_transpose_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_transpose_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_trapezoid_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_trapezoid_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_trapz_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_trapz_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_triangular_solve_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_triangular_solve_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_tril_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_tril_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_triu_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_triu_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_true_divide_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_true_divide_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_trunc_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_unbind_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_unbind_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_unflatten_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_unflatten_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_unfold_copy_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_unfold_copy_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_unfold_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_unfold_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_uniform_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_uniform_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_unique_consecutive_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_unique_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_unsafe_split_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_unsafe_split_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_unsqueeze_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_unsqueeze_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_var_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_var_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_var_mean_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_var_mean_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_var_mean_unbiased_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_var_mean_unbiased_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_var_unbiased_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_var_unbiased_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_vdot_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_vdot_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_view_as_complex_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_view_as_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_view_as_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_view_as_real_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_view_copy_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_view_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_view_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_vsplit_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_vsplit_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_vstack_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_vstack_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_where_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_where_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_xlogy_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_zero__cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_zero__cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does support gradgrad">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Operation does support gradgrad</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_zeros_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_zeros_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_zeros_like_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_fail_gradgrad_zeros_like_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:63: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_H_cpu_complex128" time="0.019" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_H_cpu_float64" time="0.004" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_T_cpu_complex128" time="0.015" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_T_cpu_float64" time="0.004" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad___getitem___cpu_complex128" time="0.137" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad___getitem___cpu_float64" time="0.031" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad___radd___cpu_complex128" time="0.107" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad___radd___cpu_float64" time="0.020" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad___rdiv___cpu_complex128" time="0.120" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad___rdiv___cpu_float64" time="0.024" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad___rmatmul___cpu_complex128" time="0.018" file="test_ops_gradients.py">
      <failure message="torch.autograd.gradcheck.GradcheckError: Backward is not reentrant, i.e., running backward with same input and grad_output multiple times gives different values, although analytical gradient matches numerical gradient.The tolerance for nondeterminism was 0.0.&#10;&#10;NOTE: If your op relies on non-deterministic operations i.e., it is listed here:&#10;https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html&#10;this failure might be expected.&#10;&#10;If you are adding a new operator, please file an issue and then use one of the&#10;workarounds. The workaround depends on how your test invokes gradcheck/gradgradcheck.&#10;If the test&#10;- manually invokes gradcheck/gradgradcheck, then call gradcheck/gradgradcheck&#10;  with `nondet_tol=&lt;tol&gt;` as a keyword argument.&#10;- is OpInfo-based (e.g., in test_ops_gradients.py), then modify the OpInfo for the test&#10;  to have `gradcheck_nondet_tol=&lt;tol&gt;`.&#10;- is a Module test (e.g., in common_nn.py), then modify the corresponding&#10;  module_test entry to have `gradcheck_nondet_tol=&lt;tol&gt;`">Traceback (most recent call last):
  File "C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py", line 26, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 4335, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 4304, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 3859, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1536, in gradcheck
    return _gradcheck_helper(**args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1550, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1157, in _gradcheck_real_imag
    gradcheck_fn(imag_fn, imag_func_out, tupled_inputs, imag_outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1419, in _fast_gradcheck
    analytical_vJu = _get_analytical_vJu_backward_mode(inputs, outputs, nondet_tol, check_grad_dtypes, all_v, all_u_dense)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 646, in _get_analytical_vJu_backward_mode
    all_vJ = _check_analytical_jacobian_attributes(inputs, output, nondet_tol, check_grad_dtypes,
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 635, in _check_analytical_jacobian_attributes
    raise GradcheckError('Backward is not reentrant, i.e., running backward with '
torch.autograd.gradcheck.GradcheckError: Backward is not reentrant, i.e., running backward with same input and grad_output multiple times gives different values, although analytical gradient matches numerical gradient.The tolerance for nondeterminism was 0.0.

NOTE: If your op relies on non-deterministic operations i.e., it is listed here:
https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html
this failure might be expected.

If you are adding a new operator, please file an issue and then use one of the
workarounds. The workaround depends on how your test invokes gradcheck/gradgradcheck.
If the test
- manually invokes gradcheck/gradgradcheck, then call gradcheck/gradgradcheck
  with `nondet_tol=&lt;tol&gt;` as a keyword argument.
- is OpInfo-based (e.g., in test_ops_gradients.py), then modify the OpInfo for the test
  to have `gradcheck_nondet_tol=&lt;tol&gt;`.
- is a Module test (e.g., in common_nn.py), then modify the corresponding
  module_test entry to have `gradcheck_nondet_tol=&lt;tol&gt;`</failure>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad___rmatmul___cpu_float64" time="0.045" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad___rmod___cpu_float64" time="0.023" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad___rmul___cpu_complex128" time="0.108" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad___rmul___cpu_float64" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad___rpow___cpu_complex128" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad___rpow___cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad___rsub___cpu_complex128" time="0.105" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad___rsub___cpu_float64" time="0.021" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad__native_batch_norm_legit_cpu_float64" time="0.062" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad__segment_reduce_lengths_cpu_float64" time="0.150" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad__segment_reduce_offsets_cpu_float64" time="0.146" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad__softmax_backward_data_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad__upsample_bilinear2d_aa_cpu_float64" time="0.014" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_abs_cpu_complex128" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_abs_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_acos_cpu_complex128" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_acos_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_acosh_cpu_complex128" time="0.023" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_acosh_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_add_cpu_complex128" time="0.126" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_add_cpu_float64" time="0.024" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_addbmm_cpu_complex128" time="0.345" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_addbmm_cpu_float64" time="0.024" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_addcdiv_cpu_complex128" time="0.202" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_addcdiv_cpu_float64" time="0.041" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_addcmul_cpu_complex128" time="0.205" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_addcmul_cpu_float64" time="0.038" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_addmm_cpu_complex128" time="0.006" file="test_ops_gradients.py">
      <failure message="torch.autograd.gradcheck.GradcheckError: Backward is not reentrant, i.e., running backward with same input and grad_output multiple times gives different values, although analytical gradient matches numerical gradient.The tolerance for nondeterminism was 1e-12.&#10;&#10;NOTE: If your op relies on non-deterministic operations i.e., it is listed here:&#10;https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html&#10;this failure might be expected.&#10;&#10;If you are adding a new operator, please file an issue and then use one of the&#10;workarounds. The workaround depends on how your test invokes gradcheck/gradgradcheck.&#10;If the test&#10;- manually invokes gradcheck/gradgradcheck, then call gradcheck/gradgradcheck&#10;  with `nondet_tol=&lt;tol&gt;` as a keyword argument.&#10;- is OpInfo-based (e.g., in test_ops_gradients.py), then modify the OpInfo for the test&#10;  to have `gradcheck_nondet_tol=&lt;tol&gt;`.&#10;- is a Module test (e.g., in common_nn.py), then modify the corresponding&#10;  module_test entry to have `gradcheck_nondet_tol=&lt;tol&gt;`">Traceback (most recent call last):
  File "C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py", line 26, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 4335, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 4304, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 3859, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1536, in gradcheck
    return _gradcheck_helper(**args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1550, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1157, in _gradcheck_real_imag
    gradcheck_fn(imag_fn, imag_func_out, tupled_inputs, imag_outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1419, in _fast_gradcheck
    analytical_vJu = _get_analytical_vJu_backward_mode(inputs, outputs, nondet_tol, check_grad_dtypes, all_v, all_u_dense)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 646, in _get_analytical_vJu_backward_mode
    all_vJ = _check_analytical_jacobian_attributes(inputs, output, nondet_tol, check_grad_dtypes,
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 635, in _check_analytical_jacobian_attributes
    raise GradcheckError('Backward is not reentrant, i.e., running backward with '
torch.autograd.gradcheck.GradcheckError: Backward is not reentrant, i.e., running backward with same input and grad_output multiple times gives different values, although analytical gradient matches numerical gradient.The tolerance for nondeterminism was 1e-12.

NOTE: If your op relies on non-deterministic operations i.e., it is listed here:
https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html
this failure might be expected.

If you are adding a new operator, please file an issue and then use one of the
workarounds. The workaround depends on how your test invokes gradcheck/gradgradcheck.
If the test
- manually invokes gradcheck/gradgradcheck, then call gradcheck/gradgradcheck
  with `nondet_tol=&lt;tol&gt;` as a keyword argument.
- is OpInfo-based (e.g., in test_ops_gradients.py), then modify the OpInfo for the test
  to have `gradcheck_nondet_tol=&lt;tol&gt;`.
- is a Module test (e.g., in common_nn.py), then modify the corresponding
  module_test entry to have `gradcheck_nondet_tol=&lt;tol&gt;`</failure>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_addmm_cpu_float64" time="0.011" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_addmm_decomposed_cpu_complex128" time="0.006" file="test_ops_gradients.py">
      <failure message="torch.autograd.gradcheck.GradcheckError: Backward is not reentrant, i.e., running backward with same input and grad_output multiple times gives different values, although analytical gradient matches numerical gradient.The tolerance for nondeterminism was 1e-12.&#10;&#10;NOTE: If your op relies on non-deterministic operations i.e., it is listed here:&#10;https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html&#10;this failure might be expected.&#10;&#10;If you are adding a new operator, please file an issue and then use one of the&#10;workarounds. The workaround depends on how your test invokes gradcheck/gradgradcheck.&#10;If the test&#10;- manually invokes gradcheck/gradgradcheck, then call gradcheck/gradgradcheck&#10;  with `nondet_tol=&lt;tol&gt;` as a keyword argument.&#10;- is OpInfo-based (e.g., in test_ops_gradients.py), then modify the OpInfo for the test&#10;  to have `gradcheck_nondet_tol=&lt;tol&gt;`.&#10;- is a Module test (e.g., in common_nn.py), then modify the corresponding&#10;  module_test entry to have `gradcheck_nondet_tol=&lt;tol&gt;`">Traceback (most recent call last):
  File "C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py", line 26, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 4335, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 4304, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 3859, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1536, in gradcheck
    return _gradcheck_helper(**args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1550, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1157, in _gradcheck_real_imag
    gradcheck_fn(imag_fn, imag_func_out, tupled_inputs, imag_outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1419, in _fast_gradcheck
    analytical_vJu = _get_analytical_vJu_backward_mode(inputs, outputs, nondet_tol, check_grad_dtypes, all_v, all_u_dense)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 646, in _get_analytical_vJu_backward_mode
    all_vJ = _check_analytical_jacobian_attributes(inputs, output, nondet_tol, check_grad_dtypes,
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 635, in _check_analytical_jacobian_attributes
    raise GradcheckError('Backward is not reentrant, i.e., running backward with '
torch.autograd.gradcheck.GradcheckError: Backward is not reentrant, i.e., running backward with same input and grad_output multiple times gives different values, although analytical gradient matches numerical gradient.The tolerance for nondeterminism was 1e-12.

NOTE: If your op relies on non-deterministic operations i.e., it is listed here:
https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html
this failure might be expected.

If you are adding a new operator, please file an issue and then use one of the
workarounds. The workaround depends on how your test invokes gradcheck/gradgradcheck.
If the test
- manually invokes gradcheck/gradgradcheck, then call gradcheck/gradgradcheck
  with `nondet_tol=&lt;tol&gt;` as a keyword argument.
- is OpInfo-based (e.g., in test_ops_gradients.py), then modify the OpInfo for the test
  to have `gradcheck_nondet_tol=&lt;tol&gt;`.
- is a Module test (e.g., in common_nn.py), then modify the corresponding
  module_test entry to have `gradcheck_nondet_tol=&lt;tol&gt;`</failure>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_addmm_decomposed_cpu_float64" time="0.011" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_addmv_cpu_complex128" time="0.101" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_addmv_cpu_float64" time="0.019" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_addr_cpu_complex128" time="0.069" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_addr_cpu_float64" time="0.013" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_all_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_all_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_allclose_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_allclose_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_amax_cpu_float64" time="0.033" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_amin_cpu_float64" time="0.033" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_aminmax_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_angle_cpu_complex128" time="0.007" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_angle_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_any_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_any_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_arange_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_argmax_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_argmin_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_argsort_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_argwhere_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_argwhere_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_as_strided_cpu_complex128" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Numerous errors">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Numerous errors</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_as_strided_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Numerous errors">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Numerous errors</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_as_strided_partial_views_cpu_complex128" time="0.023" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_as_strided_partial_views_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_as_strided_scatter_cpu_complex128" time="0.008" file="test_ops_gradients.py">
      <skipped type="pytest.xfail" message="" />
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_as_strided_scatter_cpu_float64" time="0.005" file="test_ops_gradients.py">
      <skipped type="pytest.xfail" message="" />
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_asin_cpu_complex128" time="0.012" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_asin_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_asinh_cpu_complex128" time="0.012" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_asinh_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_atan2_cpu_float64" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_atan_cpu_complex128" time="0.010" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_atan_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_atanh_cpu_complex128" time="0.010" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_atanh_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_atleast_1d_cpu_complex128" time="0.113" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_atleast_1d_cpu_float64" time="0.021" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_atleast_2d_cpu_complex128" time="0.118" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_atleast_2d_cpu_float64" time="0.023" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_atleast_3d_cpu_complex128" time="0.118" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_atleast_3d_cpu_float64" time="0.023" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_baddbmm_cpu_complex128" time="0.363" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_baddbmm_cpu_float64" time="0.023" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_bernoulli_cpu_float64" time="0.039" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_bfloat16_cpu_complex128" time="0.008" file="test_ops_gradients.py">
      <skipped type="pytest.xfail" message="" />
      <system-err>c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_methods_invocations.py:15153: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at C:\Users\radekbarton\Projects\pytorch\aten\src\ATen\native\Copy.cpp:276.) op=lambda x, *args, **kwargs: x.bfloat16(*args, **kwargs),
      </system-err>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_bfloat16_cpu_float64" time="0.006" file="test_ops_gradients.py">
      <skipped type="pytest.xfail" message="" />
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_block_diag_cpu_complex128" time="0.097" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_block_diag_cpu_float64" time="0.013" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_bmm_cpu_complex128" time="0.104" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_bmm_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_bool_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_bool_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_broadcast_tensors_cpu_complex128" time="0.088" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_broadcast_tensors_cpu_float64" time="0.013" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_broadcast_to_cpu_complex128" time="0.048" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_broadcast_to_cpu_float64" time="0.011" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_bucketize_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_byte_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_byte_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_cartesian_prod_cpu_complex128" time="0.044" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_cartesian_prod_cpu_float64" time="0.010" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_cat_cpu_complex128" time="0.099" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_cat_cpu_float64" time="0.019" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_cauchy_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_cdist_cpu_float64" time="0.677" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_cdouble_cpu_complex128" time="0.031" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_cdouble_cpu_float64" time="0.014" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_ceil_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_cfloat_cpu_complex128" time="0.006" file="test_ops_gradients.py">
      <skipped type="pytest.xfail" message="" />
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_cfloat_cpu_float64" time="0.004" file="test_ops_gradients.py">
      <skipped type="pytest.xfail" message="" />
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_chalf_cpu_complex128" time="0.003" file="test_ops_gradients.py">
      <skipped type="pytest.xfail" message="" />
      <system-err>c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_methods_invocations.py:15319: UserWarning: ComplexHalf support is experimental and many operators don't support it yet. (Triggered internally at C:\Users\radekbarton\Projects\pytorch\aten\src\ATen\EmptyTensor.cpp:32.) op=lambda x, *args, **kwargs: x.chalf(*args, **kwargs),
      </system-err>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_chalf_cpu_float64" time="0.005" file="test_ops_gradients.py">
      <skipped type="pytest.xfail" message="" />
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_char_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_char_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_cholesky_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_cholesky_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_cholesky_inverse_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_cholesky_inverse_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_cholesky_solve_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_cholesky_solve_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_chunk_cpu_complex128" time="0.072" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_chunk_cpu_float64" time="0.018" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_clamp_cpu_float64" time="0.015" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_clamp_max_cpu_float64" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_clamp_min_cpu_float64" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_clone_cpu_complex128" time="0.014" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_clone_cpu_float64" time="0.004" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_column_stack_cpu_complex128" time="0.037" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_column_stack_cpu_float64" time="0.008" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_combinations_cpu_complex128" time="0.210" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_combinations_cpu_float64" time="0.049" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_complex_cpu_float64" time="0.037" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_conj_cpu_complex128" time="0.021" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_conj_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_conj_physical_cpu_complex128" time="0.009" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_conj_physical_cpu_float64" time="0.002" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_constant_pad_nd_cpu_complex128" time="0.357" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_constant_pad_nd_cpu_float64" time="0.077" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_contiguous_cpu_complex128" time="0.013" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_contiguous_cpu_float64" time="0.004" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_copysign_cpu_float64" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_corrcoef_cpu_complex128" time="0.039" file="test_ops_gradients.py">
      <failure message="torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 0 with respect to input 0,&#10;numerical:tensor(nan+nanj, dtype=torch.complex128)&#10;analytical:tensor(nan+nanj, dtype=torch.complex128)&#10;&#10;The above quantities relating the numerical and analytical jacobians are computed &#10;in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background &#10;about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:&#10;&#10;Numerical:&#10; tensor([[0.+0.j, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj],&#10;        [0.+0.j, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj],&#10;        [0.+0.j, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj],&#10;        [0.+0.j, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj],&#10;        [0.+0.j, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj],&#10;        [0.+0.j, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj]],&#10; dtype=torch.complex128)&#10;Analytical:&#10;tensor([[nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj],&#10;        [nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj],&#10;        [nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj],&#10;        [nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj],&#10;        [nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj],&#10;        [nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj]],&#10; dtype=torch.complex128)&#10;&#10;The max per-element difference (slow mode) is: nan.">Traceback (most recent call last):
  File "C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py", line 26, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 4335, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 4304, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 3859, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1536, in gradcheck
    return _gradcheck_helper(**args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1550, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1157, in _gradcheck_real_imag
    gradcheck_fn(imag_fn, imag_func_out, tupled_inputs, imag_outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1421, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1389, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(nan+nanj, dtype=torch.complex128)
analytical:tensor(nan+nanj, dtype=torch.complex128)

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[0.+0.j, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj],
        [0.+0.j, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj],
        [0.+0.j, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj],
        [0.+0.j, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj],
        [0.+0.j, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj],
        [0.+0.j, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj]], dtype=torch.complex128)
Analytical:
tensor([[nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj],
        [nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj],
        [nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj],
        [nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj],
        [nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj],
        [nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj, nan+nanj]], dtype=torch.complex128)

The max per-element difference (slow mode) is: nan.</failure>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_corrcoef_cpu_float64" time="0.014" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_cos_cpu_complex128" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_cos_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_cosh_cpu_complex128" time="0.020" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_cosh_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_count_nonzero_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_count_nonzero_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_cov_cpu_complex128" time="0.029" file="test_ops_gradients.py">
      <skipped type="pytest.xfail" message="" />
      <system-err>c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py:739: UserWarning: Input #1 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  warnings.warn(
      </system-err>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_cov_cpu_float64" time="0.013" file="test_ops_gradients.py">
      <skipped type="pytest.xfail" message="" />
      <system-err>c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py:739: UserWarning: Input #1 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  warnings.warn(
      </system-err>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_cross_cpu_complex128" time="0.038" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_cross_cpu_float64" time="0.008" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_cummax_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_cummin_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_cumprod_cpu_complex128" time="4.818" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_cumprod_cpu_float64" time="1.175" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_cumsum_cpu_complex128" time="0.030" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_cumsum_cpu_float64" time="0.008" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_cumulative_trapezoid_cpu_complex128" time="0.127" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_cumulative_trapezoid_cpu_float64" time="0.028" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_deg2rad_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_diag_cpu_complex128" time="0.106" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_diag_cpu_float64" time="0.023" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_diag_embed_cpu_complex128" time="0.106" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_diag_embed_cpu_float64" time="0.023" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_diagflat_cpu_complex128" time="0.036" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_diagflat_cpu_float64" time="0.009" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_diagonal_copy_cpu_complex128" time="0.100" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_diagonal_copy_cpu_float64" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_diagonal_cpu_complex128" time="0.101" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_diagonal_cpu_float64" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_diagonal_scatter_cpu_complex128" time="0.178" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_diagonal_scatter_cpu_float64" time="0.036" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_diff_cpu_complex128" time="1.029" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_diff_cpu_float64" time="0.207" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_digamma_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_dist_cpu_complex128" time="0.364" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_dist_cpu_float64" time="0.131" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_div_floor_rounding_cpu_float64" time="0.023" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_div_no_rounding_mode_cpu_complex128" time="0.111" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_div_no_rounding_mode_cpu_float64" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_div_trunc_rounding_cpu_float64" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_dot_cpu_complex128" time="0.024" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_dot_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_double_cpu_complex128" time="0.019" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_double_cpu_float64" time="0.008" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_dsplit_cpu_complex128" time="0.038" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_dsplit_cpu_float64" time="0.010" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_dstack_cpu_complex128" time="0.037" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_dstack_cpu_float64" time="0.008" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_einsum_cpu_complex128" time="0.147" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_einsum_cpu_float64" time="0.030" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_empty_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_empty_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_empty_like_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_empty_like_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_empty_permuted_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_empty_permuted_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_eq_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_eq_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_equal_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_equal_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_erf_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_erfc_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_erfinv_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_exp2_cpu_complex128" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_exp2_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_exp_cpu_complex128" time="0.021" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_exp_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_expand_as_cpu_complex128" time="0.021" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_expand_as_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_expand_cpu_complex128" time="0.064" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_expand_cpu_float64" time="0.014" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_expm1_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_exponential_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_eye_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_eye_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fft_fft2_cpu_complex128" time="0.051" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fft_fft2_cpu_float64" time="0.021" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fft_fft_cpu_complex128" time="0.055" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fft_fft_cpu_float64" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fft_fftn_cpu_complex128" time="0.064" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fft_fftn_cpu_float64" time="0.027" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fft_fftshift_cpu_complex128" time="0.040" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fft_fftshift_cpu_float64" time="0.010" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fft_hfft2_cpu_complex128" time="0.035" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fft_hfft2_cpu_float64" time="0.016" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fft_hfft_cpu_complex128" time="0.036" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fft_hfft_cpu_float64" time="0.016" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fft_hfftn_cpu_complex128" time="0.043" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fft_hfftn_cpu_float64" time="0.019" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fft_ifft2_cpu_complex128" time="0.052" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fft_ifft2_cpu_float64" time="0.021" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fft_ifft_cpu_complex128" time="0.054" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fft_ifft_cpu_float64" time="0.025" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fft_ifftn_cpu_complex128" time="0.066" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fft_ifftn_cpu_float64" time="0.027" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fft_ifftshift_cpu_complex128" time="0.040" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fft_ifftshift_cpu_float64" time="0.009" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fft_ihfft2_cpu_float64" time="0.019" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fft_ihfft_cpu_float64" time="0.020" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fft_ihfftn_cpu_float64" time="0.024" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fft_irfft2_cpu_complex128" time="0.030" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fft_irfft2_cpu_float64" time="0.014" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fft_irfft_cpu_complex128" time="0.033" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fft_irfft_cpu_float64" time="0.015" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fft_irfftn_cpu_complex128" time="0.038" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fft_irfftn_cpu_float64" time="0.018" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fft_rfft2_cpu_float64" time="0.017" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fft_rfft_cpu_float64" time="0.019" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fft_rfftn_cpu_float64" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fill_cpu_complex128" time="0.021" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fill_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_flatten_cpu_complex128" time="0.039" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_flatten_cpu_float64" time="0.009" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_flip_cpu_complex128" time="0.068" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_flip_cpu_float64" time="0.015" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fliplr_cpu_complex128" time="0.015" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fliplr_cpu_float64" time="0.004" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_flipud_cpu_complex128" time="0.015" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_flipud_cpu_float64" time="0.004" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_float_cpu_complex128" time="0.005" file="test_ops_gradients.py">
      <skipped type="pytest.xfail" message="" />
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_float_cpu_float64" time="0.003" file="test_ops_gradients.py">
      <skipped type="pytest.xfail" message="" />
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_float_power_cpu_complex128" time="0.138" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_float_power_cpu_float64" time="0.026" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_floor_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_floor_divide_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fmax_cpu_float64" time="0.023" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fmin_cpu_float64" time="0.023" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_fmod_cpu_float64" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_frac_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_frexp_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_full_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_full_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_full_like_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_full_like_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_gather_cpu_complex128" time="0.037" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_gather_cpu_float64" time="0.009" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_ge_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_geometric_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_geqrf_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_geqrf_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_gradient_cpu_complex128" time="0.247" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_gradient_cpu_float64" time="0.061" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_grid_sampler_2d_cpu_float64" time="0.066" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_gt_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_half_cpu_complex128" time="0.003" file="test_ops_gradients.py">
      <skipped type="pytest.xfail" message="" />
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_half_cpu_float64" time="0.003" file="test_ops_gradients.py">
      <skipped type="pytest.xfail" message="" />
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_heaviside_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_histc_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_histogram_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_histogramdd_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_hsplit_cpu_complex128" time="0.037" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_hsplit_cpu_float64" time="0.009" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_hstack_cpu_complex128" time="0.033" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_hstack_cpu_float64" time="0.007" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_hypot_cpu_float64" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_i0_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_igamma_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_igammac_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_imag_cpu_complex128" time="0.012" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_index_add_cpu_complex128" time="0.103" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_index_add_cpu_float64" time="0.021" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_index_copy_cpu_complex128" time="0.037" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_index_copy_cpu_float64" time="0.008" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_index_fill_cpu_complex128" time="0.057" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_index_fill_cpu_float64" time="0.013" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_index_put_cpu_complex128" time="0.051" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_index_put_cpu_float64" time="0.011" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_index_reduce_cpu_float64" time="0.092" file="test_ops_gradients.py">
      <system-err>c:\users\radekbarton\projects\pytorch\torch\testing\_internal\opinfo\core.py:766: UserWarning: index_reduce() is in beta and the API may change at any time. (Triggered internally at C:\Users\radekbarton\Projects\pytorch\aten\src\ATen\native\TensorAdvancedIndexing.cpp:1110.)
  gradcheck_wrapper: Callable = lambda op, *args, **kwargs: op(*args, **kwargs)
      </system-err>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_index_select_cpu_complex128" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_index_select_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_inner_cpu_complex128" time="0.014" file="test_ops_gradients.py">
      <failure message="torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 0 with respect to input 0,&#10;numerical:tensor(2.7695+9.1335j, dtype=torch.complex128)&#10;analytical:tensor(-2.7695+9.1335j, dtype=torch.complex128)&#10;&#10;The above quantities relating the numerical and analytical jacobians are computed &#10;in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background &#10;about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:&#10;&#10;Numerical:&#10; tensor([[ 0.1566-0.5461j],&#10;        [ 6.4699+6.8944j],&#10;        [ 7.2419+7.8815j],&#10;        [ 2.8554+6.6666j],&#10;        [-7.1478-0.5572j]], dtype=torch.complex128)&#10;Analytical:&#10;tensor([[-0.1566-0.5461j],&#10;        [-6.4699+6.8944j],&#10;        [-7.2419+7.8815j],&#10;        [-2.8554+6.6666j],&#10;        [ 7.1478-0.5572j]], dtype=torch.complex128)&#10;&#10;The max per-element difference (slow mode) is: 14.483799531716283.">Traceback (most recent call last):
  File "C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py", line 26, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 4335, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 4304, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 3859, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1536, in gradcheck
    return _gradcheck_helper(**args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1550, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1157, in _gradcheck_real_imag
    gradcheck_fn(imag_fn, imag_func_out, tupled_inputs, imag_outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1421, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1389, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(2.7695+9.1335j, dtype=torch.complex128)
analytical:tensor(-2.7695+9.1335j, dtype=torch.complex128)

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[ 0.1566-0.5461j],
        [ 6.4699+6.8944j],
        [ 7.2419+7.8815j],
        [ 2.8554+6.6666j],
        [-7.1478-0.5572j]], dtype=torch.complex128)
Analytical:
tensor([[-0.1566-0.5461j],
        [-6.4699+6.8944j],
        [-7.2419+7.8815j],
        [-2.8554+6.6666j],
        [ 7.1478-0.5572j]], dtype=torch.complex128)

The max per-element difference (slow mode) is: 14.483799531716283.</failure>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_inner_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_int_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_int_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_isclose_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_isclose_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_isfinite_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_isfinite_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_isin_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_isinf_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_isinf_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_isnan_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_isnan_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_isneginf_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_isposinf_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_isreal_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_isreal_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_istft_cpu_complex128" time="1.658" file="test_ops_gradients.py">
      <skipped type="pytest.xfail" message="" />
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_jiterator_2inputs_2outputs_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_jiterator_2inputs_2outputs_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_jiterator_4inputs_with_extra_args_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_jiterator_4inputs_with_extra_args_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_jiterator_binary_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_jiterator_binary_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_jiterator_binary_return_by_ref_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_jiterator_binary_return_by_ref_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_jiterator_unary_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_jiterator_unary_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_kron_cpu_complex128" time="0.031" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_kron_cpu_float64" time="0.007" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_kthvalue_cpu_float64" time="0.026" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_ldexp_cpu_complex128" time="0.120" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_ldexp_cpu_float64" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_le_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_lerp_cpu_complex128" time="0.284" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_lerp_cpu_float64" time="0.037" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_lgamma_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_cholesky_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_cholesky_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_cholesky_ex_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_cholesky_ex_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_cond_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_cond_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_cross_cpu_complex128" time="0.037" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_cross_cpu_float64" time="0.008" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_det_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_det_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_det_singular_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_det_singular_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_diagonal_cpu_complex128" time="0.101" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_diagonal_cpu_float64" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_eig_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_eig_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_eigh_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_eigh_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_eigvals_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_eigvals_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_eigvalsh_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_eigvalsh_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_householder_product_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_householder_product_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_inv_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_inv_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_inv_ex_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_inv_ex_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_ldl_factor_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_ldl_factor_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_ldl_factor_ex_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_ldl_factor_ex_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_ldl_solve_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_ldl_solve_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_lstsq_cpu_complex128" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_lstsq_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_lstsq_grad_oriented_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_lstsq_grad_oriented_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_lu_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_lu_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_lu_factor_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_lu_factor_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_lu_factor_ex_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_lu_factor_ex_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_lu_solve_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_lu_solve_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_matrix_norm_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_matrix_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_matrix_power_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_matrix_power_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_matrix_rank_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_matrix_rank_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_matrix_rank_hermitian_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_matrix_rank_hermitian_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_multi_dot_cpu_complex128" time="0.010" file="test_ops_gradients.py">
      <failure message="torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 0 with respect to input 0,&#10;numerical:tensor(-2.7019-3.8731j, dtype=torch.complex128)&#10;analytical:tensor(2.7019-3.8731j, dtype=torch.complex128)&#10;&#10;The above quantities relating the numerical and analytical jacobians are computed &#10;in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background &#10;about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:&#10;&#10;Numerical:&#10; tensor([[ 1.6124-3.2001j],&#10;        [-3.3198-2.3758j]], dtype=torch.complex128)&#10;Analytical:&#10;tensor([[-1.6124-3.2001j],&#10;        [ 3.3198-2.3758j]], dtype=torch.complex128)&#10;&#10;The max per-element difference (slow mode) is: 6.63952751383267.">Traceback (most recent call last):
  File "C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py", line 26, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 4335, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 4304, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 3859, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1536, in gradcheck
    return _gradcheck_helper(**args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1550, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1157, in _gradcheck_real_imag
    gradcheck_fn(imag_fn, imag_func_out, tupled_inputs, imag_outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1421, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1389, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(-2.7019-3.8731j, dtype=torch.complex128)
analytical:tensor(2.7019-3.8731j, dtype=torch.complex128)

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[ 1.6124-3.2001j],
        [-3.3198-2.3758j]], dtype=torch.complex128)
Analytical:
tensor([[-1.6124-3.2001j],
        [ 3.3198-2.3758j]], dtype=torch.complex128)

The max per-element difference (slow mode) is: 6.63952751383267.</failure>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_multi_dot_cpu_float64" time="0.017" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_norm_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_norm_subgradients_at_zero_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_norm_subgradients_at_zero_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_pinv_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_pinv_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_pinv_hermitian_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_pinv_hermitian_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_pinv_singular_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_pinv_singular_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_qr_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_qr_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_slogdet_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_slogdet_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_solve_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_solve_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_solve_ex_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_solve_ex_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_solve_triangular_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_solve_triangular_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_svd_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_svd_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_svdvals_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_svdvals_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_tensorinv_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_tensorinv_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_tensorsolve_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_tensorsolve_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_vander_cpu_complex128" time="0.096" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_vander_cpu_float64" time="0.023" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_vecdot_cpu_complex128" time="0.526" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_vecdot_cpu_float64" time="0.092" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_vector_norm_cpu_complex128" time="0.744" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linalg_vector_norm_cpu_float64" time="0.287" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linspace_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_linspace_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_log10_cpu_complex128" time="0.021" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_log10_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_log1p_cpu_complex128" time="0.010" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_log1p_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_log2_cpu_complex128" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_log2_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_log_cpu_complex128" time="0.020" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_log_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_log_normal_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_log_softmax_cpu_float64" time="0.012" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_log_softmax_with_dtype_cpu_complex128" time="0.029" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_log_softmax_with_dtype_cpu_float64" time="0.012" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_logaddexp2_cpu_float64" time="0.004" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_logaddexp_cpu_complex128" time="0.130" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_logaddexp_cpu_float64" time="0.023" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_logcumsumexp_cpu_complex128" time="0.052" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_logcumsumexp_cpu_float64" time="0.015" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_logdet_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_logdet_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_logical_and_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_logical_and_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_logical_not_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_logical_not_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_logical_or_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_logical_or_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_logical_xor_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_logical_xor_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_logit_cpu_float64" time="0.007" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_logspace_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_logspace_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_logsumexp_cpu_float64" time="0.106" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_long_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_long_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_lt_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_lu_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_lu_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_lu_solve_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_lu_solve_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_lu_unpack_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_lu_unpack_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_mH_cpu_complex128" time="0.044" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_mH_cpu_float64" time="0.009" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_mT_cpu_complex128" time="0.039" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_mT_cpu_float64" time="0.009" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_masked_amax_cpu_float64" time="0.180" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_masked_amin_cpu_float64" time="0.178" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_masked_argmax_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_masked_argmin_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_masked_cumprod_cpu_complex128" time="0.196" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_masked_cumprod_cpu_float64" time="0.042" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_masked_cumsum_cpu_complex128" time="0.182" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_masked_cumsum_cpu_float64" time="0.040" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_masked_fill_cpu_complex128" time="0.073" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_masked_fill_cpu_float64" time="0.016" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_masked_log_softmax_cpu_float64" time="0.053" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_masked_logaddexp_cpu_float64" time="0.045" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_masked_logsumexp_cpu_float64" time="0.188" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_masked_mean_cpu_complex128" time="0.982" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_masked_mean_cpu_float64" time="0.200" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_masked_median_cpu_float64" time="0.039" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_masked_norm_cpu_float64" time="0.884" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_masked_normalize_cpu_complex128" time="0.386" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_masked_normalize_cpu_float64" time="0.079" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_masked_prod_cpu_complex128" time="0.833" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_masked_prod_cpu_float64" time="0.180" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_masked_scatter_cpu_complex128" time="0.050" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_masked_scatter_cpu_float64" time="0.011" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_masked_select_cpu_complex128" time="0.054" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_masked_select_cpu_float64" time="0.013" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_masked_softmax_cpu_float64" time="0.053" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_masked_softmin_cpu_float64" time="0.055" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_masked_std_cpu_complex128" time="0.786" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_masked_std_cpu_float64" time="0.156" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_masked_sum_cpu_complex128" time="0.692" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_masked_sum_cpu_float64" time="0.141" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_masked_var_cpu_complex128" time="0.750" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_masked_var_cpu_float64" time="0.150" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_matmul_cpu_complex128" time="0.060" file="test_ops_gradients.py">
      <failure message="torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 0 with respect to input 0,&#10;numerical:tensor(-5.4641+1.5160j, dtype=torch.complex128)&#10;analytical:tensor(21.4736+0.4841j, dtype=torch.complex128)&#10;&#10;The above quantities relating the numerical and analytical jacobians are computed &#10;in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background &#10;about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:&#10;&#10;Numerical:&#10; tensor([[ 0.3142+4.6551j, -1.2112+4.1163j,  3.3540+5.5089j, -7.2121+6.9711j,&#10;          8.1426-2.9480j],&#10;        [-6.8702-4.2623j, -7.7593-5.2365j, -8.8576-7.6004j,  6.2623-3.3311j,&#10;         -8.7005+3.7920j],&#10;        [ 2.3469-2.9610j,  0.3075-6.7447j,  7.5576+6.0656j, -4.0985+3.3758j,&#10;         -7.2926-4.7286j],&#10;        [ 6.3172+5.4643j, -5.8316-2.1744j, -1.4822+8.6413j,  7.7615-5.2964j,&#10;         -6.7219-2.1284j],&#10;        [-3.1535+3.6208j,  0.1218-8.6433j,  3.5591-2.6182j,  6.1059-5.0949j,&#10;         -0.0261+6.1196j],&#10;        [ 2.9463+1.9439j,  6.0270-5.1074j,  4.7828-1.3698j, -1.6207+4.9976j,&#10;          7.8116+2.7221j],&#10;        [-5.6230+8.2865j, -5.5448+0.0601j, -2.2275+3.2058j, -3.4649-0.7534j,&#10;         -2.0329+4.8251j],&#10;        [-4.0793+3.5154j, -6.3153-2.1796j,  0.9348-1.2434j,  5.8404+8.6417j,&#10;         -2.7541+8.8165j],&#10;        [-4.1198-6.6074j,  5.1320+3.5038j, -8.5293+8.1905j,  6.9186+2.5708j,&#10;         -6.0272+0.3845j],&#10;        [-4.1917-1.3899j, -2.8770-8.5045j,  7.7401-0.7434j, -6.8162-7.5620j,&#10;         -4.1208-7.9884j]], dtype=torch.complex128)&#10;Analytical:&#10;tensor([[ -3.1420e-01+4.6551e+00j, -9.4717e-311+4.4363e-311j,&#10;         -9.4717e-311+4.4363e-311j, -9.4717e-311+4.4363e-311j,&#10;         -1.5170e-305+6.8042e-306j],&#10;        [  6.8702e+00-4.2623e+00j,  1.0669e-310-4.1919e-311j,&#10;          1.0669e-310-4.1919e-311j,  1.0669e-310-4.1919e-311j,&#10;          1.7066e-305-6.3848e-306j],&#10;        [ -2.3469e+00-2.9610e+00j,  2.1898e-311-1.0266e-310j,&#10;          2.1898e-311-1.0266e-310j,  2.1898e-311-1.0266e-310j,&#10;          3.7478e-306-1.6260e-305j],&#10;        [ -6.3172e+00+5.4643e+00j,  3.9230e-311-7.5584e-311j,&#10;          3.9230e-311-7.5584e-311j,  3.9230e-311-7.5584e-311j,&#10;          6.4322e-306-1.1911e-305j],&#10;        [  3.1535e+00+3.6208e+00j,  5.2486e-311+5.2041e-311j,&#10;          5.2486e-311+5.2041e-311j,  5.2486e-311+5.2041e-311j,&#10;          8.2063e-306+8.4080e-306j],&#10;        [ -2.9463e+00+1.9439e+00j, -4.3466e-311+8.9960e-311j,&#10;         -4.3466e-311+8.9960e-311j, -4.3466e-311+8.9960e-311j,&#10;         -7.1429e-306+1.4185e-305j],&#10;        [  5.6230e+00+8.2865e+00j,  5.8569e-311+2.3846e-311j,&#10;          5.8569e-311+2.3846e-311j,  5.8569e-311+2.3846e-311j,&#10;          9.2467e-306+3.9426e-306j],&#10;        [  4.0793e+00+3.5154e+00j,  9.8816e-311+5.1775e-311j,&#10;          9.8816e-311+5.1775e-311j,  9.8816e-311+5.1775e-311j,&#10;          1.5571e-305+8.4864e-306j],&#10;        [  4.1198e+00-6.6074e+00j,  5.4757e-311-4.8190e-311j,&#10;          5.4757e-311-4.8190e-311j,  5.4757e-311-4.8190e-311j,&#10;          8.8287e-306-7.5166e-306j],&#10;        [  4.1917e+00-1.3899e+00j, -3.3030e-311-1.0342e-310j,&#10;         -3.3030e-311-1.0342e-310j, -3.3030e-311-1.0342e-310j,&#10;         -4.9804e-306-1.6523e-305j]], dtype=torch.complex128)&#10;&#10;The max per-element difference (slow mode) is: 13.740387730898426.">Traceback (most recent call last):
  File "C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py", line 26, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 4335, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 4304, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 3859, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1536, in gradcheck
    return _gradcheck_helper(**args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1550, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1157, in _gradcheck_real_imag
    gradcheck_fn(imag_fn, imag_func_out, tupled_inputs, imag_outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1421, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1389, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(-5.4641+1.5160j, dtype=torch.complex128)
analytical:tensor(21.4736+0.4841j, dtype=torch.complex128)

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[ 0.3142+4.6551j, -1.2112+4.1163j,  3.3540+5.5089j, -7.2121+6.9711j,
          8.1426-2.9480j],
        [-6.8702-4.2623j, -7.7593-5.2365j, -8.8576-7.6004j,  6.2623-3.3311j,
         -8.7005+3.7920j],
        [ 2.3469-2.9610j,  0.3075-6.7447j,  7.5576+6.0656j, -4.0985+3.3758j,
         -7.2926-4.7286j],
        [ 6.3172+5.4643j, -5.8316-2.1744j, -1.4822+8.6413j,  7.7615-5.2964j,
         -6.7219-2.1284j],
        [-3.1535+3.6208j,  0.1218-8.6433j,  3.5591-2.6182j,  6.1059-5.0949j,
         -0.0261+6.1196j],
        [ 2.9463+1.9439j,  6.0270-5.1074j,  4.7828-1.3698j, -1.6207+4.9976j,
          7.8116+2.7221j],
        [-5.6230+8.2865j, -5.5448+0.0601j, -2.2275+3.2058j, -3.4649-0.7534j,
         -2.0329+4.8251j],
        [-4.0793+3.5154j, -6.3153-2.1796j,  0.9348-1.2434j,  5.8404+8.6417j,
         -2.7541+8.8165j],
        [-4.1198-6.6074j,  5.1320+3.5038j, -8.5293+8.1905j,  6.9186+2.5708j,
         -6.0272+0.3845j],
        [-4.1917-1.3899j, -2.8770-8.5045j,  7.7401-0.7434j, -6.8162-7.5620j,
         -4.1208-7.9884j]], dtype=torch.complex128)
Analytical:
tensor([[ -3.1420e-01+4.6551e+00j, -9.4717e-311+4.4363e-311j,
         -9.4717e-311+4.4363e-311j, -9.4717e-311+4.4363e-311j,
         -1.5170e-305+6.8042e-306j],
        [  6.8702e+00-4.2623e+00j,  1.0669e-310-4.1919e-311j,
          1.0669e-310-4.1919e-311j,  1.0669e-310-4.1919e-311j,
          1.7066e-305-6.3848e-306j],
        [ -2.3469e+00-2.9610e+00j,  2.1898e-311-1.0266e-310j,
          2.1898e-311-1.0266e-310j,  2.1898e-311-1.0266e-310j,
          3.7478e-306-1.6260e-305j],
        [ -6.3172e+00+5.4643e+00j,  3.9230e-311-7.5584e-311j,
          3.9230e-311-7.5584e-311j,  3.9230e-311-7.5584e-311j,
          6.4322e-306-1.1911e-305j],
        [  3.1535e+00+3.6208e+00j,  5.2486e-311+5.2041e-311j,
          5.2486e-311+5.2041e-311j,  5.2486e-311+5.2041e-311j,
          8.2063e-306+8.4080e-306j],
        [ -2.9463e+00+1.9439e+00j, -4.3466e-311+8.9960e-311j,
         -4.3466e-311+8.9960e-311j, -4.3466e-311+8.9960e-311j,
         -7.1429e-306+1.4185e-305j],
        [  5.6230e+00+8.2865e+00j,  5.8569e-311+2.3846e-311j,
          5.8569e-311+2.3846e-311j,  5.8569e-311+2.3846e-311j,
          9.2467e-306+3.9426e-306j],
        [  4.0793e+00+3.5154e+00j,  9.8816e-311+5.1775e-311j,
          9.8816e-311+5.1775e-311j,  9.8816e-311+5.1775e-311j,
          1.5571e-305+8.4864e-306j],
        [  4.1198e+00-6.6074e+00j,  5.4757e-311-4.8190e-311j,
          5.4757e-311-4.8190e-311j,  5.4757e-311-4.8190e-311j,
          8.8287e-306-7.5166e-306j],
        [  4.1917e+00-1.3899e+00j, -3.3030e-311-1.0342e-310j,
         -3.3030e-311-1.0342e-310j, -3.3030e-311-1.0342e-310j,
         -4.9804e-306-1.6523e-305j]], dtype=torch.complex128)

The max per-element difference (slow mode) is: 13.740387730898426.</failure>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_matmul_cpu_float64" time="0.039" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_matrix_exp_cpu_complex128" time="0.058" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_matrix_exp_cpu_float64" time="0.012" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_max_binary_cpu_float64" time="0.037" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_max_pool2d_with_indices_backward_cpu_float64" time="4.442" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_max_reduction_no_dim_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_max_reduction_with_dim_cpu_float64" time="0.008" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_maximum_cpu_float64" time="0.025" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_mean_cpu_complex128" time="0.147" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_mean_cpu_float64" time="0.031" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_median_cpu_float64" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_meshgrid_list_of_tensors_cpu_complex128" time="0.259" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_meshgrid_list_of_tensors_cpu_float64" time="0.053" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_meshgrid_variadic_tensors_cpu_complex128" time="0.264" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_meshgrid_variadic_tensors_cpu_float64" time="0.052" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_min_binary_cpu_float64" time="0.024" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_min_reduction_no_dim_cpu_float64" time="0.004" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_min_reduction_with_dim_cpu_float64" time="0.011" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_minimum_cpu_float64" time="0.025" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_mm_cpu_complex128" time="0.073" file="test_ops_gradients.py">
      <failure message="torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 0 with respect to input 0,&#10;numerical:tensor(-0.4563+1.2114j, dtype=torch.complex128)&#10;analytical:tensor(1.0389+2.9091j, dtype=torch.complex128)&#10;&#10;The above quantities relating the numerical and analytical jacobians are computed &#10;in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background &#10;about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:&#10;&#10;Numerical:&#10; tensor([[-3.4853-3.0241j,  8.9239-5.8371j,  3.4400+3.2711j,  ...,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],&#10;        [-1.8386-8.7061j, -2.2313+2.7661j, -0.0289+8.9090j,  ...,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 4.5771-1.4682j,  5.8989+4.0793j, -6.8950-1.9867j,  ...,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],&#10;        ...,&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,&#10;          5.8062-6.1318j, -0.9727+1.0767j, -8.8267-2.8697j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,&#10;          8.3572-3.7276j, -4.1167+0.2597j,  7.6591-2.3304j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,&#10;         -8.6721+6.8627j,  5.4461-6.2506j,  4.2591+4.7654j]],&#10; dtype=torch.complex128)&#10;Analytical:&#10;tensor([[ 3.4853-3.0241j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 1.8386-8.7061j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],&#10;        [-4.5771-1.4682j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],&#10;        ...,&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  8.8267-2.8697j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,&#10;          0.0000+0.0000j,  0.0000+0.0000j, -7.6591-2.3304j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,&#10;          0.0000+0.0000j,  0.0000+0.0000j, -4.2591+4.7654j]],&#10; dtype=torch.complex128)&#10;&#10;The max per-element difference (slow mode) is: 17.847893447383544.">Traceback (most recent call last):
  File "C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py", line 26, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 4335, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 4304, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 3859, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1536, in gradcheck
    return _gradcheck_helper(**args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1550, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1157, in _gradcheck_real_imag
    gradcheck_fn(imag_fn, imag_func_out, tupled_inputs, imag_outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1421, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1389, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(-0.4563+1.2114j, dtype=torch.complex128)
analytical:tensor(1.0389+2.9091j, dtype=torch.complex128)

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[-3.4853-3.0241j,  8.9239-5.8371j,  3.4400+3.2711j,  ...,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],
        [-1.8386-8.7061j, -2.2313+2.7661j, -0.0289+8.9090j,  ...,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],
        [ 4.5771-1.4682j,  5.8989+4.0793j, -6.8950-1.9867j,  ...,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],
        ...,
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,
          5.8062-6.1318j, -0.9727+1.0767j, -8.8267-2.8697j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,
          8.3572-3.7276j, -4.1167+0.2597j,  7.6591-2.3304j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,
         -8.6721+6.8627j,  5.4461-6.2506j,  4.2591+4.7654j]], dtype=torch.complex128)
Analytical:
tensor([[ 3.4853-3.0241j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],
        [ 1.8386-8.7061j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],
        [-4.5771-1.4682j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],
        ...,
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,
          0.0000+0.0000j,  0.0000+0.0000j,  8.8267-2.8697j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,
          0.0000+0.0000j,  0.0000+0.0000j, -7.6591-2.3304j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,
          0.0000+0.0000j,  0.0000+0.0000j, -4.2591+4.7654j]], dtype=torch.complex128)

The max per-element difference (slow mode) is: 17.847893447383544.</failure>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_mm_cpu_float64" time="0.004" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_mode_cpu_float64" time="0.014" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_movedim_cpu_complex128" time="0.016" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_movedim_cpu_float64" time="0.004" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_msort_cpu_float64" time="0.015" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_mul_cpu_complex128" time="0.105" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_mul_cpu_float64" time="0.021" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_multinomial_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_mv_cpu_complex128" time="0.013" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_mv_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_mvlgamma_mvlgamma_p_1_cpu_float64" time="0.018" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_mvlgamma_mvlgamma_p_3_cpu_float64" time="0.018" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_mvlgamma_mvlgamma_p_5_cpu_float64" time="0.018" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nan_to_num_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nanmean_cpu_float64" time="0.062" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nanmedian_cpu_float64" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nanquantile_cpu_float64" time="0.208" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nansum_cpu_float64" time="0.053" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_narrow_copy_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_narrow_copy_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_narrow_cpu_complex128" time="0.071" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_narrow_cpu_float64" time="0.016" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_native_batch_norm_cpu_float64" time="0.035" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_native_dropout_backward_cpu_float64" time="0.016" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_native_layer_norm_cpu_float64" time="0.050" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_ne_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_ne_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_neg_cpu_complex128" time="0.009" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_neg_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_new_empty_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_new_empty_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_new_empty_strided_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_new_empty_strided_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_new_full_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_new_full_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_new_ones_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_new_ones_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_new_zeros_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_new_zeros_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nextafter_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_adaptive_avg_pool1d_cpu_float64" time="0.032" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_adaptive_avg_pool2d_cpu_float64" time="0.027" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_adaptive_avg_pool3d_cpu_float64" time="0.037" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_adaptive_max_pool1d_cpu_float64" time="0.024" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_adaptive_max_pool2d_cpu_float64" time="0.060" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_adaptive_max_pool3d_cpu_float64" time="0.053" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_alpha_dropout_cpu_float64" time="0.176" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_avg_pool1d_cpu_float64" time="0.018" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_avg_pool2d_cpu_float64" time="0.013" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_avg_pool3d_cpu_float64" time="0.014" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_batch_norm_cpu_float64" time="0.049" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_bilinear_cpu_float64" time="0.339" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_binary_cross_entropy_cpu_float64" time="0.197" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_binary_cross_entropy_with_logits_cpu_float64" time="0.071" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_celu_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_conv1d_cpu_complex128" time="0.458" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_conv1d_cpu_float64" time="0.038" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_conv2d_cpu_complex128" time="2.841" file="test_ops_gradients.py">
      <system-err>c:\users\radekbarton\projects\pytorch\torch\testing\_internal\opinfo\core.py:766: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at C:\Users\radekbarton\Projects\pytorch\aten\src\ATen\native\Convolution.cpp:1004.)
  gradcheck_wrapper: Callable = lambda op, *args, **kwargs: op(*args, **kwargs)
      </system-err>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_conv2d_cpu_float64" time="0.194" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_conv_transpose1d_cpu_complex128" time="0.603" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_conv_transpose1d_cpu_float64" time="0.041" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_conv_transpose2d_cpu_complex128" time="0.703" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_conv_transpose2d_cpu_float64" time="0.047" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_conv_transpose3d_cpu_complex128" time="0.936" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_conv_transpose3d_cpu_float64" time="0.077" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_cosine_embedding_loss_cpu_float64" time="0.056" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_cosine_similarity_cpu_float64" time="0.040" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_cross_entropy_cpu_float64" time="0.076" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_ctc_loss_cpu_float64" time="22.692" file="test_ops_gradients.py">
      <skipped type="pytest.xfail" message="" />
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_dropout2d_cpu_float64" time="0.064" file="test_ops_gradients.py">
      <system-err>c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
      </system-err>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_dropout3d_cpu_float64" time="0.069" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_dropout_cpu_float64" time="0.086" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_elu_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_embedding_bag_cpu_float64" time="0.109" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_embedding_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_feature_alpha_dropout_with_train_cpu_float64" time="0.038" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_feature_alpha_dropout_without_train_cpu_complex128" time="0.413" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_feature_alpha_dropout_without_train_cpu_float64" time="0.071" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_fractional_max_pool2d_cpu_float64" time="0.528" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_fractional_max_pool3d_cpu_float64" time="0.286" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_gaussian_nll_loss_cpu_float64" time="1.504" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_gelu_cpu_float64" time="0.013" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_glu_cpu_float64" time="0.079" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_grid_sample_cpu_float64" time="0.121" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_group_norm_cpu_float64" time="0.106" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_hardshrink_cpu_float64" time="0.015" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_hardsigmoid_cpu_float64" time="0.008" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_hardswish_cpu_float64" time="0.011" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_hardtanh_cpu_float64" time="0.015" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_hinge_embedding_loss_cpu_float64" time="0.050" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_huber_loss_cpu_float64" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_instance_norm_cpu_float64" time="0.037" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_interpolate_area_cpu_float64" time="0.034" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_interpolate_bicubic_cpu_float64" time="0.034" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_interpolate_bilinear_cpu_float64" time="0.030" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_interpolate_linear_cpu_float64" time="0.029" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_interpolate_nearest_cpu_float64" time="0.032" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_interpolate_trilinear_cpu_float64" time="0.036" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_kl_div_cpu_float64" time="0.045" file="test_ops_gradients.py">
      <system-err>c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:2924: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:2924: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:2924: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:2924: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:2924: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:2924: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
      </system-err>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_l1_loss_cpu_complex128" time="0.057" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_l1_loss_cpu_float64" time="0.016" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_layer_norm_cpu_float64" time="0.018" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_leaky_relu_cpu_float64" time="0.015" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_linear_cpu_complex128" time="0.264" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_linear_cpu_float64" time="0.046" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_local_response_norm_cpu_float64" time="0.032" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_logsigmoid_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_margin_ranking_loss_cpu_float64" time="0.049" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_max_pool1d_cpu_float64" time="3.378" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_max_pool2d_cpu_float64" time="4.195" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_max_pool3d_cpu_float64" time="1.817" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_max_unpool1d_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_max_unpool1d_grad_cpu_float64" time="0.072" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_max_unpool2d_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_max_unpool2d_grad_cpu_float64" time="0.101" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_max_unpool3d_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_max_unpool3d_grad_cpu_float64" time="0.270" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_mish_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_mse_loss_cpu_float64" time="0.015" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_multi_margin_loss_cpu_float64" time="0.009" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_multilabel_margin_loss_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_multilabel_soft_margin_loss_cpu_float64" time="0.014" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_nll_loss_cpu_float64" time="0.094" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_normalize_cpu_complex128" time="0.089" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_normalize_cpu_float64" time="0.019" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_pad_circular_cpu_complex128" time="0.074" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_pad_circular_cpu_float64" time="0.016" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_pad_constant_cpu_complex128" time="0.358" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_pad_constant_cpu_float64" time="0.079" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_pad_reflect_cpu_complex128" time="0.089" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_pad_reflect_cpu_float64" time="0.019" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_pad_replicate_cpu_complex128" time="0.086" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_pad_replicate_cpu_float64" time="0.019" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_pairwise_distance_cpu_complex128" time="0.048" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_pairwise_distance_cpu_float64" time="0.019" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_pdist_cpu_float64" time="0.015" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_pixel_shuffle_cpu_complex128" time="0.035" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_pixel_shuffle_cpu_float64" time="0.008" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_pixel_unshuffle_cpu_complex128" time="0.035" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_pixel_unshuffle_cpu_float64" time="0.008" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_poisson_nll_loss_cpu_float64" time="0.235" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_prelu_cpu_float64" time="0.055" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_relu6_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_relu_cpu_float64" time="0.007" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_rrelu_cpu_float64" time="0.023" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_scaled_dot_product_attention_cpu_float64" time="0.212" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_selu_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_silu_complex_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_silu_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_smooth_l1_loss_cpu_float64" time="0.020" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_soft_margin_loss_cpu_float64" time="0.012" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_softmin_cpu_float64" time="0.012" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_softmin_with_dtype_cpu_complex128" time="0.032" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_softmin_with_dtype_cpu_float64" time="0.012" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_softplus_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_softshrink_cpu_float64" time="0.008" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_softsign_cpu_complex128" time="0.029" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_softsign_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_tanhshrink_cpu_complex128" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_tanhshrink_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_threshold_cpu_float64" time="0.007" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_triplet_margin_loss_cpu_complex128" time="0.096" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_triplet_margin_loss_cpu_float64" time="0.033" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_triplet_margin_with_distance_loss_cpu_complex128" time="0.105" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_triplet_margin_with_distance_loss_cpu_float64" time="0.035" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_unfold_cpu_complex128" time="1.572" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_unfold_cpu_float64" time="0.478" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_upsample_bilinear_cpu_float64" time="0.008" file="test_ops_gradients.py">
      <system-err>c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4084: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4084: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4084: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4084: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4084: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.")
      </system-err>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nn_functional_upsample_nearest_cpu_float64" time="0.021" file="test_ops_gradients.py">
      <system-err>c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
      </system-err>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nonzero_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_nonzero_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_norm_cpu_complex128" time="0.185" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_norm_cpu_float64" time="0.072" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_norm_fro_cpu_complex128" time="0.014" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_norm_fro_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_norm_inf_cpu_complex128" time="0.122" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_norm_inf_cpu_float64" time="0.032" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_norm_nuc_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_norm_nuc_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_normal_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Gradients are incorrect!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Gradients are incorrect!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_normal_in_place_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_normal_in_place_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_normal_number_mean_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Gradients are incorrect!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Gradients are incorrect!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_ones_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_ones_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_ones_like_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_ones_like_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_ormqr_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_ormqr_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_outer_cpu_complex128" time="0.014" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_outer_cpu_float64" time="0.004" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_pca_lowrank_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_permute_cpu_complex128" time="0.028" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_permute_cpu_float64" time="0.007" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_pinverse_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_pinverse_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_polar_cpu_float64" time="0.041" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_polygamma_polygamma_n_0_cpu_float64" time="0.016" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_polygamma_polygamma_n_1_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_polygamma_polygamma_n_2_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_polygamma_polygamma_n_3_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_polygamma_polygamma_n_4_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_positive_cpu_complex128" time="0.008" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_positive_cpu_float64" time="0.002" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_pow_cpu_complex128" time="0.138" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_pow_cpu_float64" time="0.026" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_prod_cpu_complex128" time="0.293" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_prod_cpu_float64" time="0.066" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_put_cpu_complex128" time="0.244" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_put_cpu_float64" time="0.048" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_qr_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_qr_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_quantile_cpu_float64" time="0.206" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_rad2deg_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_rand_like_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_rand_like_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_randint_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_randint_like_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_randn_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_randn_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_randn_like_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_randn_like_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_ravel_cpu_complex128" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_ravel_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_real_cpu_complex128" time="0.012" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_real_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_reciprocal_cpu_complex128" time="0.021" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_reciprocal_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_remainder_cpu_float64" time="0.023" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_renorm_cpu_complex128" time="0.042" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_renorm_cpu_float64" time="0.010" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_repeat_cpu_complex128" time="0.161" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_repeat_cpu_float64" time="0.035" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_repeat_interleave_cpu_complex128" time="0.128" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_repeat_interleave_cpu_float64" time="0.013" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_reshape_as_cpu_complex128" time="0.043" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_reshape_as_cpu_float64" time="0.010" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_reshape_cpu_complex128" time="0.064" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_reshape_cpu_float64" time="0.011" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_resize__cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_resize__cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_resize_as__cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_resize_as__cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_resolve_conj_cpu_complex128" time="0.013" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_resolve_conj_cpu_float64" time="0.004" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_resolve_neg_cpu_complex128" time="0.014" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_resolve_neg_cpu_float64" time="0.004" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_roll_cpu_complex128" time="0.119" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_roll_cpu_float64" time="0.027" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_rot90_cpu_complex128" time="0.244" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_rot90_cpu_float64" time="0.053" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_round_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_round_decimals_0_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_round_decimals_3_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_round_decimals_neg_3_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_rsqrt_cpu_complex128" time="0.021" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_rsqrt_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_rsub_cpu_complex128" time="0.123" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_rsub_cpu_float64" time="0.024" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_scalar_tensor_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_scalar_tensor_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_scatter_add_cpu_complex128" time="0.083" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_scatter_add_cpu_float64" time="0.017" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_scatter_cpu_complex128" time="0.090" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_scatter_cpu_float64" time="0.019" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_scatter_reduce_amax_cpu_float64" time="0.056" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_scatter_reduce_amin_cpu_float64" time="0.056" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_scatter_reduce_mean_cpu_float64" time="0.061" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_scatter_reduce_prod_cpu_float64" time="0.072" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_scatter_reduce_sum_cpu_float64" time="0.050" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_searchsorted_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_select_cpu_complex128" time="0.036" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_select_cpu_float64" time="0.008" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_select_scatter_cpu_float64" time="0.013" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_sgn_cpu_complex128" time="0.011" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_sgn_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_short_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_short_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_sigmoid_cpu_complex128" time="0.021" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_sigmoid_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_sign_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_signal_windows_bartlett_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_signal_windows_blackman_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_signal_windows_cosine_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_signal_windows_exponential_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_signal_windows_gaussian_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_signal_windows_general_cosine_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_signal_windows_general_hamming_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_signal_windows_hamming_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_signal_windows_hann_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_signal_windows_kaiser_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_signal_windows_nuttall_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_signbit_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_sin_cpu_complex128" time="0.011" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_sin_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_sinc_cpu_complex128" time="0.024" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_sinc_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_sinh_cpu_complex128" time="0.011" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_sinh_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_slice_cpu_complex128" time="0.094" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_slice_cpu_float64" time="0.017" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_slice_scatter_cpu_float64" time="0.040" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_softmax_cpu_float64" time="0.012" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_softmax_with_dtype_cpu_complex128" time="0.030" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_softmax_with_dtype_cpu_float64" time="0.011" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_sort_cpu_float64" time="0.104" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_sparse_mm_reduce_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_sparse_sampled_addmm_cpu_complex128" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_sparse_sampled_addmm_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_special_airy_ai_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_special_bessel_j0_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_special_bessel_j1_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_special_bessel_y0_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_special_bessel_y1_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_special_chebyshev_polynomial_t_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_special_chebyshev_polynomial_u_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_special_chebyshev_polynomial_v_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipping - testing takes an unreasonably long time, #79528">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipping - testing takes an unreasonably long time, #79528</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_special_chebyshev_polynomial_w_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipping - testing takes an unreasonably long time, #79528">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipping - testing takes an unreasonably long time, #79528</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_special_entr_cpu_float64" time="0.004" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_special_erfcx_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_special_hermite_polynomial_h_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_special_hermite_polynomial_he_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_special_i0e_cpu_float64" time="0.004" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_special_i1_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_special_i1e_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_special_laguerre_polynomial_l_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_special_legendre_polynomial_p_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipping - testing takes an unreasonably long time, #79528">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipping - testing takes an unreasonably long time, #79528</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_special_log_ndtr_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_special_modified_bessel_i0_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_special_modified_bessel_i1_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_special_modified_bessel_k0_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_special_modified_bessel_k1_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_special_ndtr_cpu_float64" time="0.007" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_special_ndtri_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_special_polygamma_special_polygamma_n_0_cpu_float64" time="0.016" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_special_scaled_modified_bessel_k0_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_special_scaled_modified_bessel_k1_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_special_shifted_chebyshev_polynomial_t_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipping - testing takes an unreasonably long time, #79528">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipping - testing takes an unreasonably long time, #79528</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_special_shifted_chebyshev_polynomial_u_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipping - testing takes an unreasonably long time, #79528">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipping - testing takes an unreasonably long time, #79528</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_special_shifted_chebyshev_polynomial_v_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipping - testing takes an unreasonably long time, #79528">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipping - testing takes an unreasonably long time, #79528</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_special_shifted_chebyshev_polynomial_w_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipping - testing takes an unreasonably long time, #79528">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipping - testing takes an unreasonably long time, #79528</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_special_spherical_bessel_j0_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_special_xlog1py_cpu_float64" time="0.025" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_special_zeta_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_split_cpu_complex128" time="0.027" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_split_cpu_float64" time="0.007" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_split_list_args_cpu_complex128" time="0.056" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_split_list_args_cpu_float64" time="0.013" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_split_with_sizes_cpu_complex128" time="0.072" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_split_with_sizes_cpu_float64" time="0.017" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_sqrt_cpu_complex128" time="0.010" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_sqrt_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_square_cpu_complex128" time="0.021" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_square_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_squeeze_cpu_complex128" time="0.053" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_squeeze_cpu_float64" time="0.012" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_squeeze_multiple_cpu_complex128" time="0.040" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_squeeze_multiple_cpu_float64" time="0.009" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_stack_cpu_complex128" time="0.111" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_stack_cpu_float64" time="0.020" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_std_cpu_complex128" time="0.052" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_std_cpu_float64" time="0.020" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_std_mean_cpu_complex128" time="0.140" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_std_mean_cpu_float64" time="0.034" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_std_mean_unbiased_cpu_complex128" time="0.028" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_std_mean_unbiased_cpu_float64" time="0.008" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_std_unbiased_cpu_complex128" time="0.011" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_std_unbiased_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_stft_cpu_complex128" time="0.110" file="test_ops_gradients.py">
      <system-err>c:\users\radekbarton\projects\pytorch\torch\functional.py:642: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.
Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at C:\Users\radekbarton\Projects\pytorch\aten\src\ATen\native\SpectralOps.cpp:867.)
  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]
      </system-err>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_stft_cpu_float64" time="0.043" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_sub_cpu_complex128" time="0.136" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_sub_cpu_float64" time="0.024" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_sum_cpu_complex128" time="0.133" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_sum_cpu_float64" time="0.028" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_sum_to_size_cpu_complex128" time="0.105" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_sum_to_size_cpu_float64" time="0.023" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_svd_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_svd_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_svd_lowrank_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_t_cpu_complex128" time="0.020" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_t_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_take_along_dim_cpu_complex128" time="0.039" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_take_along_dim_cpu_float64" time="0.010" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_take_cpu_complex128" time="0.056" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_take_cpu_float64" time="0.013" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_tan_cpu_complex128" time="0.010" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_tan_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_tanh_cpu_complex128" time="0.010" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_tanh_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_tensor_split_cpu_complex128" time="0.183" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_tensor_split_cpu_float64" time="0.043" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_tensordot_cpu_complex128" time="0.023" file="test_ops_gradients.py">
      <failure message="torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 0 with respect to input 0,&#10;numerical:tensor(-1.4482+9.2472j, dtype=torch.complex128)&#10;analytical:tensor(1.0818+9.4837j, dtype=torch.complex128)&#10;&#10;The above quantities relating the numerical and analytical jacobians are computed &#10;in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background &#10;about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:&#10;&#10;Numerical:&#10; tensor([[-3.0004+8.7504j, -6.3773+8.5671j,  0.0000+0.0000j,  0.0000+0.0000j],&#10;        [-0.1426+6.0237j,  5.3611-0.0862j,  0.0000+0.0000j,  0.0000+0.0000j],&#10;        [-1.8770+7.2619j, -6.6831-1.0867j,  0.0000+0.0000j,  0.0000+0.0000j],&#10;        [-1.3645+0.2022j,  3.8557+3.8706j,  0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j, -3.0004+8.7504j, -6.3773+8.5671j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j, -0.1426+6.0237j,  5.3611-0.0862j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j, -1.8770+7.2619j, -6.6831-1.0867j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j, -1.3645+0.2022j,  3.8557+3.8706j]],&#10; dtype=torch.complex128)&#10;Analytical:&#10;tensor([[ 3.0004+8.7504j,  0.0000+0.0000j,  6.3773+8.5671j,  0.0000+0.0000j],&#10;        [ 0.1426+6.0237j,  0.0000+0.0000j, -5.3611-0.0862j,  0.0000+0.0000j],&#10;        [ 1.8770+7.2619j,  0.0000+0.0000j,  6.6831-1.0867j,  0.0000+0.0000j],&#10;        [ 1.3645+0.2022j,  0.0000+0.0000j, -3.8557+3.8706j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  3.0004+8.7504j,  0.0000+0.0000j,  6.3773+8.5671j],&#10;        [ 0.0000+0.0000j,  0.1426+6.0237j,  0.0000+0.0000j, -5.3611-0.0862j],&#10;        [ 0.0000+0.0000j,  1.8770+7.2619j,  0.0000+0.0000j,  6.6831-1.0867j],&#10;        [ 0.0000+0.0000j,  1.3645+0.2022j,  0.0000+0.0000j, -3.8557+3.8706j]],&#10; dtype=torch.complex128)&#10;&#10;The max per-element difference (slow mode) is: 13.366194590564849.">Traceback (most recent call last):
  File "C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py", line 26, in test_fn_grad
    self._grad_test_helper(device, dtype, op, op.get_op())
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 4335, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 4304, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 3859, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1536, in gradcheck
    return _gradcheck_helper(**args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1550, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1157, in _gradcheck_real_imag
    gradcheck_fn(imag_fn, imag_func_out, tupled_inputs, imag_outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1421, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1389, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 0 with respect to input 0,
numerical:tensor(-1.4482+9.2472j, dtype=torch.complex128)
analytical:tensor(1.0818+9.4837j, dtype=torch.complex128)

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[-3.0004+8.7504j, -6.3773+8.5671j,  0.0000+0.0000j,  0.0000+0.0000j],
        [-0.1426+6.0237j,  5.3611-0.0862j,  0.0000+0.0000j,  0.0000+0.0000j],
        [-1.8770+7.2619j, -6.6831-1.0867j,  0.0000+0.0000j,  0.0000+0.0000j],
        [-1.3645+0.2022j,  3.8557+3.8706j,  0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j, -3.0004+8.7504j, -6.3773+8.5671j],
        [ 0.0000+0.0000j,  0.0000+0.0000j, -0.1426+6.0237j,  5.3611-0.0862j],
        [ 0.0000+0.0000j,  0.0000+0.0000j, -1.8770+7.2619j, -6.6831-1.0867j],
        [ 0.0000+0.0000j,  0.0000+0.0000j, -1.3645+0.2022j,  3.8557+3.8706j]], dtype=torch.complex128)
Analytical:
tensor([[ 3.0004+8.7504j,  0.0000+0.0000j,  6.3773+8.5671j,  0.0000+0.0000j],
        [ 0.1426+6.0237j,  0.0000+0.0000j, -5.3611-0.0862j,  0.0000+0.0000j],
        [ 1.8770+7.2619j,  0.0000+0.0000j,  6.6831-1.0867j,  0.0000+0.0000j],
        [ 1.3645+0.2022j,  0.0000+0.0000j, -3.8557+3.8706j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  3.0004+8.7504j,  0.0000+0.0000j,  6.3773+8.5671j],
        [ 0.0000+0.0000j,  0.1426+6.0237j,  0.0000+0.0000j, -5.3611-0.0862j],
        [ 0.0000+0.0000j,  1.8770+7.2619j,  0.0000+0.0000j,  6.6831-1.0867j],
        [ 0.0000+0.0000j,  1.3645+0.2022j,  0.0000+0.0000j, -3.8557+3.8706j]], dtype=torch.complex128)

The max per-element difference (slow mode) is: 13.366194590564849.</failure>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_tensordot_cpu_float64" time="0.007" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_tile_cpu_complex128" time="0.186" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_tile_cpu_float64" time="0.040" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_to_cpu_complex128" time="0.114" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_to_cpu_float64" time="0.042" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_to_sparse_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_to_sparse_cpu_float64" time="0.026" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_topk_cpu_float64" time="0.024" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_trace_cpu_complex128" time="0.008" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_trace_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_transpose_cpu_complex128" time="0.055" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_transpose_cpu_float64" time="0.012" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_trapezoid_cpu_complex128" time="0.124" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_trapezoid_cpu_float64" time="0.027" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_trapz_cpu_complex128" time="0.127" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_trapz_cpu_float64" time="0.027" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_triangular_solve_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_triangular_solve_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_tril_cpu_complex128" time="0.125" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_tril_cpu_float64" time="0.021" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_triu_cpu_complex128" time="0.089" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_triu_cpu_float64" time="0.020" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_true_divide_cpu_complex128" time="0.139" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_true_divide_cpu_float64" time="0.023" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_trunc_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_unbind_cpu_complex128" time="0.174" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_unbind_cpu_float64" time="0.044" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_unflatten_cpu_complex128" time="0.066" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_unflatten_cpu_float64" time="0.015" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_unfold_copy_cpu_complex128" time="0.180" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_unfold_copy_cpu_float64" time="0.039" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_unfold_cpu_complex128" time="0.176" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_unfold_cpu_float64" time="0.038" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_uniform_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_uniform_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_unique_consecutive_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_unique_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_unsafe_split_cpu_complex128" time="0.027" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_unsafe_split_cpu_float64" time="0.007" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_unsqueeze_cpu_complex128" time="0.061" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_unsqueeze_cpu_float64" time="0.013" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_var_cpu_complex128" time="0.048" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_var_cpu_float64" time="0.019" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_var_mean_cpu_complex128" time="0.134" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_var_mean_cpu_float64" time="0.031" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_var_mean_unbiased_cpu_complex128" time="0.027" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_var_mean_unbiased_cpu_float64" time="0.007" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_var_unbiased_cpu_complex128" time="0.010" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_var_unbiased_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_vdot_cpu_complex128" time="0.024" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_vdot_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_view_as_complex_cpu_float64" time="0.004" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_view_as_cpu_complex128" time="0.028" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_view_as_cpu_float64" time="0.007" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_view_as_real_cpu_complex128" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_view_copy_cpu_float64" time="0.011" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_view_cpu_complex128" time="0.050" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_view_cpu_float64" time="0.011" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_vsplit_cpu_complex128" time="0.037" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_vsplit_cpu_float64" time="0.009" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_vstack_cpu_complex128" time="0.035" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_vstack_cpu_float64" time="0.007" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_where_cpu_complex128" time="0.072" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_where_cpu_float64" time="0.016" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_xlogy_cpu_float64" time="0.023" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_zero__cpu_complex128" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_zero__cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_zeros_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_zeros_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_zeros_like_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_grad_zeros_like_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Dtype is not in supported backward dtypes!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:20: Skipped! Dtype is not in supported backward dtypes!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_H_cpu_complex128" time="0.076" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_H_cpu_float64" time="0.011" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_T_cpu_complex128" time="0.058" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_T_cpu_float64" time="0.011" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad___getitem___cpu_complex128" time="0.644" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad___getitem___cpu_float64" time="0.098" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad___radd___cpu_complex128" time="0.535" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad___radd___cpu_float64" time="0.096" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad___rdiv___cpu_complex128" time="0.790" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad___rdiv___cpu_float64" time="0.135" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad___rmatmul___cpu_complex128" time="0.177" file="test_ops_gradients.py">
      <failure message="torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 1 with respect to input 0,&#10;numerical:tensor(26.5831+19.8910j, dtype=torch.complex128)&#10;analytical:tensor(0.0033-0.4287j, dtype=torch.complex128)&#10;&#10;The above quantities relating the numerical and analytical jacobians are computed &#10;in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background &#10;about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:&#10;&#10;Numerical:&#10; tensor([[ 0.3566-0.9534j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [60.9942+62.2642j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.3566-0.9534j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j, 60.9942+62.2642j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.3566-0.9534j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j, 60.9942+62.2642j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.3566-0.9534j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j, 60.9942+62.2642j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.3566-0.9534j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;         60.9942+62.2642j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.3566-0.9534j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j, 60.9942+62.2642j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.3566-0.9534j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j, 60.9942+62.2642j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.3566-0.9534j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j, 60.9942+62.2642j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.3566-0.9534j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;         60.9942+62.2642j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.3566-0.9534j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j, 60.9942+62.2642j]], dtype=torch.complex128)&#10;Analytical:&#10;tensor([[ 0.3566+0.9534j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.9539-0.9793j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [-0.3248-0.6213j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.1479-0.6155j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [-0.8195-0.1612j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.3566+0.9534j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.9539-0.9793j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j, -0.3248-0.6213j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.1479-0.6155j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j, -0.8195-0.1612j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.3566+0.9534j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.9539-0.9793j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j, -0.3248-0.6213j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.1479-0.6155j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j, -0.8195-0.1612j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.3566+0.9534j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.9539-0.9793j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j, -0.3248-0.6213j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.1479-0.6155j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j, -0.8195-0.1612j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.3566+0.9534j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.9539-0.9793j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;         -0.3248-0.6213j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.1479-0.6155j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;         -0.8195-0.1612j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.3566+0.9534j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.9539-0.9793j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j, -0.3248-0.6213j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.1479-0.6155j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j, -0.8195-0.1612j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.3566+0.9534j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.9539-0.9793j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j, -0.3248-0.6213j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.1479-0.6155j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j, -0.8195-0.1612j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.3566+0.9534j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.9539-0.9793j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j, -0.3248-0.6213j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.1479-0.6155j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j, -0.8195-0.1612j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.3566+0.9534j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.9539-0.9793j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;         -0.3248-0.6213j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.1479-0.6155j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;         -0.8195-0.1612j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.3566+0.9534j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.9539-0.9793j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j, -0.3248-0.6213j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.1479-0.6155j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j, -0.8195-0.1612j]], dtype=torch.complex128)&#10;&#10;The max per-element difference (slow mode) is: 87.85130591039845.">Traceback (most recent call last):
  File "C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py", line 60, in test_fn_gradgrad
    self._check_helper(device, dtype, op, op.get_op(), 'bwgrad_bwgrad')
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 4329, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args, **kwargs))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 3879, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1694, in gradgradcheck
    return gradcheck(
           ^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1536, in gradcheck
    return _gradcheck_helper(**args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1550, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1157, in _gradcheck_real_imag
    gradcheck_fn(imag_fn, imag_func_out, tupled_inputs, imag_outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1421, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1389, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 1 with respect to input 0,
numerical:tensor(26.5831+19.8910j, dtype=torch.complex128)
analytical:tensor(0.0033-0.4287j, dtype=torch.complex128)

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[ 0.3566-0.9534j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [60.9942+62.2642j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.3566-0.9534j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j, 60.9942+62.2642j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.3566-0.9534j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j, 60.9942+62.2642j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.3566-0.9534j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j, 60.9942+62.2642j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.3566-0.9534j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
         60.9942+62.2642j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.3566-0.9534j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j, 60.9942+62.2642j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.3566-0.9534j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j, 60.9942+62.2642j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.3566-0.9534j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j, 60.9942+62.2642j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.3566-0.9534j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
         60.9942+62.2642j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.3566-0.9534j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j, 60.9942+62.2642j]], dtype=torch.complex128)
Analytical:
tensor([[ 0.3566+0.9534j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.9539-0.9793j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [-0.3248-0.6213j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.1479-0.6155j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [-0.8195-0.1612j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.3566+0.9534j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.9539-0.9793j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j, -0.3248-0.6213j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.1479-0.6155j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j, -0.8195-0.1612j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.3566+0.9534j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.9539-0.9793j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j, -0.3248-0.6213j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.1479-0.6155j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j, -0.8195-0.1612j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.3566+0.9534j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.9539-0.9793j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j, -0.3248-0.6213j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.1479-0.6155j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j, -0.8195-0.1612j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.3566+0.9534j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.9539-0.9793j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
         -0.3248-0.6213j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.1479-0.6155j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
         -0.8195-0.1612j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.3566+0.9534j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.9539-0.9793j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j, -0.3248-0.6213j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.1479-0.6155j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j, -0.8195-0.1612j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.3566+0.9534j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.9539-0.9793j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j, -0.3248-0.6213j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.1479-0.6155j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j, -0.8195-0.1612j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.3566+0.9534j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.9539-0.9793j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j, -0.3248-0.6213j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.1479-0.6155j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j, -0.8195-0.1612j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.3566+0.9534j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.9539-0.9793j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
         -0.3248-0.6213j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.1479-0.6155j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
         -0.8195-0.1612j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.3566+0.9534j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.9539-0.9793j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j, -0.3248-0.6213j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.1479-0.6155j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j, -0.8195-0.1612j]], dtype=torch.complex128)

The max per-element difference (slow mode) is: 87.85130591039845.</failure>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad___rmatmul___cpu_float64" time="0.330" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad___rmod___cpu_float64" time="0.176" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad___rmul___cpu_complex128" time="0.756" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad___rmul___cpu_float64" time="0.137" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad___rpow___cpu_complex128" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad___rpow___cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad___rsub___cpu_complex128" time="0.587" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad___rsub___cpu_float64" time="0.129" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad__native_batch_norm_legit_cpu_float64" time="0.244" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad__segment_reduce_lengths_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op claims it doesn't support gradgrad. This is not verified.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Op claims it doesn't support gradgrad. This is not verified.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad__segment_reduce_offsets_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op claims it doesn't support gradgrad. This is not verified.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Op claims it doesn't support gradgrad. This is not verified.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad__softmax_backward_data_cpu_float64" time="0.017" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad__upsample_bilinear2d_aa_cpu_float64" time="0.043" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_abs_cpu_complex128" time="0.034" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_abs_cpu_float64" time="0.007" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_acos_cpu_complex128" time="0.121" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_acos_cpu_float64" time="0.023" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_acosh_cpu_complex128" time="0.123" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_acosh_cpu_float64" time="0.019" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_add_cpu_complex128" time="0.638" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_add_cpu_float64" time="0.118" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_addbmm_cpu_complex128" time="2.885" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_addbmm_cpu_float64" time="0.153" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_addcdiv_cpu_complex128" time="1.752" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_addcdiv_cpu_float64" time="0.308" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_addcmul_cpu_complex128" time="1.527" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_addcmul_cpu_float64" time="0.277" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_addmm_cpu_complex128" time="0.057" file="test_ops_gradients.py">
      <failure message="torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 2 with respect to input 1,&#10;numerical:tensor(-0.1847-2.1866j, dtype=torch.complex128)&#10;analytical:tensor(1.2251+1.1312j, dtype=torch.complex128)&#10;&#10;The above quantities relating the numerical and analytical jacobians are computed &#10;in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background &#10;about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:&#10;&#10;Numerical:&#10; tensor([[  1.4334e+00+1.9862e+00j,   1.1185e+00-4.1834e+00j,&#10;         -2.7788e-306+1.9068e-306j,   0.0000e+00+0.0000e+00j,&#10;           0.0000e+00+0.0000e+00j,   0.0000e+00+0.0000e+00j],&#10;        [  0.0000e+00+0.0000e+00j,   0.0000e+00+0.0000e+00j,&#10;           0.0000e+00+0.0000e+00j,   1.4334e+00+1.9862e+00j,&#10;           1.1185e+00-4.1834e+00j, -2.7788e-306+1.9068e-306j],&#10;        [ -2.5487e+00+1.4633e+00j,   4.8637e-02-1.9648e+00j,&#10;         -1.8691e-306+1.2461e-306j,   0.0000e+00+0.0000e+00j,&#10;           0.0000e+00+0.0000e+00j,   0.0000e+00+0.0000e+00j],&#10;        [  0.0000e+00+0.0000e+00j,   0.0000e+00+0.0000e+00j,&#10;           0.0000e+00+0.0000e+00j,  -2.5487e+00+1.4633e+00j,&#10;           4.8637e-02-1.9648e+00j, -1.8691e-306+1.2461e-306j]],&#10; dtype=torch.complex128)&#10;Analytical:&#10;tensor([[ 1.4334-1.9862j, -2.5487-1.4633j,  1.9870-1.7309j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  1.4334-1.9862j,&#10;         -2.5487-1.4633j,  1.9870-1.7309j],&#10;        [ 1.1185+4.1834j,  0.0486+1.9648j,  1.9194+2.2038j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  1.1185+4.1834j,&#10;          0.0486+1.9648j,  1.9194+2.2038j]], dtype=torch.complex128)&#10;&#10;The max per-element difference (slow mode) is: 4.565870490719988.">Traceback (most recent call last):
  File "C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py", line 60, in test_fn_gradgrad
    self._check_helper(device, dtype, op, op.get_op(), 'bwgrad_bwgrad')
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 4329, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args, **kwargs))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 3879, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1694, in gradgradcheck
    return gradcheck(
           ^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1536, in gradcheck
    return _gradcheck_helper(**args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1550, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1157, in _gradcheck_real_imag
    gradcheck_fn(imag_fn, imag_func_out, tupled_inputs, imag_outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1421, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1389, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 2 with respect to input 1,
numerical:tensor(-0.1847-2.1866j, dtype=torch.complex128)
analytical:tensor(1.2251+1.1312j, dtype=torch.complex128)

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[  1.4334e+00+1.9862e+00j,   1.1185e+00-4.1834e+00j,
         -2.7788e-306+1.9068e-306j,   0.0000e+00+0.0000e+00j,
           0.0000e+00+0.0000e+00j,   0.0000e+00+0.0000e+00j],
        [  0.0000e+00+0.0000e+00j,   0.0000e+00+0.0000e+00j,
           0.0000e+00+0.0000e+00j,   1.4334e+00+1.9862e+00j,
           1.1185e+00-4.1834e+00j, -2.7788e-306+1.9068e-306j],
        [ -2.5487e+00+1.4633e+00j,   4.8637e-02-1.9648e+00j,
         -1.8691e-306+1.2461e-306j,   0.0000e+00+0.0000e+00j,
           0.0000e+00+0.0000e+00j,   0.0000e+00+0.0000e+00j],
        [  0.0000e+00+0.0000e+00j,   0.0000e+00+0.0000e+00j,
           0.0000e+00+0.0000e+00j,  -2.5487e+00+1.4633e+00j,
           4.8637e-02-1.9648e+00j, -1.8691e-306+1.2461e-306j]], dtype=torch.complex128)
Analytical:
tensor([[ 1.4334-1.9862j, -2.5487-1.4633j,  1.9870-1.7309j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  1.4334-1.9862j,
         -2.5487-1.4633j,  1.9870-1.7309j],
        [ 1.1185+4.1834j,  0.0486+1.9648j,  1.9194+2.2038j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  1.1185+4.1834j,
          0.0486+1.9648j,  1.9194+2.2038j]], dtype=torch.complex128)

The max per-element difference (slow mode) is: 4.565870490719988.</failure>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_addmm_cpu_float64" time="0.069" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_addmm_decomposed_cpu_complex128" time="0.054" file="test_ops_gradients.py">
      <failure message="torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 2 with respect to input 1,&#10;numerical:tensor(-0.4064-0.3963j, dtype=torch.complex128)&#10;analytical:tensor(-0.3713+0.3920j, dtype=torch.complex128)&#10;&#10;The above quantities relating the numerical and analytical jacobians are computed &#10;in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background &#10;about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:&#10;&#10;Numerical:&#10; tensor([[ 6.7889e-01-2.5206e-02j, -7.9331e-01-9.0171e-01j,&#10;         7.5660e-307+1.7802e-306j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j],&#10;        [ 0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  6.7889e-01-2.5206e-02j,&#10;         -7.9331e-01-9.0171e-01j, 7.5660e-307+1.7802e-306j],&#10;        [-5.4426e-02+8.1329e-01j, -4.4593e-01-3.1350e-01j,&#10;         1.3351e-306+1.3796e-306j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j],&#10;        [ 0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j, -5.4426e-02+8.1329e-01j,&#10;         -4.4593e-01-3.1350e-01j, 1.3351e-306+1.3796e-306j]],&#10; dtype=torch.complex128)&#10;Analytical:&#10;tensor([[ 0.6789+0.0252j, -0.0544-0.8133j,  0.7051+0.1922j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.6789+0.0252j,&#10;         -0.0544-0.8133j,  0.7051+0.1922j],&#10;        [-0.7933+0.9017j, -0.4459+0.3135j, -0.2133+0.7820j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j, -0.7933+0.9017j,&#10;         -0.4459+0.3135j, -0.2133+0.7820j]], dtype=torch.complex128)&#10;&#10;The max per-element difference (slow mode) is: 0.8105540337898993.">Traceback (most recent call last):
  File "C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py", line 60, in test_fn_gradgrad
    self._check_helper(device, dtype, op, op.get_op(), 'bwgrad_bwgrad')
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 4329, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args, **kwargs))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 3879, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1694, in gradgradcheck
    return gradcheck(
           ^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1536, in gradcheck
    return _gradcheck_helper(**args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1550, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1157, in _gradcheck_real_imag
    gradcheck_fn(imag_fn, imag_func_out, tupled_inputs, imag_outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1421, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1389, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 2 with respect to input 1,
numerical:tensor(-0.4064-0.3963j, dtype=torch.complex128)
analytical:tensor(-0.3713+0.3920j, dtype=torch.complex128)

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[ 6.7889e-01-2.5206e-02j, -7.9331e-01-9.0171e-01j,
         7.5660e-307+1.7802e-306j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j],
        [ 0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  6.7889e-01-2.5206e-02j,
         -7.9331e-01-9.0171e-01j, 7.5660e-307+1.7802e-306j],
        [-5.4426e-02+8.1329e-01j, -4.4593e-01-3.1350e-01j,
         1.3351e-306+1.3796e-306j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j],
        [ 0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j, -5.4426e-02+8.1329e-01j,
         -4.4593e-01-3.1350e-01j, 1.3351e-306+1.3796e-306j]], dtype=torch.complex128)
Analytical:
tensor([[ 0.6789+0.0252j, -0.0544-0.8133j,  0.7051+0.1922j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.6789+0.0252j,
         -0.0544-0.8133j,  0.7051+0.1922j],
        [-0.7933+0.9017j, -0.4459+0.3135j, -0.2133+0.7820j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j, -0.7933+0.9017j,
         -0.4459+0.3135j, -0.2133+0.7820j]], dtype=torch.complex128)

The max per-element difference (slow mode) is: 0.8105540337898993.</failure>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_addmm_decomposed_cpu_float64" time="0.065" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_addmv_cpu_complex128" time="0.739" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_addmv_cpu_float64" time="0.223" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_addr_cpu_complex128" time="0.030" file="test_ops_gradients.py">
      <failure message="torch.autograd.gradcheck.GradcheckError: For output 2 and input 1:&#10;&#10;gradcheck or gradgradcheck failed while testing batched gradient computation.&#10;This could have been invoked in a number of ways (via a test that calls&#10;gradcheck/gradgradcheck directly or via an autogenerated test).&#10;&#10;If you are adding a new operator, please file an issue and then use one of the&#10;workarounds. The workaround depends on how your test invokes gradcheck/gradgradcheck.&#10;If the test&#10;- manually invokes gradcheck/gradgradcheck, then call gradcheck/gradgradcheck&#10;  with `check_batched_grad=False` as a keyword argument.&#10;- is OpInfo-based (e.g., in test_ops_gradients.py), then modify the OpInfo for the test&#10;  to have `check_batched_grad=False` and/or `check_batched_gradgrad=False`.&#10;&#10;If you're modifying an existing operator that supports batched grad computation,&#10;or wish to make a new operator work with batched grad computation, please read&#10;the following.&#10;&#10;To compute batched grads (e.g., jacobians, hessians), we vmap over the backward&#10;computation. The most common failure case is if there is a 'vmap-incompatible&#10;operation' in the backward pass. Please see&#10;NOTE: [How to write vmap-compatible backward formulas]&#10;in the codebase for an explanation of how to fix this.&#10;&#10;Got:&#10;tensor([[ 2.5366-0.5109j,  1.3246-0.6083j, -0.9270+1.5779j, -1.4224-1.4708j,&#10;          0.3032+0.2778j],&#10;        [-0.0561-1.4396j,  0.5513-0.8274j, -0.0415+0.5399j, -2.2319+0.2523j,&#10;         -0.1619-0.7784j]], dtype=torch.complex128)&#10;&#10;Expected:&#10;tensor([[ 1.0842+0.5543j, -0.3350+1.6438j,  0.3208+0.4515j, -0.1550+1.4121j,&#10;         -1.4954-0.8838j],&#10;        [-0.8455+0.6540j,  1.2100+1.5291j,  0.3054-0.6695j, -3.1500-0.7513j,&#10;         -1.5255+1.3565j]], dtype=torch.complex128)">Traceback (most recent call last):
  File "C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py", line 60, in test_fn_gradgrad
    self._check_helper(device, dtype, op, op.get_op(), 'bwgrad_bwgrad')
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 4329, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args, **kwargs))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 3879, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1694, in gradgradcheck
    return gradcheck(
           ^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1536, in gradcheck
    return _gradcheck_helper(**args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1564, in _gradcheck_helper
    _test_batched_grad(tupled_inputs, o, i)
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 939, in _test_batched_grad
    raise GradcheckError(_get_failed_batched_grad_test_msg(output_idx, input_idx, res, exp))
torch.autograd.gradcheck.GradcheckError: For output 2 and input 1:

gradcheck or gradgradcheck failed while testing batched gradient computation.
This could have been invoked in a number of ways (via a test that calls
gradcheck/gradgradcheck directly or via an autogenerated test).

If you are adding a new operator, please file an issue and then use one of the
workarounds. The workaround depends on how your test invokes gradcheck/gradgradcheck.
If the test
- manually invokes gradcheck/gradgradcheck, then call gradcheck/gradgradcheck
  with `check_batched_grad=False` as a keyword argument.
- is OpInfo-based (e.g., in test_ops_gradients.py), then modify the OpInfo for the test
  to have `check_batched_grad=False` and/or `check_batched_gradgrad=False`.

If you're modifying an existing operator that supports batched grad computation,
or wish to make a new operator work with batched grad computation, please read
the following.

To compute batched grads (e.g., jacobians, hessians), we vmap over the backward
computation. The most common failure case is if there is a 'vmap-incompatible
operation' in the backward pass. Please see
NOTE: [How to write vmap-compatible backward formulas]
in the codebase for an explanation of how to fix this.

Got:
tensor([[ 2.5366-0.5109j,  1.3246-0.6083j, -0.9270+1.5779j, -1.4224-1.4708j,
          0.3032+0.2778j],
        [-0.0561-1.4396j,  0.5513-0.8274j, -0.0415+0.5399j, -2.2319+0.2523j,
         -0.1619-0.7784j]], dtype=torch.complex128)

Expected:
tensor([[ 1.0842+0.5543j, -0.3350+1.6438j,  0.3208+0.4515j, -0.1550+1.4121j,
         -1.4954-0.8838j],
        [-0.8455+0.6540j,  1.2100+1.5291j,  0.3054-0.6695j, -3.1500-0.7513j,
         -1.5255+1.3565j]], dtype=torch.complex128)</failure>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_addr_cpu_float64" time="0.084" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_all_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_all_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_allclose_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_allclose_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_amax_cpu_float64" time="0.120" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_amin_cpu_float64" time="0.118" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_aminmax_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_angle_cpu_complex128" time="0.067" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_angle_cpu_float64" time="0.007" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_any_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_any_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_arange_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_argmax_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_argmin_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_argsort_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_argwhere_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_argwhere_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_as_strided_cpu_complex128" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Numerous errors">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Numerous errors</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_as_strided_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Numerous errors">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Numerous errors</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_as_strided_partial_views_cpu_complex128" time="0.097" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_as_strided_partial_views_cpu_float64" time="0.031" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_as_strided_scatter_cpu_complex128" time="0.593" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_as_strided_scatter_cpu_float64" time="0.091" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_asin_cpu_complex128" time="0.052" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_asin_cpu_float64" time="0.008" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_asinh_cpu_complex128" time="0.050" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_asinh_cpu_float64" time="0.008" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_atan2_cpu_float64" time="0.150" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_atan_cpu_complex128" time="0.059" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_atan_cpu_float64" time="0.008" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_atanh_cpu_complex128" time="0.066" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_atanh_cpu_float64" time="0.009" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_atleast_1d_cpu_complex128" time="0.493" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_atleast_1d_cpu_float64" time="0.172" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_atleast_2d_cpu_complex128" time="0.477" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_atleast_2d_cpu_float64" time="0.151" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_atleast_3d_cpu_complex128" time="0.563" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_atleast_3d_cpu_float64" time="0.085" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_baddbmm_cpu_complex128" time="2.985" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_baddbmm_cpu_float64" time="0.140" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_bernoulli_cpu_float64" time="0.084" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_bfloat16_cpu_complex128" time="0.009" file="test_ops_gradients.py">
      <skipped type="pytest.xfail" message="" />
      <system-err>c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py:739: UserWarning: Input #1 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  warnings.warn(
      </system-err>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_bfloat16_cpu_float64" time="0.004" file="test_ops_gradients.py">
      <skipped type="pytest.xfail" message="" />
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_block_diag_cpu_complex128" time="0.412" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_block_diag_cpu_float64" time="0.075" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_bmm_cpu_complex128" time="0.083" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_bmm_cpu_float64" time="0.032" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_bool_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_bool_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_broadcast_tensors_cpu_complex128" time="0.307" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_broadcast_tensors_cpu_float64" time="0.045" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_broadcast_to_cpu_complex128" time="0.192" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_broadcast_to_cpu_float64" time="0.036" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_bucketize_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_byte_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_byte_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_cartesian_prod_cpu_complex128" time="0.266" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_cartesian_prod_cpu_float64" time="0.044" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_cat_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="TODO(whc) fix pre-existing bug with cat for newly added opinfo for empty+nonempty">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: TODO(whc) fix pre-existing bug with cat for newly added opinfo for empty+nonempty</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_cat_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="TODO(whc) fix pre-existing bug with cat for newly added opinfo for empty+nonempty">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: TODO(whc) fix pre-existing bug with cat for newly added opinfo for empty+nonempty</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_cauchy_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_cdist_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op claims it doesn't support gradgrad. This is not verified.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Op claims it doesn't support gradgrad. This is not verified.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_cdouble_cpu_complex128" time="0.121" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_cdouble_cpu_float64" time="0.032" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_ceil_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_cfloat_cpu_complex128" time="0.008" file="test_ops_gradients.py">
      <skipped type="pytest.xfail" message="" />
      <system-err>c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py:739: UserWarning: Input #1 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  warnings.warn(
      </system-err>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_cfloat_cpu_float64" time="0.007" file="test_ops_gradients.py">
      <skipped type="pytest.xfail" message="" />
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_chalf_cpu_complex128" time="0.003" file="test_ops_gradients.py">
      <skipped type="pytest.xfail" message="" />
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_chalf_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.xfail" message="" />
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_char_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_char_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_cholesky_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_cholesky_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_cholesky_inverse_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_cholesky_inverse_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_cholesky_solve_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_cholesky_solve_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_chunk_cpu_complex128" time="0.233" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_chunk_cpu_float64" time="0.095" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_clamp_cpu_float64" time="0.080" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_clamp_max_cpu_float64" time="0.110" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_clamp_min_cpu_float64" time="0.126" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_clone_cpu_complex128" time="0.093" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_clone_cpu_float64" time="0.024" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_column_stack_cpu_complex128" time="0.253" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_column_stack_cpu_float64" time="0.036" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_combinations_cpu_complex128" time="1.020" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_combinations_cpu_float64" time="0.184" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_complex_cpu_float64" time="0.130" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_conj_cpu_complex128" time="0.087" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_conj_cpu_float64" time="0.024" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_conj_physical_cpu_complex128" time="0.044" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_conj_physical_cpu_float64" time="0.025" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_constant_pad_nd_cpu_complex128" time="1.634" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_constant_pad_nd_cpu_float64" time="0.396" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_contiguous_cpu_complex128" time="0.049" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_contiguous_cpu_float64" time="0.010" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_copysign_cpu_float64" time="0.084" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_corrcoef_cpu_complex128" time="0.022" file="test_ops_gradients.py">
      <failure message="torch.autograd.gradcheck.GradcheckError: backward not multiplied by grad_output">Traceback (most recent call last):
  File "C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py", line 60, in test_fn_gradgrad
    self._check_helper(device, dtype, op, op.get_op(), 'bwgrad_bwgrad')
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 4329, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args, **kwargs))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 3879, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1694, in gradgradcheck
    return gradcheck(
           ^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1536, in gradcheck
    return _gradcheck_helper(**args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1566, in _gradcheck_helper
    _test_backward_mul_by_grad_output(outputs, tupled_inputs, check_sparse_nnz)
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 970, in _test_backward_mul_by_grad_output
    raise GradcheckError('backward not multiplied by grad_output')
torch.autograd.gradcheck.GradcheckError: backward not multiplied by grad_output</failure>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_corrcoef_cpu_float64" time="0.090" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_cos_cpu_complex128" time="0.105" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_cos_cpu_float64" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_cosh_cpu_complex128" time="0.097" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_cosh_cpu_float64" time="0.017" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_count_nonzero_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_count_nonzero_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_cov_cpu_complex128" time="0.011" file="test_ops_gradients.py">
      <skipped type="pytest.xfail" message="" />
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_cov_cpu_float64" time="0.051" file="test_ops_gradients.py">
      <skipped type="pytest.xfail" message="" />
      <system-err>c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py:739: UserWarning: Input #1 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  warnings.warn(
      </system-err>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_cross_cpu_complex128" time="0.227" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_cross_cpu_float64" time="0.071" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_cummax_cpu_float64" time="0.125" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_cummin_cpu_float64" time="0.054" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_cumprod_cpu_complex128" time="67.278" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_cumprod_cpu_float64" time="10.592" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_cumsum_cpu_complex128" time="0.120" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_cumsum_cpu_float64" time="0.024" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_cumulative_trapezoid_cpu_complex128" time="0.829" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_cumulative_trapezoid_cpu_float64" time="0.188" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_deg2rad_cpu_float64" time="0.010" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_diag_cpu_complex128" time="0.564" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_diag_cpu_float64" time="0.079" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_diag_embed_cpu_complex128" time="0.645" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_diag_embed_cpu_float64" time="0.078" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_diagflat_cpu_complex128" time="0.163" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_diagflat_cpu_float64" time="0.104" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_diagonal_copy_cpu_complex128" time="0.425" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_diagonal_copy_cpu_float64" time="0.218" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_diagonal_cpu_complex128" time="0.703" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_diagonal_cpu_float64" time="0.074" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_diagonal_scatter_cpu_complex128" time="1.265" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_diagonal_scatter_cpu_float64" time="0.182" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_diff_cpu_complex128" time="6.639" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_diff_cpu_float64" time="1.738" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_digamma_cpu_float64" time="0.026" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_dist_cpu_complex128" time="4.779" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_dist_cpu_float64" time="1.028" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_div_floor_rounding_cpu_float64" time="0.077" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_div_no_rounding_mode_cpu_complex128" time="0.782" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_div_no_rounding_mode_cpu_float64" time="0.172" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_div_trunc_rounding_cpu_float64" time="0.130" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_dot_cpu_complex128" time="0.148" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_dot_cpu_float64" time="0.013" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_double_cpu_complex128" time="0.128" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_double_cpu_float64" time="0.023" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_dsplit_cpu_complex128" time="0.138" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_dsplit_cpu_float64" time="0.023" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_dstack_cpu_complex128" time="0.276" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_dstack_cpu_float64" time="0.056" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_einsum_cpu_complex128" time="1.280" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_einsum_cpu_float64" time="0.229" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_empty_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_empty_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_empty_like_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_empty_like_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_empty_permuted_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_empty_permuted_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_eq_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_eq_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_equal_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_equal_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_erf_cpu_float64" time="0.027" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_erfc_cpu_float64" time="0.076" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_erfinv_cpu_float64" time="0.041" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_exp2_cpu_complex128" time="0.159" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_exp2_cpu_float64" time="0.018" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_exp_cpu_complex128" time="0.095" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_exp_cpu_float64" time="0.017" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_expand_as_cpu_complex128" time="0.082" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_expand_as_cpu_float64" time="0.016" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_expand_cpu_complex128" time="0.342" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_expand_cpu_float64" time="0.049" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_expm1_cpu_float64" time="0.009" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_exponential_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_eye_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_eye_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fft_fft2_cpu_complex128" time="0.221" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fft_fft2_cpu_float64" time="0.083" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fft_fft_cpu_complex128" time="0.342" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fft_fft_cpu_float64" time="0.059" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fft_fftn_cpu_complex128" time="0.365" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fft_fftn_cpu_float64" time="0.069" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fft_fftshift_cpu_complex128" time="0.163" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fft_fftshift_cpu_float64" time="0.031" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fft_hfft2_cpu_complex128" time="0.289" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fft_hfft2_cpu_float64" time="0.063" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fft_hfft_cpu_complex128" time="0.216" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fft_hfft_cpu_float64" time="0.070" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fft_hfftn_cpu_complex128" time="0.384" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fft_hfftn_cpu_float64" time="0.056" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fft_ifft2_cpu_complex128" time="0.204" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fft_ifft2_cpu_float64" time="0.096" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fft_ifft_cpu_complex128" time="0.270" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fft_ifft_cpu_float64" time="0.062" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fft_ifftn_cpu_complex128" time="0.361" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fft_ifftn_cpu_float64" time="0.070" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fft_ifftshift_cpu_complex128" time="0.159" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fft_ifftshift_cpu_float64" time="0.031" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fft_ihfft2_cpu_float64" time="0.052" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fft_ihfft_cpu_float64" time="0.086" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fft_ihfftn_cpu_float64" time="0.132" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fft_irfft2_cpu_complex128" time="0.201" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fft_irfft2_cpu_float64" time="0.042" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fft_irfft_cpu_complex128" time="0.218" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fft_irfft_cpu_float64" time="0.151" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fft_irfftn_cpu_complex128" time="0.264" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fft_irfftn_cpu_float64" time="0.053" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fft_rfft2_cpu_float64" time="0.047" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fft_rfft_cpu_float64" time="0.169" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fft_rfftn_cpu_float64" time="0.150" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fill_cpu_complex128" time="0.131" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fill_cpu_float64" time="0.018" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_flatten_cpu_complex128" time="0.269" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_flatten_cpu_float64" time="0.139" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_flip_cpu_complex128" time="0.286" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_flip_cpu_float64" time="0.094" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fliplr_cpu_complex128" time="0.061" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fliplr_cpu_float64" time="0.018" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_flipud_cpu_complex128" time="0.075" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_flipud_cpu_float64" time="0.018" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_float_cpu_complex128" time="0.009" file="test_ops_gradients.py">
      <skipped type="pytest.xfail" message="" />
      <system-err>c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py:739: UserWarning: Input #1 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  warnings.warn(
      </system-err>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_float_cpu_float64" time="0.005" file="test_ops_gradients.py">
      <skipped type="pytest.xfail" message="" />
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_float_power_cpu_complex128" time="1.063" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_float_power_cpu_float64" time="0.206" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_floor_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_floor_divide_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fmax_cpu_float64" time="0.113" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fmin_cpu_float64" time="0.120" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_fmod_cpu_float64" time="0.190" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_frac_cpu_float64" time="0.039" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_frexp_cpu_float64" time="0.038" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_full_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_full_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_full_like_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_full_like_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_gather_cpu_complex128" time="0.137" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_gather_cpu_float64" time="0.026" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_ge_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_geometric_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_geqrf_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_geqrf_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_gradient_cpu_complex128" time="1.460" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_gradient_cpu_float64" time="0.342" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_grid_sampler_2d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op claims it doesn't support gradgrad. This is not verified.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Op claims it doesn't support gradgrad. This is not verified.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_gt_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_half_cpu_complex128" time="0.005" file="test_ops_gradients.py">
      <skipped type="pytest.xfail" message="" />
      <system-err>c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py:739: UserWarning: Input #1 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. 
  warnings.warn(
      </system-err>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_half_cpu_float64" time="0.003" file="test_ops_gradients.py">
      <skipped type="pytest.xfail" message="" />
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_heaviside_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_histc_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_histogram_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_histogramdd_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_hsplit_cpu_complex128" time="0.135" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_hsplit_cpu_float64" time="0.166" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_hstack_cpu_complex128" time="0.242" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_hstack_cpu_float64" time="0.038" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_hypot_cpu_float64" time="0.163" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_i0_cpu_float64" time="0.072" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_igamma_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_igammac_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_imag_cpu_complex128" time="0.146" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_index_add_cpu_complex128" time="0.620" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_index_add_cpu_float64" time="0.105" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_index_copy_cpu_complex128" time="0.192" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_index_copy_cpu_float64" time="0.051" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_index_fill_cpu_complex128" time="0.298" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_index_fill_cpu_float64" time="0.052" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_index_put_cpu_complex128" time="0.269" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_index_put_cpu_float64" time="0.067" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_index_reduce_cpu_float64" time="0.557" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_index_select_cpu_complex128" time="0.085" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_index_select_cpu_float64" time="0.016" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_inner_cpu_complex128" time="0.035" file="test_ops_gradients.py">
      <failure message="torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 0 with respect to input 1,&#10;numerical:tensor(0.6021+0.1955j, dtype=torch.complex128)&#10;analytical:tensor(0.6021-0.1955j, dtype=torch.complex128)&#10;&#10;The above quantities relating the numerical and analytical jacobians are computed &#10;in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background &#10;about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:&#10;&#10;Numerical:&#10; tensor([[0.9781+0.2337j, 0.0000+0.0000j, 0.0000+0.0000j, 0.0000+0.0000j,&#10;         0.0000+0.0000j],&#10;        [0.0000+0.0000j, 0.9781+0.2337j, 0.0000+0.0000j, 0.0000+0.0000j,&#10;         0.0000+0.0000j],&#10;        [0.0000+0.0000j, 0.0000+0.0000j, 0.9781+0.2337j, 0.0000+0.0000j,&#10;         0.0000+0.0000j],&#10;        [0.0000+0.0000j, 0.0000+0.0000j, 0.0000+0.0000j, 0.9781+0.2337j,&#10;         0.0000+0.0000j],&#10;        [0.0000+0.0000j, 0.0000+0.0000j, 0.0000+0.0000j, 0.0000+0.0000j,&#10;         0.9781+0.2337j]], dtype=torch.complex128)&#10;Analytical:&#10;tensor([[0.9781-0.2337j, 0.0000-0.0000j, 0.0000-0.0000j, 0.0000-0.0000j,&#10;         0.0000-0.0000j],&#10;        [0.0000-0.0000j, 0.9781-0.2337j, 0.0000-0.0000j, 0.0000-0.0000j,&#10;         0.0000-0.0000j],&#10;        [0.0000-0.0000j, 0.0000-0.0000j, 0.9781-0.2337j, 0.0000-0.0000j,&#10;         0.0000-0.0000j],&#10;        [0.0000-0.0000j, 0.0000-0.0000j, 0.0000-0.0000j, 0.9781-0.2337j,&#10;         0.0000-0.0000j],&#10;        [0.0000-0.0000j, 0.0000-0.0000j, 0.0000-0.0000j, 0.0000-0.0000j,&#10;         0.9781-0.2337j]], dtype=torch.complex128)&#10;&#10;The max per-element difference (slow mode) is: 0.4673379024435991.">Traceback (most recent call last):
  File "C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py", line 60, in test_fn_gradgrad
    self._check_helper(device, dtype, op, op.get_op(), 'bwgrad_bwgrad')
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 4329, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args, **kwargs))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 3879, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1694, in gradgradcheck
    return gradcheck(
           ^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1536, in gradcheck
    return _gradcheck_helper(**args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1550, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1157, in _gradcheck_real_imag
    gradcheck_fn(imag_fn, imag_func_out, tupled_inputs, imag_outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1421, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1389, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 0 with respect to input 1,
numerical:tensor(0.6021+0.1955j, dtype=torch.complex128)
analytical:tensor(0.6021-0.1955j, dtype=torch.complex128)

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[0.9781+0.2337j, 0.0000+0.0000j, 0.0000+0.0000j, 0.0000+0.0000j,
         0.0000+0.0000j],
        [0.0000+0.0000j, 0.9781+0.2337j, 0.0000+0.0000j, 0.0000+0.0000j,
         0.0000+0.0000j],
        [0.0000+0.0000j, 0.0000+0.0000j, 0.9781+0.2337j, 0.0000+0.0000j,
         0.0000+0.0000j],
        [0.0000+0.0000j, 0.0000+0.0000j, 0.0000+0.0000j, 0.9781+0.2337j,
         0.0000+0.0000j],
        [0.0000+0.0000j, 0.0000+0.0000j, 0.0000+0.0000j, 0.0000+0.0000j,
         0.9781+0.2337j]], dtype=torch.complex128)
Analytical:
tensor([[0.9781-0.2337j, 0.0000-0.0000j, 0.0000-0.0000j, 0.0000-0.0000j,
         0.0000-0.0000j],
        [0.0000-0.0000j, 0.9781-0.2337j, 0.0000-0.0000j, 0.0000-0.0000j,
         0.0000-0.0000j],
        [0.0000-0.0000j, 0.0000-0.0000j, 0.9781-0.2337j, 0.0000-0.0000j,
         0.0000-0.0000j],
        [0.0000-0.0000j, 0.0000-0.0000j, 0.0000-0.0000j, 0.9781-0.2337j,
         0.0000-0.0000j],
        [0.0000-0.0000j, 0.0000-0.0000j, 0.0000-0.0000j, 0.0000-0.0000j,
         0.9781-0.2337j]], dtype=torch.complex128)

The max per-element difference (slow mode) is: 0.4673379024435991.</failure>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_inner_cpu_float64" time="0.029" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_int_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_int_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_isclose_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_isclose_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_isfinite_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_isfinite_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_isin_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_isinf_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_isinf_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_isnan_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_isnan_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_isneginf_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_isposinf_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_isreal_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_isreal_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_istft_cpu_complex128" time="0.888" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_jiterator_2inputs_2outputs_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_jiterator_2inputs_2outputs_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_jiterator_4inputs_with_extra_args_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_jiterator_4inputs_with_extra_args_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_jiterator_binary_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_jiterator_binary_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_jiterator_binary_return_by_ref_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_jiterator_binary_return_by_ref_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_jiterator_unary_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_jiterator_unary_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_kron_cpu_complex128" time="0.156" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_kron_cpu_float64" time="0.025" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_kthvalue_cpu_float64" time="0.076" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_ldexp_cpu_complex128" time="0.764" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_ldexp_cpu_float64" time="0.128" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_le_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_lerp_cpu_complex128" time="2.163" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_lerp_cpu_float64" time="0.354" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_lgamma_cpu_float64" time="0.027" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_cholesky_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_cholesky_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_cholesky_ex_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_cholesky_ex_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_cond_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_cond_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_cross_cpu_complex128" time="0.234" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_cross_cpu_float64" time="0.039" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_det_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_det_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_det_singular_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_det_singular_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_diagonal_cpu_complex128" time="0.469" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_diagonal_cpu_float64" time="0.074" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_eig_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_eig_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_eigh_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_eigh_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_eigvals_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_eigvals_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_eigvalsh_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_eigvalsh_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_householder_product_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_householder_product_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_inv_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_inv_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_inv_ex_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_inv_ex_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_ldl_factor_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_ldl_factor_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_ldl_factor_ex_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_ldl_factor_ex_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_ldl_solve_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_ldl_solve_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_lstsq_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_lstsq_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_lstsq_grad_oriented_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_lstsq_grad_oriented_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_lu_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_lu_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_lu_factor_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_lu_factor_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_lu_factor_ex_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_lu_factor_ex_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_lu_solve_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_lu_solve_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_matrix_norm_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_matrix_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_matrix_power_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_matrix_power_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_matrix_rank_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_matrix_rank_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_matrix_rank_hermitian_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_matrix_rank_hermitian_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_multi_dot_cpu_complex128" time="0.021" file="test_ops_gradients.py">
      <failure message="torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 0 with respect to input 1,&#10;numerical:tensor(-0.3419+0.7510j, dtype=torch.complex128)&#10;analytical:tensor(-0.3419-0.7510j, dtype=torch.complex128)&#10;&#10;The above quantities relating the numerical and analytical jacobians are computed &#10;in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background &#10;about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:&#10;&#10;Numerical:&#10; tensor([[-0.5324+0.7545j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j, -0.5324+0.7545j]], dtype=torch.complex128)&#10;Analytical:&#10;tensor([[-0.5324-0.7545j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j, -0.5324-0.7545j]], dtype=torch.complex128)&#10;&#10;The max per-element difference (slow mode) is: 1.5090242812951076.">Traceback (most recent call last):
  File "C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py", line 60, in test_fn_gradgrad
    self._check_helper(device, dtype, op, op.get_op(), 'bwgrad_bwgrad')
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 4329, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args, **kwargs))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 3879, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1694, in gradgradcheck
    return gradcheck(
           ^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1536, in gradcheck
    return _gradcheck_helper(**args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1550, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1157, in _gradcheck_real_imag
    gradcheck_fn(imag_fn, imag_func_out, tupled_inputs, imag_outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1421, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1389, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 0 with respect to input 1,
numerical:tensor(-0.3419+0.7510j, dtype=torch.complex128)
analytical:tensor(-0.3419-0.7510j, dtype=torch.complex128)

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[-0.5324+0.7545j,  0.0000+0.0000j],
        [ 0.0000+0.0000j, -0.5324+0.7545j]], dtype=torch.complex128)
Analytical:
tensor([[-0.5324-0.7545j,  0.0000-0.0000j],
        [ 0.0000-0.0000j, -0.5324-0.7545j]], dtype=torch.complex128)

The max per-element difference (slow mode) is: 1.5090242812951076.</failure>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_multi_dot_cpu_float64" time="0.198" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_norm_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_norm_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_norm_subgradients_at_zero_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_norm_subgradients_at_zero_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_pinv_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_pinv_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_pinv_hermitian_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_pinv_hermitian_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_pinv_singular_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_pinv_singular_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_qr_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_qr_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_slogdet_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_slogdet_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_solve_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_solve_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_solve_ex_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_solve_ex_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_solve_triangular_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_solve_triangular_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_svd_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_svd_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_svdvals_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_svdvals_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_tensorinv_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_tensorinv_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_tensorsolve_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_tensorsolve_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_vander_cpu_complex128" time="0.553" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_vander_cpu_float64" time="0.138" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_vecdot_cpu_complex128" time="3.428" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_vecdot_cpu_float64" time="0.617" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_vector_norm_cpu_complex128" time="6.337" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linalg_vector_norm_cpu_float64" time="1.358" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linspace_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_linspace_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_log10_cpu_complex128" time="0.108" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_log10_cpu_float64" time="0.017" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_log1p_cpu_complex128" time="0.041" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_log1p_cpu_float64" time="0.007" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_log2_cpu_complex128" time="0.093" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_log2_cpu_float64" time="0.017" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_log_cpu_complex128" time="0.092" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_log_cpu_float64" time="0.036" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_log_normal_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_log_softmax_cpu_float64" time="0.088" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_log_softmax_with_dtype_cpu_complex128" time="0.229" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_log_softmax_with_dtype_cpu_float64" time="0.044" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_logaddexp2_cpu_float64" time="0.016" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_logaddexp_cpu_complex128" time="0.874" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_logaddexp_cpu_float64" time="0.188" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_logcumsumexp_cpu_complex128" time="0.271" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_logcumsumexp_cpu_float64" time="0.088" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_logdet_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_logdet_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_logical_and_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_logical_and_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_logical_not_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_logical_not_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_logical_or_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_logical_or_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_logical_xor_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_logical_xor_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_logit_cpu_float64" time="0.051" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_logspace_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_logspace_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_logsumexp_cpu_float64" time="0.763" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_long_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_long_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_lt_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_lu_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_lu_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_lu_solve_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_lu_solve_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_lu_unpack_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_lu_unpack_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_mH_cpu_complex128" time="0.179" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_mH_cpu_float64" time="0.028" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_mT_cpu_complex128" time="0.166" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_mT_cpu_float64" time="0.087" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_masked_amax_cpu_float64" time="0.770" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_masked_amin_cpu_float64" time="0.691" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_masked_argmax_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_masked_argmin_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_masked_cumprod_cpu_complex128" time="1.396" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_masked_cumprod_cpu_float64" time="0.252" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_masked_cumsum_cpu_complex128" time="0.845" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_masked_cumsum_cpu_float64" time="0.193" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_masked_fill_cpu_complex128" time="0.468" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_masked_fill_cpu_float64" time="0.104" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_masked_log_softmax_cpu_float64" time="0.255" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_masked_logaddexp_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_masked_logsumexp_cpu_float64" time="0.789" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_masked_mean_cpu_complex128" time="4.444" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_masked_mean_cpu_float64" time="0.815" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_masked_median_cpu_float64" time="0.213" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_masked_norm_cpu_float64" time="3.545" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_masked_normalize_cpu_complex128" time="2.416" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_masked_normalize_cpu_float64" time="0.489" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_masked_prod_cpu_complex128" time="4.543" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_masked_prod_cpu_float64" time="0.862" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_masked_scatter_cpu_complex128" time="0.275" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_masked_scatter_cpu_float64" time="0.053" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_masked_select_cpu_complex128" time="0.274" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_masked_select_cpu_float64" time="0.088" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_masked_softmax_cpu_float64" time="0.213" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_masked_softmin_cpu_float64" time="0.227" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_masked_std_cpu_complex128" time="4.060" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_masked_std_cpu_float64" time="0.736" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_masked_sum_cpu_complex128" time="3.115" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_masked_sum_cpu_float64" time="0.509" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_masked_var_cpu_complex128" time="3.625" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_masked_var_cpu_float64" time="0.588" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_matmul_cpu_complex128" time="0.210" file="test_ops_gradients.py">
      <failure message="torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 1 with respect to input 0,&#10;numerical:tensor(0.2892-0.3979j, dtype=torch.complex128)&#10;analytical:tensor(0.2181-0.4381j, dtype=torch.complex128)&#10;&#10;The above quantities relating the numerical and analytical jacobians are computed &#10;in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background &#10;about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:&#10;&#10;Numerical:&#10; tensor([[ 3.5657e-01-9.5340e-01j, 1.2516e-308+7.7486e-304j,&#10;         8.5403e-312+8.5357e-312j, 8.5398e-312+0.0000e+00j,&#10;          4.6121e-01+1.4129e-01j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j],&#10;        [ 0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  3.5657e-01-9.5340e-01j,&#10;         1.2516e-308+7.7486e-304j, 8.5403e-312+8.5357e-312j,&#10;         8.5398e-312+0.0000e+00j,  4.6121e-01+1.4129e-01j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j],&#10;        [ 0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          3.5657e-01-9.5340e-01j, 1.2516e-308+7.7486e-304j,&#10;         8.5403e-312+8.5357e-312j, 8.5398e-312+0.0000e+00j,&#10;          4.6121e-01+1.4129e-01j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j],&#10;        [ 0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  3.5657e-01-9.5340e-01j,&#10;         1.2516e-308+7.7486e-304j, 8.5403e-312+8.5357e-312j,&#10;         8.5398e-312+0.0000e+00j,  4.6121e-01+1.4129e-01j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j],&#10;        [ 0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          3.5657e-01-9.5340e-01j, 1.2516e-308+7.7486e-304j,&#10;         8.5403e-312+8.5357e-312j, 8.5398e-312+0.0000e+00j,&#10;          4.6121e-01+1.4129e-01j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j],&#10;        [ 0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  3.5657e-01-9.5340e-01j,&#10;         1.2516e-308+7.7486e-304j, 8.5403e-312+8.5357e-312j,&#10;         8.5398e-312+0.0000e+00j,  4.6121e-01+1.4129e-01j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j],&#10;        [ 0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          3.5657e-01-9.5340e-01j, 1.2516e-308+7.7486e-304j,&#10;         8.5403e-312+8.5357e-312j, 8.5398e-312+0.0000e+00j,&#10;          4.6121e-01+1.4129e-01j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j],&#10;        [ 0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  3.5657e-01-9.5340e-01j,&#10;         1.2516e-308+7.7486e-304j, 8.5403e-312+8.5357e-312j,&#10;         8.5398e-312+0.0000e+00j,  4.6121e-01+1.4129e-01j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j],&#10;        [ 0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          3.5657e-01-9.5340e-01j, 1.2516e-308+7.7486e-304j,&#10;         8.5403e-312+8.5357e-312j, 8.5398e-312+0.0000e+00j,&#10;          4.6121e-01+1.4129e-01j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j],&#10;        [ 0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,&#10;          0.0000e+00+0.0000e+00j,  3.5657e-01-9.5340e-01j,&#10;         1.2516e-308+7.7486e-304j, 8.5403e-312+8.5357e-312j,&#10;         8.5398e-312+0.0000e+00j,  4.6121e-01+1.4129e-01j]],&#10; dtype=torch.complex128)&#10;Analytical:&#10;tensor([[ 0.3566+0.9534j,  0.9539-0.9793j, -0.3248-0.6213j,  0.1479-0.6155j,&#10;         -0.8195-0.1612j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.3566+0.9534j,  0.9539-0.9793j, -0.3248-0.6213j,&#10;          0.1479-0.6155j, -0.8195-0.1612j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.3566+0.9534j,  0.9539-0.9793j,&#10;         -0.3248-0.6213j,  0.1479-0.6155j, -0.8195-0.1612j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.3566+0.9534j,&#10;          0.9539-0.9793j, -0.3248-0.6213j,  0.1479-0.6155j, -0.8195-0.1612j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.3566+0.9534j,  0.9539-0.9793j, -0.3248-0.6213j,  0.1479-0.6155j,&#10;         -0.8195-0.1612j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.3566+0.9534j,  0.9539-0.9793j, -0.3248-0.6213j,&#10;          0.1479-0.6155j, -0.8195-0.1612j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.3566+0.9534j,  0.9539-0.9793j,&#10;         -0.3248-0.6213j,  0.1479-0.6155j, -0.8195-0.1612j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.3566+0.9534j,&#10;          0.9539-0.9793j, -0.3248-0.6213j,  0.1479-0.6155j, -0.8195-0.1612j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.3566+0.9534j,  0.9539-0.9793j, -0.3248-0.6213j,  0.1479-0.6155j,&#10;         -0.8195-0.1612j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.3566+0.9534j,  0.9539-0.9793j, -0.3248-0.6213j,&#10;          0.1479-0.6155j, -0.8195-0.1612j]], dtype=torch.complex128)&#10;&#10;The max per-element difference (slow mode) is: 1.9068009569761848.">Traceback (most recent call last):
  File "C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py", line 60, in test_fn_gradgrad
    self._check_helper(device, dtype, op, op.get_op(), 'bwgrad_bwgrad')
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 4329, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args, **kwargs))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 3879, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1694, in gradgradcheck
    return gradcheck(
           ^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1536, in gradcheck
    return _gradcheck_helper(**args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1550, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1157, in _gradcheck_real_imag
    gradcheck_fn(imag_fn, imag_func_out, tupled_inputs, imag_outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1421, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1389, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 1 with respect to input 0,
numerical:tensor(0.2892-0.3979j, dtype=torch.complex128)
analytical:tensor(0.2181-0.4381j, dtype=torch.complex128)

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[ 3.5657e-01-9.5340e-01j, 1.2516e-308+7.7486e-304j,
         8.5403e-312+8.5357e-312j, 8.5398e-312+0.0000e+00j,
          4.6121e-01+1.4129e-01j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j],
        [ 0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  3.5657e-01-9.5340e-01j,
         1.2516e-308+7.7486e-304j, 8.5403e-312+8.5357e-312j,
         8.5398e-312+0.0000e+00j,  4.6121e-01+1.4129e-01j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j],
        [ 0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          3.5657e-01-9.5340e-01j, 1.2516e-308+7.7486e-304j,
         8.5403e-312+8.5357e-312j, 8.5398e-312+0.0000e+00j,
          4.6121e-01+1.4129e-01j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j],
        [ 0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  3.5657e-01-9.5340e-01j,
         1.2516e-308+7.7486e-304j, 8.5403e-312+8.5357e-312j,
         8.5398e-312+0.0000e+00j,  4.6121e-01+1.4129e-01j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j],
        [ 0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          3.5657e-01-9.5340e-01j, 1.2516e-308+7.7486e-304j,
         8.5403e-312+8.5357e-312j, 8.5398e-312+0.0000e+00j,
          4.6121e-01+1.4129e-01j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j],
        [ 0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  3.5657e-01-9.5340e-01j,
         1.2516e-308+7.7486e-304j, 8.5403e-312+8.5357e-312j,
         8.5398e-312+0.0000e+00j,  4.6121e-01+1.4129e-01j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j],
        [ 0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          3.5657e-01-9.5340e-01j, 1.2516e-308+7.7486e-304j,
         8.5403e-312+8.5357e-312j, 8.5398e-312+0.0000e+00j,
          4.6121e-01+1.4129e-01j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j],
        [ 0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  3.5657e-01-9.5340e-01j,
         1.2516e-308+7.7486e-304j, 8.5403e-312+8.5357e-312j,
         8.5398e-312+0.0000e+00j,  4.6121e-01+1.4129e-01j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j],
        [ 0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          3.5657e-01-9.5340e-01j, 1.2516e-308+7.7486e-304j,
         8.5403e-312+8.5357e-312j, 8.5398e-312+0.0000e+00j,
          4.6121e-01+1.4129e-01j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j],
        [ 0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  0.0000e+00+0.0000e+00j,
          0.0000e+00+0.0000e+00j,  3.5657e-01-9.5340e-01j,
         1.2516e-308+7.7486e-304j, 8.5403e-312+8.5357e-312j,
         8.5398e-312+0.0000e+00j,  4.6121e-01+1.4129e-01j]], dtype=torch.complex128)
Analytical:
tensor([[ 0.3566+0.9534j,  0.9539-0.9793j, -0.3248-0.6213j,  0.1479-0.6155j,
         -0.8195-0.1612j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.3566+0.9534j,  0.9539-0.9793j, -0.3248-0.6213j,
          0.1479-0.6155j, -0.8195-0.1612j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.3566+0.9534j,  0.9539-0.9793j,
         -0.3248-0.6213j,  0.1479-0.6155j, -0.8195-0.1612j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.3566+0.9534j,
          0.9539-0.9793j, -0.3248-0.6213j,  0.1479-0.6155j, -0.8195-0.1612j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.3566+0.9534j,  0.9539-0.9793j, -0.3248-0.6213j,  0.1479-0.6155j,
         -0.8195-0.1612j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.3566+0.9534j,  0.9539-0.9793j, -0.3248-0.6213j,
          0.1479-0.6155j, -0.8195-0.1612j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.3566+0.9534j,  0.9539-0.9793j,
         -0.3248-0.6213j,  0.1479-0.6155j, -0.8195-0.1612j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.3566+0.9534j,
          0.9539-0.9793j, -0.3248-0.6213j,  0.1479-0.6155j, -0.8195-0.1612j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.3566+0.9534j,  0.9539-0.9793j, -0.3248-0.6213j,  0.1479-0.6155j,
         -0.8195-0.1612j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.3566+0.9534j,  0.9539-0.9793j, -0.3248-0.6213j,
          0.1479-0.6155j, -0.8195-0.1612j]], dtype=torch.complex128)

The max per-element difference (slow mode) is: 1.9068009569761848.</failure>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_matmul_cpu_float64" time="0.219" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_matrix_exp_cpu_complex128" time="0.317" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_matrix_exp_cpu_float64" time="0.037" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_max_binary_cpu_float64" time="0.134" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_max_pool2d_with_indices_backward_cpu_float64" time="14.098" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_max_reduction_no_dim_cpu_float64" time="0.013" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_max_reduction_with_dim_cpu_float64" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_maximum_cpu_float64" time="0.133" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_mean_cpu_complex128" time="0.678" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_mean_cpu_float64" time="0.117" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_median_cpu_float64" time="0.072" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_meshgrid_list_of_tensors_cpu_complex128" time="1.220" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_meshgrid_list_of_tensors_cpu_float64" time="0.200" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_meshgrid_variadic_tensors_cpu_complex128" time="1.277" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_meshgrid_variadic_tensors_cpu_float64" time="0.283" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_min_binary_cpu_float64" time="0.135" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_min_reduction_no_dim_cpu_float64" time="0.013" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_min_reduction_with_dim_cpu_float64" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_minimum_cpu_float64" time="0.145" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_mm_cpu_complex128" time="0.196" file="test_ops_gradients.py">
      <failure message="torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 1 with respect to input 0,&#10;numerical:tensor(0.5575+0.0841j, dtype=torch.complex128)&#10;analytical:tensor(0.7161+0.1655j, dtype=torch.complex128)&#10;&#10;The above quantities relating the numerical and analytical jacobians are computed &#10;in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background &#10;about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:&#10;&#10;Numerical:&#10; tensor([[-0.7833+0.7064j, -0.7970+0.2046j,  0.3313-0.8532j,  ...,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],&#10;        ...,&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,&#10;         -0.9623+0.5258j, -0.1751-0.9210j,  0.5355-0.2686j]],&#10; dtype=torch.complex128)&#10;Analytical:&#10;tensor([[-0.7833-0.7064j,  0.6547-0.8569j,  0.2435-0.4200j,  ...,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  ...,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  ...,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j],&#10;        ...,&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  ...,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  ...,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  ...,&#10;          0.9950+0.2023j, -0.6779-0.5530j,  0.5355+0.2686j]],&#10; dtype=torch.complex128)&#10;&#10;The max per-element difference (slow mode) is: 1.989741786900266.">Traceback (most recent call last):
  File "C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py", line 60, in test_fn_gradgrad
    self._check_helper(device, dtype, op, op.get_op(), 'bwgrad_bwgrad')
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 4329, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args, **kwargs))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 3879, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1694, in gradgradcheck
    return gradcheck(
           ^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1536, in gradcheck
    return _gradcheck_helper(**args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1550, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1157, in _gradcheck_real_imag
    gradcheck_fn(imag_fn, imag_func_out, tupled_inputs, imag_outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1421, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1389, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 1 with respect to input 0,
numerical:tensor(0.5575+0.0841j, dtype=torch.complex128)
analytical:tensor(0.7161+0.1655j, dtype=torch.complex128)

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[-0.7833+0.7064j, -0.7970+0.2046j,  0.3313-0.8532j,  ...,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],
        ...,
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  ...,
         -0.9623+0.5258j, -0.1751-0.9210j,  0.5355-0.2686j]], dtype=torch.complex128)
Analytical:
tensor([[-0.7833-0.7064j,  0.6547-0.8569j,  0.2435-0.4200j,  ...,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  ...,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  ...,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j],
        ...,
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  ...,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  ...,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  ...,
          0.9950+0.2023j, -0.6779-0.5530j,  0.5355+0.2686j]], dtype=torch.complex128)

The max per-element difference (slow mode) is: 1.989741786900266.</failure>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_mm_cpu_float64" time="0.016" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_mode_cpu_float64" time="0.062" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_movedim_cpu_complex128" time="0.057" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_movedim_cpu_float64" time="0.011" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_msort_cpu_float64" time="0.044" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_mul_cpu_complex128" time="0.713" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_mul_cpu_float64" time="0.192" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_multinomial_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_mv_cpu_complex128" time="0.096" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_mv_cpu_float64" time="0.019" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_mvlgamma_mvlgamma_p_1_cpu_float64" time="0.069" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_mvlgamma_mvlgamma_p_3_cpu_float64" time="0.068" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_mvlgamma_mvlgamma_p_5_cpu_float64" time="0.069" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nan_to_num_cpu_float64" time="0.016" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nanmean_cpu_float64" time="0.253" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nanmedian_cpu_float64" time="0.096" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nanquantile_cpu_float64" time="0.994" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nansum_cpu_float64" time="0.276" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_narrow_copy_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_narrow_copy_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_narrow_cpu_complex128" time="0.280" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_narrow_cpu_float64" time="0.135" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_native_batch_norm_cpu_float64" time="0.339" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_native_dropout_backward_cpu_float64" time="0.050" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_native_layer_norm_cpu_float64" time="0.046" file="test_ops_gradients.py">
      <skipped type="pytest.xfail" message="" />
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_ne_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_ne_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_neg_cpu_complex128" time="0.031" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_neg_cpu_float64" time="0.024" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_new_empty_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_new_empty_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_new_empty_strided_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_new_empty_strided_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_new_full_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_new_full_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_new_ones_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_new_ones_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_new_zeros_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_new_zeros_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nextafter_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_adaptive_avg_pool1d_cpu_float64" time="0.210" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_adaptive_avg_pool2d_cpu_float64" time="0.062" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_adaptive_avg_pool3d_cpu_float64" time="0.080" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_adaptive_max_pool1d_cpu_float64" time="0.055" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_adaptive_max_pool2d_cpu_float64" time="0.138" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_adaptive_max_pool3d_cpu_float64" time="0.274" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_alpha_dropout_cpu_float64" time="0.528" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_avg_pool1d_cpu_float64" time="0.067" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_avg_pool2d_cpu_float64" time="0.043" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_avg_pool3d_cpu_float64" time="0.047" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_batch_norm_cpu_float64" time="0.509" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_bilinear_cpu_float64" time="1.865" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_binary_cross_entropy_cpu_float64" time="1.694" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_binary_cross_entropy_with_logits_cpu_float64" time="0.397" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_celu_cpu_float64" time="0.017" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_conv1d_cpu_complex128" time="4.965" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_conv1d_cpu_float64" time="0.516" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_conv2d_cpu_complex128" time="17.333" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_conv2d_cpu_float64" time="1.812" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_conv_transpose1d_cpu_complex128" time="4.918" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_conv_transpose1d_cpu_float64" time="0.477" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_conv_transpose2d_cpu_complex128" time="5.869" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_conv_transpose2d_cpu_float64" time="0.583" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_conv_transpose3d_cpu_complex128" time="7.098" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_conv_transpose3d_cpu_float64" time="0.560" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_cosine_embedding_loss_cpu_float64" time="0.277" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_cosine_similarity_cpu_float64" time="0.342" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_cross_entropy_cpu_float64" time="0.275" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_ctc_loss_cpu_float64" time="0.008" file="test_ops_gradients.py">
      <skipped type="pytest.xfail" message="" />
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_dropout2d_cpu_float64" time="0.326" file="test_ops_gradients.py">
      <system-err>c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
      </system-err>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_dropout3d_cpu_float64" time="0.235" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_dropout_cpu_float64" time="0.367" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_elu_cpu_float64" time="0.018" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_embedding_bag_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op claims it doesn't support gradgrad. This is not verified.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Op claims it doesn't support gradgrad. This is not verified.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_embedding_cpu_float64" time="0.018" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_feature_alpha_dropout_with_train_cpu_float64" time="0.126" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_feature_alpha_dropout_without_train_cpu_complex128" time="1.595" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_feature_alpha_dropout_without_train_cpu_float64" time="0.311" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_fractional_max_pool2d_cpu_float64" time="0.857" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_fractional_max_pool3d_cpu_float64" time="1.212" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_gaussian_nll_loss_cpu_float64" time="11.854" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_gelu_cpu_float64" time="0.054" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_glu_cpu_float64" time="0.344" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_grid_sample_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op claims it doesn't support gradgrad. This is not verified.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Op claims it doesn't support gradgrad. This is not verified.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_group_norm_cpu_float64" time="1.070" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_hardshrink_cpu_float64" time="0.031" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_hardsigmoid_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op claims it doesn't support gradgrad. This is not verified.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Op claims it doesn't support gradgrad. This is not verified.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_hardswish_cpu_float64" time="0.024" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_hardtanh_cpu_float64" time="0.032" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_hinge_embedding_loss_cpu_float64" time="0.130" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_huber_loss_cpu_float64" time="0.186" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_instance_norm_cpu_float64" time="0.467" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_interpolate_area_cpu_float64" time="0.117" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_interpolate_bicubic_cpu_float64" time="0.120" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_interpolate_bilinear_cpu_float64" time="0.222" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_interpolate_linear_cpu_float64" time="0.182" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_interpolate_nearest_cpu_float64" time="0.167" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_interpolate_trilinear_cpu_float64" time="0.206" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_kl_div_cpu_float64" time="0.269" file="test_ops_gradients.py">
      <system-err>c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:2924: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:2924: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:2924: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:2924: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:2924: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:2924: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:2924: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:2924: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:2924: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:2924: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
      </system-err>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_l1_loss_cpu_complex128" time="0.617" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_l1_loss_cpu_float64" time="0.089" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_layer_norm_cpu_float64" time="0.142" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_leaky_relu_cpu_float64" time="0.048" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_linear_cpu_complex128" time="0.065" file="test_ops_gradients.py">
      <failure message="torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 0 with respect to input 1,&#10;numerical:tensor(0.2381+0.8275j, dtype=torch.complex128)&#10;analytical:tensor(-0.0163+0.1201j, dtype=torch.complex128)&#10;&#10;The above quantities relating the numerical and analytical jacobians are computed &#10;in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background &#10;about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:&#10;&#10;Numerical:&#10; tensor([[ 0.2292+0.9335j,  0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.2292+0.9335j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.2292+0.9335j],&#10;        [ 0.2548+0.9061j,  0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.2548+0.9061j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.2548+0.9061j],&#10;        [-0.7506+0.5653j,  0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j, -0.7506+0.5653j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j, -0.7506+0.5653j],&#10;        [ 0.4147+0.3092j,  0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.4147+0.3092j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.4147+0.3092j]],&#10; dtype=torch.complex128)&#10;Analytical:&#10;tensor([[ -2.2917e-01+9.3355e-01j,   0.0000e+00-0.0000e+00j,&#10;           0.0000e+00-0.0000e+00j],&#10;        [-9.9299e-312+6.0156e-312j, -9.9299e-312+6.0156e-312j,&#10;         -9.9299e-312+6.0156e-312j],&#10;        [-2.8782e-309+1.1690e-308j, -2.8782e-309+1.1690e-308j,&#10;         -2.8782e-309+1.1690e-308j],&#10;        [ -2.5481e-01+9.0614e-01j,   0.0000e+00-0.0000e+00j,&#10;           0.0000e+00-0.0000e+00j],&#10;        [-9.9148e-312+5.5625e-312j, -9.9148e-312+5.5625e-312j,&#10;         -9.9148e-312+5.5625e-312j],&#10;        [-3.1991e-309+1.1347e-308j, -3.1991e-309+1.1347e-308j,&#10;         -3.1991e-309+1.1347e-308j],&#10;        [  7.5063e-01+5.6533e-01j,   0.0000e+00-0.0000e+00j,&#10;           0.0000e+00-0.0000e+00j],&#10;        [ 1.5826e-312+1.1239e-311j,  1.5826e-312+1.1239e-311j,&#10;          1.5826e-312+1.1239e-311j],&#10;        [ 9.3965e-309+7.0869e-309j,  9.3965e-309+7.0869e-309j,&#10;          9.3965e-309+7.0869e-309j],&#10;        [ -4.1466e-01+3.0923e-01j,   0.0000e+00-0.0000e+00j,&#10;           0.0000e+00-0.0000e+00j],&#10;        [-6.1822e-312-9.0034e-313j, -6.1822e-312-9.0034e-313j,&#10;         -6.1822e-312-9.0034e-313j],&#10;        [-5.1960e-309+3.8695e-309j, -5.1960e-309+3.8695e-309j,&#10;         -5.1960e-309+3.8695e-309j]], dtype=torch.complex128)&#10;&#10;The max per-element difference (slow mode) is: 1.501267783981859.">Traceback (most recent call last):
  File "C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py", line 60, in test_fn_gradgrad
    self._check_helper(device, dtype, op, op.get_op(), 'bwgrad_bwgrad')
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 4329, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args, **kwargs))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 3879, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1694, in gradgradcheck
    return gradcheck(
           ^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1536, in gradcheck
    return _gradcheck_helper(**args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1550, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1157, in _gradcheck_real_imag
    gradcheck_fn(imag_fn, imag_func_out, tupled_inputs, imag_outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1421, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1389, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 0 with respect to input 1,
numerical:tensor(0.2381+0.8275j, dtype=torch.complex128)
analytical:tensor(-0.0163+0.1201j, dtype=torch.complex128)

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[ 0.2292+0.9335j,  0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.2292+0.9335j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.2292+0.9335j],
        [ 0.2548+0.9061j,  0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.2548+0.9061j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.2548+0.9061j],
        [-0.7506+0.5653j,  0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j, -0.7506+0.5653j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j, -0.7506+0.5653j],
        [ 0.4147+0.3092j,  0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.4147+0.3092j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.4147+0.3092j]], dtype=torch.complex128)
Analytical:
tensor([[ -2.2917e-01+9.3355e-01j,   0.0000e+00-0.0000e+00j,
           0.0000e+00-0.0000e+00j],
        [-9.9299e-312+6.0156e-312j, -9.9299e-312+6.0156e-312j,
         -9.9299e-312+6.0156e-312j],
        [-2.8782e-309+1.1690e-308j, -2.8782e-309+1.1690e-308j,
         -2.8782e-309+1.1690e-308j],
        [ -2.5481e-01+9.0614e-01j,   0.0000e+00-0.0000e+00j,
           0.0000e+00-0.0000e+00j],
        [-9.9148e-312+5.5625e-312j, -9.9148e-312+5.5625e-312j,
         -9.9148e-312+5.5625e-312j],
        [-3.1991e-309+1.1347e-308j, -3.1991e-309+1.1347e-308j,
         -3.1991e-309+1.1347e-308j],
        [  7.5063e-01+5.6533e-01j,   0.0000e+00-0.0000e+00j,
           0.0000e+00-0.0000e+00j],
        [ 1.5826e-312+1.1239e-311j,  1.5826e-312+1.1239e-311j,
          1.5826e-312+1.1239e-311j],
        [ 9.3965e-309+7.0869e-309j,  9.3965e-309+7.0869e-309j,
          9.3965e-309+7.0869e-309j],
        [ -4.1466e-01+3.0923e-01j,   0.0000e+00-0.0000e+00j,
           0.0000e+00-0.0000e+00j],
        [-6.1822e-312-9.0034e-313j, -6.1822e-312-9.0034e-313j,
         -6.1822e-312-9.0034e-313j],
        [-5.1960e-309+3.8695e-309j, -5.1960e-309+3.8695e-309j,
         -5.1960e-309+3.8695e-309j]], dtype=torch.complex128)

The max per-element difference (slow mode) is: 1.501267783981859.</failure>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_linear_cpu_float64" time="0.289" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_local_response_norm_cpu_float64" time="0.365" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_logsigmoid_cpu_float64" time="0.026" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_margin_ranking_loss_cpu_float64" time="0.219" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_max_pool1d_cpu_float64" time="9.521" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_max_pool2d_cpu_float64" time="13.103" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_max_pool3d_cpu_float64" time="6.308" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_max_unpool1d_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_max_unpool1d_grad_cpu_float64" time="0.150" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_max_unpool2d_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_max_unpool2d_grad_cpu_float64" time="0.423" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_max_unpool3d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_max_unpool3d_grad_cpu_float64" time="0.085" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_mish_cpu_float64" time="0.023" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_mse_loss_cpu_float64" time="0.085" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_multi_margin_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op claims it doesn't support gradgrad. This is not verified.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Op claims it doesn't support gradgrad. This is not verified.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_multilabel_margin_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op claims it doesn't support gradgrad. This is not verified.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Op claims it doesn't support gradgrad. This is not verified.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_multilabel_soft_margin_loss_cpu_float64" time="0.051" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_nll_loss_cpu_float64" time="0.552" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_normalize_cpu_complex128" time="0.590" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_normalize_cpu_float64" time="0.131" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_pad_circular_cpu_complex128" time="0.478" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_pad_circular_cpu_float64" time="0.130" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_pad_constant_cpu_complex128" time="1.943" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_pad_constant_cpu_float64" time="0.269" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_pad_reflect_cpu_complex128" time="0.453" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_pad_reflect_cpu_float64" time="0.066" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_pad_replicate_cpu_complex128" time="0.451" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_pad_replicate_cpu_float64" time="0.067" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_pairwise_distance_cpu_complex128" time="0.570" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_pairwise_distance_cpu_float64" time="0.126" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_pdist_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op claims it doesn't support gradgrad. This is not verified.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Op claims it doesn't support gradgrad. This is not verified.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_pixel_shuffle_cpu_complex128" time="0.141" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_pixel_shuffle_cpu_float64" time="0.025" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_pixel_unshuffle_cpu_complex128" time="0.138" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_pixel_unshuffle_cpu_float64" time="0.057" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_poisson_nll_loss_cpu_float64" time="1.380" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_prelu_cpu_float64" time="0.337" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_relu6_cpu_float64" time="0.016" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_relu_cpu_float64" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_rrelu_cpu_float64" time="0.079" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_scaled_dot_product_attention_cpu_float64" time="1.799" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_selu_cpu_float64" time="0.018" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_silu_complex_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_silu_cpu_float64" time="0.019" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_smooth_l1_loss_cpu_float64" time="0.122" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_soft_margin_loss_cpu_float64" time="0.042" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_softmin_cpu_float64" time="0.046" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_softmin_with_dtype_cpu_complex128" time="0.275" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_softmin_with_dtype_cpu_float64" time="0.048" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_softplus_cpu_float64" time="0.018" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_softshrink_cpu_float64" time="0.027" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_softsign_cpu_complex128" time="0.162" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_softsign_cpu_float64" time="0.032" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_tanhshrink_cpu_complex128" time="0.117" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_tanhshrink_cpu_float64" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_threshold_cpu_float64" time="0.050" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_triplet_margin_loss_cpu_complex128" time="1.322" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_triplet_margin_loss_cpu_float64" time="0.279" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_triplet_margin_with_distance_loss_cpu_complex128" time="1.379" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_triplet_margin_with_distance_loss_cpu_float64" time="0.294" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_unfold_cpu_complex128" time="6.538" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_unfold_cpu_float64" time="1.102" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_upsample_bilinear_cpu_float64" time="0.038" file="test_ops_gradients.py">
      <system-err>c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4084: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4084: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4084: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4084: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4084: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4084: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4084: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4084: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4084: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.")
      </system-err>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nn_functional_upsample_nearest_cpu_float64" time="0.112" file="test_ops_gradients.py">
      <system-err>c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:4028: UserWarning: nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.
  warnings.warn("nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.")
      </system-err>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nonzero_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_nonzero_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_norm_cpu_complex128" time="1.749" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_norm_cpu_float64" time="0.447" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_norm_fro_cpu_complex128" time="0.100" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_norm_fro_cpu_float64" time="0.023" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_norm_inf_cpu_complex128" time="1.706" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_norm_inf_cpu_float64" time="0.233" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_norm_nuc_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_norm_nuc_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_normal_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Gradients are incorrect!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Gradients are incorrect!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_normal_in_place_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_normal_in_place_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_normal_number_mean_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Gradients are incorrect!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Gradients are incorrect!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_ones_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_ones_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_ones_like_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_ones_like_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_ormqr_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_ormqr_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_outer_cpu_complex128" time="0.082" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_outer_cpu_float64" time="0.016" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_pca_lowrank_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_permute_cpu_complex128" time="0.110" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_permute_cpu_float64" time="0.021" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_pinverse_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_pinverse_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_polar_cpu_float64" time="0.191" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_polygamma_polygamma_n_0_cpu_float64" time="0.060" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_polygamma_polygamma_n_1_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_polygamma_polygamma_n_2_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_polygamma_polygamma_n_3_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_polygamma_polygamma_n_4_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_positive_cpu_complex128" time="0.027" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_positive_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_pow_cpu_complex128" time="1.097" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_pow_cpu_float64" time="0.221" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_prod_cpu_complex128" time="2.203" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_prod_cpu_float64" time="0.370" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_put_cpu_complex128" time="1.185" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_put_cpu_float64" time="0.192" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_qr_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_qr_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_quantile_cpu_float64" time="0.965" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_rad2deg_cpu_float64" time="0.007" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_rand_like_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_rand_like_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_randint_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_randint_like_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_randn_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_randn_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_randn_like_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_randn_like_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_ravel_cpu_complex128" time="0.082" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_ravel_cpu_float64" time="0.016" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_real_cpu_complex128" time="0.071" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_real_cpu_float64" time="0.013" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_reciprocal_cpu_complex128" time="0.096" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_reciprocal_cpu_float64" time="0.036" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_remainder_cpu_float64" time="0.145" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_renorm_cpu_complex128" time="0.256" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_renorm_cpu_float64" time="0.049" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_repeat_cpu_complex128" time="0.872" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_repeat_cpu_float64" time="0.110" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_repeat_interleave_cpu_complex128" time="0.138" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_repeat_interleave_cpu_float64" time="0.026" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_reshape_as_cpu_complex128" time="0.105" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_reshape_as_cpu_float64" time="0.038" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_reshape_cpu_complex128" time="0.227" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_reshape_cpu_float64" time="0.034" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_resize__cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_resize__cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_resize_as__cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_resize_as__cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_resolve_conj_cpu_complex128" time="0.049" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_resolve_conj_cpu_float64" time="0.009" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_resolve_neg_cpu_complex128" time="0.047" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_resolve_neg_cpu_float64" time="0.009" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_roll_cpu_complex128" time="0.622" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_roll_cpu_float64" time="0.100" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_rot90_cpu_complex128" time="1.100" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_rot90_cpu_float64" time="0.195" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_round_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_round_decimals_0_cpu_float64" time="0.039" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_round_decimals_3_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_round_decimals_neg_3_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_rsqrt_cpu_complex128" time="0.124" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_rsqrt_cpu_float64" time="0.018" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_rsub_cpu_complex128" time="0.687" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_rsub_cpu_float64" time="0.122" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_scalar_tensor_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_scalar_tensor_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_scatter_add_cpu_complex128" time="0.434" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_scatter_add_cpu_float64" time="0.077" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_scatter_cpu_complex128" time="0.548" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_scatter_cpu_float64" time="0.088" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_scatter_reduce_amax_cpu_float64" time="0.323" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_scatter_reduce_amin_cpu_float64" time="0.333" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_scatter_reduce_mean_cpu_float64" time="0.394" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_scatter_reduce_prod_cpu_float64" time="0.673" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_scatter_reduce_sum_cpu_float64" time="0.401" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_searchsorted_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_select_cpu_complex128" time="0.220" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_select_cpu_float64" time="0.040" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_select_scatter_cpu_float64" time="0.092" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_sgn_cpu_complex128" time="0.084" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_sgn_cpu_float64" time="0.021" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_short_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_short_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_sigmoid_cpu_complex128" time="0.120" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_sigmoid_cpu_float64" time="0.023" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_sign_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_signal_windows_bartlett_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_signal_windows_blackman_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_signal_windows_cosine_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_signal_windows_exponential_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_signal_windows_gaussian_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_signal_windows_general_cosine_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_signal_windows_general_hamming_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_signal_windows_hamming_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_signal_windows_hann_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_signal_windows_kaiser_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_signal_windows_nuttall_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_signbit_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_sin_cpu_complex128" time="0.045" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_sin_cpu_float64" time="0.008" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_sinc_cpu_complex128" time="0.130" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_sinc_cpu_float64" time="0.023" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_sinh_cpu_complex128" time="0.045" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_sinh_cpu_float64" time="0.008" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_slice_cpu_complex128" time="0.536" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_slice_cpu_float64" time="0.069" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_slice_scatter_cpu_float64" time="0.306" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_softmax_cpu_float64" time="0.044" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_softmax_with_dtype_cpu_complex128" time="0.203" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_softmax_with_dtype_cpu_float64" time="0.045" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_sort_cpu_float64" time="0.487" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_sparse_mm_reduce_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_sparse_sampled_addmm_cpu_complex128" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_sparse_sampled_addmm_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_special_airy_ai_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_special_bessel_j0_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_special_bessel_j1_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_special_bessel_y0_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_special_bessel_y1_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_special_chebyshev_polynomial_t_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_special_chebyshev_polynomial_u_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_special_chebyshev_polynomial_v_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipping - testing takes an unreasonably long time, #79528">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipping - testing takes an unreasonably long time, #79528</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_special_chebyshev_polynomial_w_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipping - testing takes an unreasonably long time, #79528">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipping - testing takes an unreasonably long time, #79528</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_special_entr_cpu_float64" time="0.013" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_special_erfcx_cpu_float64" time="0.018" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_special_hermite_polynomial_h_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_special_hermite_polynomial_he_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_special_i0e_cpu_float64" time="0.016" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_special_i1_cpu_float64" time="0.025" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_special_i1e_cpu_float64" time="0.029" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_special_laguerre_polynomial_l_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_special_legendre_polynomial_p_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipping - testing takes an unreasonably long time, #79528">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipping - testing takes an unreasonably long time, #79528</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_special_log_ndtr_cpu_float64" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_special_modified_bessel_i0_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_special_modified_bessel_i1_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_special_modified_bessel_k0_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_special_modified_bessel_k1_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_special_ndtr_cpu_float64" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_special_ndtri_cpu_float64" time="0.020" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_special_polygamma_special_polygamma_n_0_cpu_float64" time="0.102" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_special_scaled_modified_bessel_k0_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_special_scaled_modified_bessel_k1_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_special_shifted_chebyshev_polynomial_t_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipping - testing takes an unreasonably long time, #79528">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipping - testing takes an unreasonably long time, #79528</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_special_shifted_chebyshev_polynomial_u_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipping - testing takes an unreasonably long time, #79528">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipping - testing takes an unreasonably long time, #79528</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_special_shifted_chebyshev_polynomial_v_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipping - testing takes an unreasonably long time, #79528">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipping - testing takes an unreasonably long time, #79528</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_special_shifted_chebyshev_polynomial_w_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipping - testing takes an unreasonably long time, #79528">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipping - testing takes an unreasonably long time, #79528</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_special_spherical_bessel_j0_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_special_xlog1py_cpu_float64" time="0.139" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_special_zeta_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_split_cpu_complex128" time="0.091" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_split_cpu_float64" time="0.016" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_split_list_args_cpu_complex128" time="0.182" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_split_list_args_cpu_float64" time="0.030" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_split_with_sizes_cpu_complex128" time="0.332" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_split_with_sizes_cpu_float64" time="0.039" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_sqrt_cpu_complex128" time="0.042" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_sqrt_cpu_float64" time="0.008" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_square_cpu_complex128" time="0.100" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_square_cpu_float64" time="0.028" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_squeeze_cpu_complex128" time="0.294" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_squeeze_cpu_float64" time="0.039" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_squeeze_multiple_cpu_complex128" time="0.159" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_squeeze_multiple_cpu_float64" time="0.029" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_stack_cpu_complex128" time="0.691" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_stack_cpu_float64" time="0.115" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_std_cpu_complex128" time="0.478" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_std_cpu_float64" time="0.141" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_std_mean_cpu_complex128" time="0.683" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_std_mean_cpu_float64" time="0.125" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_std_mean_unbiased_cpu_complex128" time="0.116" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_std_mean_unbiased_cpu_float64" time="0.025" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_std_unbiased_cpu_complex128" time="0.078" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_std_unbiased_cpu_float64" time="0.018" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_stft_cpu_complex128" time="0.933" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_stft_cpu_float64" time="0.152" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_sub_cpu_complex128" time="0.710" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_sub_cpu_float64" time="0.165" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_sum_cpu_complex128" time="0.561" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_sum_cpu_float64" time="0.110" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_sum_to_size_cpu_complex128" time="0.483" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_sum_to_size_cpu_float64" time="0.100" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_svd_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_svd_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_svd_lowrank_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_t_cpu_complex128" time="0.077" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_t_cpu_float64" time="0.015" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_take_along_dim_cpu_complex128" time="0.153" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_take_along_dim_cpu_float64" time="0.030" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_take_cpu_complex128" time="0.263" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_take_cpu_float64" time="0.052" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_tan_cpu_complex128" time="0.043" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_tan_cpu_float64" time="0.008" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_tanh_cpu_complex128" time="0.038" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_tanh_cpu_float64" time="0.007" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_tensor_split_cpu_complex128" time="0.838" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_tensor_split_cpu_float64" time="0.277" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_tensordot_cpu_complex128" time="0.053" file="test_ops_gradients.py">
      <failure message="torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 1 with respect to input 0,&#10;numerical:tensor(-0.3508+0.0525j, dtype=torch.complex128)&#10;analytical:tensor(0.1578-0.9936j, dtype=torch.complex128)&#10;&#10;The above quantities relating the numerical and analytical jacobians are computed &#10;in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background &#10;about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:&#10;&#10;Numerical:&#10; tensor([[ 0.4517+0.0531j, -0.5595-0.8903j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.4517+0.0531j, -0.5595-0.8903j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.4517+0.0531j, -0.5595-0.8903j,  0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.4517+0.0531j, -0.5595-0.8903j],&#10;        [ 0.9514+0.9457j, -0.4116+0.9680j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.9514+0.9457j, -0.4116+0.9680j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.9514+0.9457j, -0.4116+0.9680j,  0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j,  0.9514+0.9457j, -0.4116+0.9680j]],&#10; dtype=torch.complex128)&#10;Analytical:&#10;tensor([[ 0.4517-0.0531j,  0.9514-0.9457j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.4517-0.0531j,  0.9514-0.9457j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.4517-0.0531j,  0.9514-0.9457j,  0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.4517-0.0531j,  0.9514-0.9457j],&#10;        [-0.5595+0.8903j, -0.4116-0.9680j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j, -0.5595+0.8903j, -0.4116-0.9680j,&#10;          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;         -0.5595+0.8903j, -0.4116-0.9680j,  0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j, -0.5595+0.8903j, -0.4116-0.9680j]],&#10; dtype=torch.complex128)&#10;&#10;The max per-element difference (slow mode) is: 1.9359453532310642.">Traceback (most recent call last):
  File "C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py", line 60, in test_fn_gradgrad
    self._check_helper(device, dtype, op, op.get_op(), 'bwgrad_bwgrad')
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 4329, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args, **kwargs))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 3879, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1694, in gradgradcheck
    return gradcheck(
           ^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1536, in gradcheck
    return _gradcheck_helper(**args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1550, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1157, in _gradcheck_real_imag
    gradcheck_fn(imag_fn, imag_func_out, tupled_inputs, imag_outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1421, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1389, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 1 with respect to input 0,
numerical:tensor(-0.3508+0.0525j, dtype=torch.complex128)
analytical:tensor(0.1578-0.9936j, dtype=torch.complex128)

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[ 0.4517+0.0531j, -0.5595-0.8903j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.4517+0.0531j, -0.5595-0.8903j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.4517+0.0531j, -0.5595-0.8903j,  0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.4517+0.0531j, -0.5595-0.8903j],
        [ 0.9514+0.9457j, -0.4116+0.9680j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.9514+0.9457j, -0.4116+0.9680j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.9514+0.9457j, -0.4116+0.9680j,  0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j,  0.9514+0.9457j, -0.4116+0.9680j]], dtype=torch.complex128)
Analytical:
tensor([[ 0.4517-0.0531j,  0.9514-0.9457j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.4517-0.0531j,  0.9514-0.9457j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.4517-0.0531j,  0.9514-0.9457j,  0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.4517-0.0531j,  0.9514-0.9457j],
        [-0.5595+0.8903j, -0.4116-0.9680j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j, -0.5595+0.8903j, -0.4116-0.9680j,
          0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
         -0.5595+0.8903j, -0.4116-0.9680j,  0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j, -0.5595+0.8903j, -0.4116-0.9680j]], dtype=torch.complex128)

The max per-element difference (slow mode) is: 1.9359453532310642.</failure>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_tensordot_cpu_float64" time="0.035" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_tile_cpu_complex128" time="0.821" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_tile_cpu_float64" time="0.197" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_to_cpu_complex128" time="0.894" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_to_cpu_float64" time="0.328" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_to_sparse_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_to_sparse_cpu_float64" time="0.027" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_topk_cpu_float64" time="0.116" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_trace_cpu_complex128" time="0.046" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_trace_cpu_float64" time="0.007" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_transpose_cpu_complex128" time="0.231" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_transpose_cpu_float64" time="0.099" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_trapezoid_cpu_complex128" time="0.756" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_trapezoid_cpu_float64" time="0.144" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_trapz_cpu_complex128" time="0.791" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_trapz_cpu_float64" time="0.163" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_triangular_solve_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_triangular_solve_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_tril_cpu_complex128" time="0.271" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_tril_cpu_float64" time="0.043" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_triu_cpu_complex128" time="0.547" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_triu_cpu_float64" time="0.292" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_true_divide_cpu_complex128" time="0.963" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_true_divide_cpu_float64" time="0.167" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_trunc_cpu_float64" time="0.008" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_unbind_cpu_complex128" time="0.568" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_unbind_cpu_float64" time="0.099" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_unflatten_cpu_complex128" time="0.272" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_unflatten_cpu_float64" time="0.053" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_unfold_copy_cpu_complex128" time="0.764" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_unfold_copy_cpu_float64" time="0.162" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_unfold_cpu_complex128" time="0.703" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_unfold_cpu_float64" time="0.103" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_uniform_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_uniform_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_unique_consecutive_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_unique_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_unsafe_split_cpu_complex128" time="0.148" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_unsafe_split_cpu_float64" time="0.057" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_unsqueeze_cpu_complex128" time="0.255" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_unsqueeze_cpu_float64" time="0.047" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_var_cpu_complex128" time="0.460" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_var_cpu_float64" time="0.074" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_var_mean_cpu_complex128" time="0.628" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_var_mean_cpu_float64" time="0.106" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_var_mean_unbiased_cpu_complex128" time="0.101" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_var_mean_unbiased_cpu_float64" time="0.020" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_var_unbiased_cpu_complex128" time="0.065" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_var_unbiased_cpu_float64" time="0.015" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_vdot_cpu_complex128" time="0.131" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_vdot_cpu_float64" time="0.013" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_view_as_complex_cpu_float64" time="0.008" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_view_as_cpu_complex128" time="0.107" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_view_as_cpu_float64" time="0.021" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_view_as_real_cpu_complex128" time="0.024" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_view_copy_cpu_float64" time="0.036" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_view_cpu_complex128" time="0.190" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_view_cpu_float64" time="0.073" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_vsplit_cpu_complex128" time="0.148" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_vsplit_cpu_float64" time="0.024" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_vstack_cpu_complex128" time="0.223" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_vstack_cpu_float64" time="0.071" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_where_cpu_complex128" time="0.437" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_where_cpu_float64" time="0.076" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_xlogy_cpu_float64" time="0.215" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_zero__cpu_complex128" time="0.091" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_zero__cpu_float64" time="0.012" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_zeros_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_zeros_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_zeros_like_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_fn_gradgrad_zeros_like_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:54: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_H_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_H_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_T_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_T_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad___getitem___cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad___getitem___cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad___radd___cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad___radd___cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad___rdiv___cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad___rdiv___cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad___rmatmul___cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad___rmatmul___cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad___rmod___cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad___rmul___cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad___rmul___cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad___rpow___cpu_complex128" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad___rpow___cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad___rsub___cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad___rsub___cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad__native_batch_norm_legit_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad__segment_reduce_lengths_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad__segment_reduce_offsets_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad__softmax_backward_data_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad__upsample_bilinear2d_aa_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_abs_cpu_complex128" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="In-place abs not supported for complex tensors">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: In-place abs not supported for complex tensors</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_abs_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_acos_cpu_complex128" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_acos_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_acosh_cpu_complex128" time="0.001" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_acosh_cpu_float64" time="0.001" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_add_cpu_complex128" time="0.084" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_add_cpu_float64" time="0.017" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_addbmm_cpu_complex128" time="0.088" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_addbmm_cpu_float64" time="0.009" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_addcdiv_cpu_complex128" time="0.109" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_addcdiv_cpu_float64" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_addcmul_cpu_complex128" time="0.107" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_addcmul_cpu_float64" time="0.021" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_addmm_cpu_complex128" time="0.006" file="test_ops_gradients.py">
      <failure message="torch.autograd.gradcheck.GradcheckError: Backward is not reentrant, i.e., running backward with same input and grad_output multiple times gives different values, although analytical gradient matches numerical gradient.The tolerance for nondeterminism was 1e-12.&#10;&#10;NOTE: If your op relies on non-deterministic operations i.e., it is listed here:&#10;https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html&#10;this failure might be expected.&#10;&#10;If you are adding a new operator, please file an issue and then use one of the&#10;workarounds. The workaround depends on how your test invokes gradcheck/gradgradcheck.&#10;If the test&#10;- manually invokes gradcheck/gradgradcheck, then call gradcheck/gradgradcheck&#10;  with `nondet_tol=&lt;tol&gt;` as a keyword argument.&#10;- is OpInfo-based (e.g., in test_ops_gradients.py), then modify the OpInfo for the test&#10;  to have `gradcheck_nondet_tol=&lt;tol&gt;`.&#10;- is a Module test (e.g., in common_nn.py), then modify the corresponding&#10;  module_test entry to have `gradcheck_nondet_tol=&lt;tol&gt;`">Traceback (most recent call last):
  File "C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py", line 51, in test_inplace_grad
    self._grad_test_helper(device, dtype, op, self._get_safe_inplace(op.get_inplace()))
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 4335, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 4304, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 3859, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1536, in gradcheck
    return _gradcheck_helper(**args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1550, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1157, in _gradcheck_real_imag
    gradcheck_fn(imag_fn, imag_func_out, tupled_inputs, imag_outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1419, in _fast_gradcheck
    analytical_vJu = _get_analytical_vJu_backward_mode(inputs, outputs, nondet_tol, check_grad_dtypes, all_v, all_u_dense)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 646, in _get_analytical_vJu_backward_mode
    all_vJ = _check_analytical_jacobian_attributes(inputs, output, nondet_tol, check_grad_dtypes,
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 635, in _check_analytical_jacobian_attributes
    raise GradcheckError('Backward is not reentrant, i.e., running backward with '
torch.autograd.gradcheck.GradcheckError: Backward is not reentrant, i.e., running backward with same input and grad_output multiple times gives different values, although analytical gradient matches numerical gradient.The tolerance for nondeterminism was 1e-12.

NOTE: If your op relies on non-deterministic operations i.e., it is listed here:
https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html
this failure might be expected.

If you are adding a new operator, please file an issue and then use one of the
workarounds. The workaround depends on how your test invokes gradcheck/gradgradcheck.
If the test
- manually invokes gradcheck/gradgradcheck, then call gradcheck/gradgradcheck
  with `nondet_tol=&lt;tol&gt;` as a keyword argument.
- is OpInfo-based (e.g., in test_ops_gradients.py), then modify the OpInfo for the test
  to have `gradcheck_nondet_tol=&lt;tol&gt;`.
- is a Module test (e.g., in common_nn.py), then modify the corresponding
  module_test entry to have `gradcheck_nondet_tol=&lt;tol&gt;`</failure>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_addmm_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_addmm_decomposed_cpu_complex128" time="0.023" file="test_ops_gradients.py">
      <failure message="torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 0 with respect to input 1,&#10;numerical:tensor(-1.0959-0.9321j, dtype=torch.complex128)&#10;analytical:tensor(0.0664-3.4710j, dtype=torch.complex128)&#10;&#10;The above quantities relating the numerical and analytical jacobians are computed &#10;in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background &#10;about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:&#10;&#10;Numerical:&#10; tensor([[ 4.6135-2.3432j, -7.9438-6.6799j, -7.7087+3.2204j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [-5.0572-7.8602j,  5.8520+8.8194j,  2.8467+6.9115j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  4.6135-2.3432j,&#10;         -7.9438-6.6799j, -7.7087+3.2204j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j, -5.0572-7.8602j,&#10;          5.8520+8.8194j,  2.8467+6.9115j]], dtype=torch.complex128)&#10;Analytical:&#10;tensor([[ 2.3963e+02+1.1108e+02j, -1.3818e+01-8.0265e+01j,&#10;         1.4735e-305-6.3354e-306j, -5.8741e+00-8.6945e+01j,&#10;          2.4425e+02+1.1342e+02j, -9.6489e+00+2.3096e+01j],&#10;        [-9.5322e+01+2.1118e+02j,  7.1751e+01-1.2724e+01j,&#10;         5.7345e-306+1.3155e-305j,  6.5899e+01-3.9043e+00j,&#10;         -1.0038e+02+2.1904e+02j, -2.0708e+01-8.5291e+00j],&#10;        [-1.8620e+02+4.4569e+02j, -5.6925e+01-6.4369e+01j,&#10;         1.4561e-305-3.9906e-306j, -5.2312e+01-6.2026e+01j,&#10;         -1.7825e+02+4.3901e+02j, -2.5546e+01+6.1149e+01j],&#10;        [-3.9960e+02-1.6459e+02j,  6.0317e+01-5.4942e+01j,&#10;         3.6358e-306+1.3010e-305j,  5.5259e+01-4.7082e+01j,&#10;         -4.0545e+02-1.5577e+02j, -5.4825e+01-2.2581e+01j]],&#10; dtype=torch.complex128)&#10;&#10;The max per-element difference (slow mode) is: 483.0219947253576.">Traceback (most recent call last):
  File "C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py", line 51, in test_inplace_grad
    self._grad_test_helper(device, dtype, op, self._get_safe_inplace(op.get_inplace()))
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 4335, in _grad_test_helper
    return self._check_helper(device, dtype, op, variant, 'gradcheck', check_forward_ad=check_forward_ad,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 4304, in _check_helper
    self.assertTrue(gradcheck(fn, gradcheck_args,
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 3859, in gradcheck
    return torch.autograd.gradcheck(fn, inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1536, in gradcheck
    return _gradcheck_helper(**args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1550, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1157, in _gradcheck_real_imag
    gradcheck_fn(imag_fn, imag_func_out, tupled_inputs, imag_outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1421, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1389, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 0 with respect to input 1,
numerical:tensor(-1.0959-0.9321j, dtype=torch.complex128)
analytical:tensor(0.0664-3.4710j, dtype=torch.complex128)

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[ 4.6135-2.3432j, -7.9438-6.6799j, -7.7087+3.2204j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [-5.0572-7.8602j,  5.8520+8.8194j,  2.8467+6.9115j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  4.6135-2.3432j,
         -7.9438-6.6799j, -7.7087+3.2204j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j, -5.0572-7.8602j,
          5.8520+8.8194j,  2.8467+6.9115j]], dtype=torch.complex128)
Analytical:
tensor([[ 2.3963e+02+1.1108e+02j, -1.3818e+01-8.0265e+01j,
         1.4735e-305-6.3354e-306j, -5.8741e+00-8.6945e+01j,
          2.4425e+02+1.1342e+02j, -9.6489e+00+2.3096e+01j],
        [-9.5322e+01+2.1118e+02j,  7.1751e+01-1.2724e+01j,
         5.7345e-306+1.3155e-305j,  6.5899e+01-3.9043e+00j,
         -1.0038e+02+2.1904e+02j, -2.0708e+01-8.5291e+00j],
        [-1.8620e+02+4.4569e+02j, -5.6925e+01-6.4369e+01j,
         1.4561e-305-3.9906e-306j, -5.2312e+01-6.2026e+01j,
         -1.7825e+02+4.3901e+02j, -2.5546e+01+6.1149e+01j],
        [-3.9960e+02-1.6459e+02j,  6.0317e+01-5.4942e+01j,
         3.6358e-306+1.3010e-305j,  5.5259e+01-4.7082e+01j,
         -4.0545e+02-1.5577e+02j, -5.4825e+01-2.2581e+01j]], dtype=torch.complex128)

The max per-element difference (slow mode) is: 483.0219947253576.</failure>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_addmm_decomposed_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_addmv_cpu_complex128" time="0.036" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_addmv_cpu_float64" time="0.008" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_addr_cpu_complex128" time="0.036" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_addr_cpu_float64" time="0.008" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_all_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_all_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_allclose_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_allclose_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_amax_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_amin_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_aminmax_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_angle_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_angle_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_any_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_any_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_arange_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_argmax_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_argmin_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_argsort_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_argwhere_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_argwhere_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_as_strided_cpu_complex128" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Numerous errors">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Numerous errors</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_as_strided_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Numerous errors">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Numerous errors</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_as_strided_partial_views_cpu_complex128" time="0.010" file="test_ops_gradients.py">
      <skipped type="pytest.xfail" message="" />
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_as_strided_partial_views_cpu_float64" time="0.005" file="test_ops_gradients.py">
      <skipped type="pytest.xfail" message="" />
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_as_strided_scatter_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_as_strided_scatter_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_asin_cpu_complex128" time="0.013" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_asin_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_asinh_cpu_complex128" time="0.001" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_asinh_cpu_float64" time="0.001" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_atan2_cpu_float64" time="0.014" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_atan_cpu_complex128" time="0.011" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_atan_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_atanh_cpu_complex128" time="0.001" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_atanh_cpu_float64" time="0.001" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_atleast_1d_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_atleast_1d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_atleast_2d_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_atleast_2d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_atleast_3d_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_atleast_3d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_baddbmm_cpu_complex128" time="0.146" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_baddbmm_cpu_float64" time="0.009" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_bernoulli_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_bfloat16_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_bfloat16_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_block_diag_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_block_diag_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_bmm_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_bmm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_bool_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_bool_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_broadcast_tensors_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_broadcast_tensors_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_broadcast_to_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_broadcast_to_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_bucketize_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_byte_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_byte_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_cartesian_prod_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_cartesian_prod_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_cat_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="TODO(whc) fix pre-existing bug with cat for newly added opinfo for empty+nonempty">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: TODO(whc) fix pre-existing bug with cat for newly added opinfo for empty+nonempty</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_cat_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="TODO(whc) fix pre-existing bug with cat for newly added opinfo for empty+nonempty">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: TODO(whc) fix pre-existing bug with cat for newly added opinfo for empty+nonempty</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_cauchy_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_cdist_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_cdouble_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_cdouble_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_ceil_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_cfloat_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_cfloat_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_chalf_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_chalf_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_char_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_char_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_cholesky_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_cholesky_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_cholesky_inverse_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_cholesky_inverse_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_cholesky_solve_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_cholesky_solve_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_chunk_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_chunk_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_clamp_cpu_float64" time="0.017" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_clamp_max_cpu_float64" time="0.014" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_clamp_min_cpu_float64" time="0.014" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_clone_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_clone_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_column_stack_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_column_stack_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_combinations_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_combinations_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_complex_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_conj_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_conj_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_conj_physical_cpu_complex128" time="0.010" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_conj_physical_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_constant_pad_nd_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_constant_pad_nd_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_contiguous_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_contiguous_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_copysign_cpu_float64" time="0.014" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_corrcoef_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_corrcoef_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_cos_cpu_complex128" time="0.023" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_cos_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_cosh_cpu_complex128" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_cosh_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_count_nonzero_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_count_nonzero_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_cov_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_cov_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_cross_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_cross_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_cummax_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_cummin_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_cumprod_cpu_complex128" time="5.916" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_cumprod_cpu_float64" time="0.832" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_cumsum_cpu_complex128" time="0.061" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_cumsum_cpu_float64" time="0.024" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_cumulative_trapezoid_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_cumulative_trapezoid_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_deg2rad_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_diag_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_diag_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_diag_embed_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_diag_embed_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_diagflat_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_diagflat_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_diagonal_copy_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_diagonal_copy_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_diagonal_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_diagonal_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_diagonal_scatter_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_diagonal_scatter_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_diff_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_diff_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_digamma_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_dist_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_dist_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_div_floor_rounding_cpu_float64" time="0.015" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_div_no_rounding_mode_cpu_complex128" time="0.069" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_div_no_rounding_mode_cpu_float64" time="0.014" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_div_trunc_rounding_cpu_float64" time="0.014" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_dot_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_dot_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_double_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_double_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_dsplit_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_dsplit_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_dstack_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_dstack_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_einsum_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_einsum_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_empty_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_empty_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_empty_like_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_empty_like_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_empty_permuted_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_empty_permuted_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_eq_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_eq_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_equal_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_equal_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_erf_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_erfc_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_erfinv_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_exp2_cpu_complex128" time="0.023" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_exp2_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_exp_cpu_complex128" time="0.021" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_exp_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_expand_as_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_expand_as_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_expand_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_expand_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_expm1_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_exponential_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_eye_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_eye_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fft_fft2_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fft_fft2_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fft_fft_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fft_fft_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fft_fftn_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fft_fftn_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fft_fftshift_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fft_fftshift_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fft_hfft2_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fft_hfft2_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fft_hfft_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fft_hfft_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fft_hfftn_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fft_hfftn_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fft_ifft2_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fft_ifft2_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fft_ifft_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fft_ifft_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fft_ifftn_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fft_ifftn_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fft_ifftshift_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fft_ifftshift_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fft_ihfft2_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fft_ihfft_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fft_ihfftn_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fft_irfft2_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fft_irfft2_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fft_irfft_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fft_irfft_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fft_irfftn_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fft_irfftn_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fft_rfft2_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fft_rfft_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fft_rfftn_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fill_cpu_complex128" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fill_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_flatten_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_flatten_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_flip_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_flip_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fliplr_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fliplr_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_flipud_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_flipud_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_float_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_float_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_float_power_cpu_complex128" time="0.081" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_float_power_cpu_float64" time="0.016" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_floor_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_floor_divide_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fmax_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fmin_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_fmod_cpu_float64" time="0.014" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_frac_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_frexp_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_full_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_full_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_full_like_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_full_like_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_gather_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_gather_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_ge_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_geometric_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_geqrf_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_geqrf_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_gradient_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_gradient_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_grid_sampler_2d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_gt_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_half_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_half_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_heaviside_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_histc_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_histogram_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_histogramdd_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_hsplit_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_hsplit_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_hstack_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_hstack_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_hypot_cpu_float64" time="0.013" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_i0_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_igamma_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_igammac_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_imag_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_index_add_cpu_complex128" time="0.110" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_index_add_cpu_float64" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_index_copy_cpu_complex128" time="0.036" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_index_copy_cpu_float64" time="0.009" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_index_fill_cpu_complex128" time="0.059" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_index_fill_cpu_float64" time="0.013" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_index_put_cpu_complex128" time="0.052" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_index_put_cpu_float64" time="0.011" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_index_reduce_cpu_float64" time="0.097" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_index_select_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_index_select_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_inner_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_inner_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_int_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_int_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_isclose_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_isclose_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_isfinite_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_isfinite_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_isin_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_isinf_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_isinf_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_isnan_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_isnan_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_isneginf_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_isposinf_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_isreal_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_isreal_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_istft_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_jiterator_2inputs_2outputs_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_jiterator_2inputs_2outputs_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_jiterator_4inputs_with_extra_args_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_jiterator_4inputs_with_extra_args_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_jiterator_binary_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_jiterator_binary_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_jiterator_binary_return_by_ref_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_jiterator_binary_return_by_ref_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_jiterator_unary_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_jiterator_unary_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_kron_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_kron_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_kthvalue_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_ldexp_cpu_complex128" time="0.002" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_ldexp_cpu_float64" time="0.002" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_le_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_lerp_cpu_complex128" time="0.191" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_lerp_cpu_float64" time="0.020" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_lgamma_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_cholesky_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_cholesky_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_cholesky_ex_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_cholesky_ex_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_cond_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_cond_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_cross_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_cross_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_det_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_det_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_det_singular_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_det_singular_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_diagonal_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_diagonal_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_eig_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_eig_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_eigh_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_eigh_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_eigvals_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_eigvals_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_eigvalsh_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_eigvalsh_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_householder_product_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_householder_product_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_inv_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_inv_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_inv_ex_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_inv_ex_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_ldl_factor_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_ldl_factor_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_ldl_factor_ex_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_ldl_factor_ex_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_ldl_solve_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_ldl_solve_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_lstsq_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_lstsq_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_lstsq_grad_oriented_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_lstsq_grad_oriented_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_lu_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_lu_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_lu_factor_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_lu_factor_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_lu_factor_ex_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_lu_factor_ex_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_lu_solve_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_lu_solve_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_matrix_norm_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_matrix_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_matrix_power_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_matrix_power_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_matrix_rank_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_matrix_rank_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_matrix_rank_hermitian_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_matrix_rank_hermitian_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_multi_dot_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_multi_dot_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_norm_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_norm_subgradients_at_zero_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_norm_subgradients_at_zero_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_pinv_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_pinv_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_pinv_hermitian_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_pinv_hermitian_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_pinv_singular_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_pinv_singular_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_qr_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_qr_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_slogdet_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_slogdet_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_solve_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_solve_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_solve_ex_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_solve_ex_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_solve_triangular_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_solve_triangular_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_svd_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_svd_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_svdvals_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_svdvals_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_tensorinv_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_tensorinv_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_tensorsolve_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_tensorsolve_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_vander_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_vander_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_vecdot_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_vecdot_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_vector_norm_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linalg_vector_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linspace_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_linspace_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_log10_cpu_complex128" time="0.023" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_log10_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_log1p_cpu_complex128" time="0.011" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_log1p_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_log2_cpu_complex128" time="0.023" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_log2_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_log_cpu_complex128" time="0.023" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_log_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_log_normal_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_log_softmax_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_log_softmax_with_dtype_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_log_softmax_with_dtype_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_logaddexp2_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_logaddexp_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_logaddexp_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_logcumsumexp_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_logcumsumexp_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_logdet_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_logdet_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_logical_and_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_logical_and_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_logical_not_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_logical_not_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_logical_or_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_logical_or_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_logical_xor_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_logical_xor_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_logit_cpu_float64" time="0.008" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_logspace_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_logspace_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_logsumexp_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_long_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_long_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_lt_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_lu_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_lu_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_lu_solve_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_lu_solve_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_lu_unpack_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_lu_unpack_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_mH_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_mH_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_mT_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_mT_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_masked_amax_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_masked_amin_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_masked_argmax_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_masked_argmin_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_masked_cumprod_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_masked_cumprod_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_masked_cumsum_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_masked_cumsum_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_masked_fill_cpu_complex128" time="0.055" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_masked_fill_cpu_float64" time="0.013" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_masked_log_softmax_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_masked_logaddexp_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_masked_logsumexp_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_masked_mean_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_masked_mean_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_masked_median_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_masked_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_masked_normalize_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_masked_normalize_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_masked_prod_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_masked_prod_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_masked_scatter_cpu_complex128" time="0.039" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_masked_scatter_cpu_float64" time="0.010" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_masked_select_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_masked_select_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_masked_softmax_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_masked_softmin_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_masked_std_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_masked_std_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_masked_sum_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_masked_sum_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_masked_var_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_masked_var_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_matmul_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_matmul_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_matrix_exp_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_matrix_exp_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_max_binary_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_max_pool2d_with_indices_backward_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_max_reduction_no_dim_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_max_reduction_with_dim_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_maximum_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_mean_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_mean_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_median_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_meshgrid_list_of_tensors_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_meshgrid_list_of_tensors_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_meshgrid_variadic_tensors_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_meshgrid_variadic_tensors_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_min_binary_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_min_reduction_no_dim_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_min_reduction_with_dim_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_minimum_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_mm_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_mm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_mode_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_movedim_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_movedim_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_msort_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_mul_cpu_complex128" time="0.064" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_mul_cpu_float64" time="0.013" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_multinomial_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_mv_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_mv_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_mvlgamma_mvlgamma_p_1_cpu_float64" time="0.019" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_mvlgamma_mvlgamma_p_3_cpu_float64" time="0.020" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_mvlgamma_mvlgamma_p_5_cpu_float64" time="0.019" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nan_to_num_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nanmean_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nanmedian_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nanquantile_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nansum_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_narrow_copy_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_narrow_copy_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_narrow_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_narrow_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_native_batch_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_native_dropout_backward_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_native_layer_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_ne_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_ne_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_neg_cpu_complex128" time="0.009" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_neg_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_new_empty_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_new_empty_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_new_empty_strided_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_new_empty_strided_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_new_full_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_new_full_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_new_ones_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_new_ones_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_new_zeros_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_new_zeros_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nextafter_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_adaptive_avg_pool1d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_adaptive_avg_pool2d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_adaptive_avg_pool3d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_adaptive_max_pool1d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_adaptive_max_pool2d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_adaptive_max_pool3d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_alpha_dropout_cpu_float64" time="0.140" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_avg_pool1d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_avg_pool2d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_avg_pool3d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_batch_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_bilinear_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_binary_cross_entropy_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_binary_cross_entropy_with_logits_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_celu_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_conv1d_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_conv1d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_conv2d_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_conv2d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_conv_transpose1d_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_conv_transpose1d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_conv_transpose2d_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_conv_transpose2d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_conv_transpose3d_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_conv_transpose3d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_cosine_embedding_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_cosine_similarity_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_cross_entropy_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_ctc_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_dropout2d_cpu_float64" time="0.064" file="test_ops_gradients.py">
      <system-err>c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
      </system-err>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_dropout3d_cpu_float64" time="0.069" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_dropout_cpu_float64" time="0.090" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_elu_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_embedding_bag_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_embedding_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_feature_alpha_dropout_with_train_cpu_float64" time="0.040" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_feature_alpha_dropout_without_train_cpu_complex128" time="0.423" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_feature_alpha_dropout_without_train_cpu_float64" time="0.072" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_fractional_max_pool2d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_fractional_max_pool3d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_gaussian_nll_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_gelu_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_glu_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_grid_sample_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_group_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_hardshrink_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_hardsigmoid_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_hardswish_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_hardtanh_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_hinge_embedding_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_huber_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_instance_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_interpolate_area_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_interpolate_bicubic_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_interpolate_bilinear_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_interpolate_linear_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_interpolate_nearest_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_interpolate_trilinear_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_kl_div_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_l1_loss_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_l1_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_layer_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_leaky_relu_cpu_float64" time="0.015" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_linear_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_linear_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_local_response_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_logsigmoid_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_margin_ranking_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_max_pool1d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_max_pool2d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_max_pool3d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_max_unpool1d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_max_unpool1d_grad_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_max_unpool2d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_max_unpool2d_grad_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_max_unpool3d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_max_unpool3d_grad_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_mish_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_mse_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_multi_margin_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_multilabel_margin_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_multilabel_soft_margin_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_nll_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_normalize_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_normalize_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_pad_circular_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_pad_circular_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_pad_constant_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_pad_constant_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_pad_reflect_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_pad_reflect_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_pad_replicate_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_pad_replicate_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_pairwise_distance_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_pairwise_distance_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_pdist_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_pixel_shuffle_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_pixel_shuffle_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_pixel_unshuffle_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_pixel_unshuffle_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_poisson_nll_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_prelu_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_relu6_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_relu_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_rrelu_cpu_float64" time="0.024" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_scaled_dot_product_attention_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_selu_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_silu_complex_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_silu_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_smooth_l1_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_soft_margin_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_softmin_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_softmin_with_dtype_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_softmin_with_dtype_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_softplus_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_softshrink_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_softsign_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_softsign_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_tanhshrink_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_tanhshrink_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_threshold_cpu_float64" time="0.008" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_triplet_margin_loss_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_triplet_margin_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_triplet_margin_with_distance_loss_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_triplet_margin_with_distance_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_unfold_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_unfold_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_upsample_bilinear_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nn_functional_upsample_nearest_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nonzero_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_nonzero_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_norm_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_norm_fro_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_norm_fro_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_norm_inf_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_norm_inf_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_norm_nuc_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_norm_nuc_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_normal_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Gradients are incorrect!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Gradients are incorrect!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_normal_in_place_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_normal_in_place_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_normal_number_mean_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Gradients are incorrect!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Gradients are incorrect!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_ones_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_ones_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_ones_like_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_ones_like_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_ormqr_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_ormqr_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_outer_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_outer_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_pca_lowrank_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_permute_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_permute_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_pinverse_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_pinverse_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_polar_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_polygamma_polygamma_n_0_cpu_float64" time="0.017" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_polygamma_polygamma_n_1_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_polygamma_polygamma_n_2_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_polygamma_polygamma_n_3_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_polygamma_polygamma_n_4_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_positive_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_positive_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_pow_cpu_complex128" time="0.002" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_pow_cpu_float64" time="0.002" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_prod_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_prod_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_put_cpu_complex128" time="0.258" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_put_cpu_float64" time="0.051" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_qr_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_qr_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_quantile_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_rad2deg_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_rand_like_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_rand_like_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_randint_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_randint_like_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_randn_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_randn_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_randn_like_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_randn_like_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_ravel_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_ravel_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_real_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_real_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_reciprocal_cpu_complex128" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_reciprocal_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_remainder_cpu_float64" time="0.014" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_renorm_cpu_complex128" time="0.044" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_renorm_cpu_float64" time="0.010" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_repeat_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_repeat_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_repeat_interleave_cpu_complex128" time="0.101" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_repeat_interleave_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_reshape_as_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_reshape_as_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_reshape_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_reshape_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_resize__cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_resize__cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_resize_as__cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_resize_as__cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_resolve_conj_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_resolve_conj_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_resolve_neg_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_resolve_neg_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_roll_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_roll_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_rot90_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_rot90_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_round_cpu_float64" time="0.004" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_round_decimals_0_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_round_decimals_3_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_round_decimals_neg_3_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_rsqrt_cpu_complex128" time="0.023" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_rsqrt_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_rsub_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_rsub_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_scalar_tensor_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_scalar_tensor_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_scatter_add_cpu_complex128" time="0.083" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_scatter_add_cpu_float64" time="0.018" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_scatter_cpu_complex128" time="0.092" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_scatter_cpu_float64" time="0.019" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_scatter_reduce_amax_cpu_float64" time="0.059" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_scatter_reduce_amin_cpu_float64" time="0.059" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_scatter_reduce_mean_cpu_float64" time="0.064" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_scatter_reduce_prod_cpu_float64" time="0.075" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_scatter_reduce_sum_cpu_float64" time="0.052" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_searchsorted_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_select_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_select_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_select_scatter_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_sgn_cpu_complex128" time="0.011" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_sgn_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_short_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_short_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_sigmoid_cpu_complex128" time="0.021" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_sigmoid_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_sign_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_signal_windows_bartlett_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_signal_windows_blackman_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_signal_windows_cosine_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_signal_windows_exponential_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_signal_windows_gaussian_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_signal_windows_general_cosine_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_signal_windows_general_hamming_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_signal_windows_hamming_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_signal_windows_hann_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_signal_windows_kaiser_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_signal_windows_nuttall_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_signbit_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_sin_cpu_complex128" time="0.012" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_sin_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_sinc_cpu_complex128" time="0.026" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_sinc_cpu_float64" time="0.007" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_sinh_cpu_complex128" time="0.012" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_sinh_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_slice_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_slice_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_slice_scatter_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_softmax_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_softmax_with_dtype_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_softmax_with_dtype_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_sort_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_sparse_mm_reduce_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_sparse_sampled_addmm_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch is built without MKL support">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch is built without MKL support</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_sparse_sampled_addmm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch is built without MKL support">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch is built without MKL support</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_special_airy_ai_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_special_bessel_j0_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_special_bessel_j1_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_special_bessel_y0_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_special_bessel_y1_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_special_chebyshev_polynomial_t_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_special_chebyshev_polynomial_u_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_special_chebyshev_polynomial_v_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipping - testing takes an unreasonably long time, #79528">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipping - testing takes an unreasonably long time, #79528</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_special_chebyshev_polynomial_w_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipping - testing takes an unreasonably long time, #79528">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipping - testing takes an unreasonably long time, #79528</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_special_entr_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_special_erfcx_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_special_hermite_polynomial_h_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_special_hermite_polynomial_he_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_special_i0e_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_special_i1_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_special_i1e_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_special_laguerre_polynomial_l_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_special_legendre_polynomial_p_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipping - testing takes an unreasonably long time, #79528">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipping - testing takes an unreasonably long time, #79528</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_special_log_ndtr_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_special_modified_bessel_i0_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_special_modified_bessel_i1_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_special_modified_bessel_k0_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_special_modified_bessel_k1_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_special_ndtr_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_special_ndtri_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_special_polygamma_special_polygamma_n_0_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_special_scaled_modified_bessel_k0_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_special_scaled_modified_bessel_k1_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_special_shifted_chebyshev_polynomial_t_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipping - testing takes an unreasonably long time, #79528">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipping - testing takes an unreasonably long time, #79528</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_special_shifted_chebyshev_polynomial_u_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipping - testing takes an unreasonably long time, #79528">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipping - testing takes an unreasonably long time, #79528</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_special_shifted_chebyshev_polynomial_v_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipping - testing takes an unreasonably long time, #79528">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipping - testing takes an unreasonably long time, #79528</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_special_shifted_chebyshev_polynomial_w_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipping - testing takes an unreasonably long time, #79528">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipping - testing takes an unreasonably long time, #79528</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_special_spherical_bessel_j0_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_special_xlog1py_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_special_zeta_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_split_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_split_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_split_list_args_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_split_list_args_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_split_with_sizes_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_split_with_sizes_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_sqrt_cpu_complex128" time="0.010" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_sqrt_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_square_cpu_complex128" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_square_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_squeeze_cpu_complex128" time="0.056" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_squeeze_cpu_float64" time="0.013" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_squeeze_multiple_cpu_complex128" time="0.042" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_squeeze_multiple_cpu_float64" time="0.010" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_stack_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_stack_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_std_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_std_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_std_mean_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_std_mean_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_std_mean_unbiased_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_std_mean_unbiased_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_std_unbiased_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_std_unbiased_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_stft_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_stft_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_sub_cpu_complex128" time="0.084" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_sub_cpu_float64" time="0.017" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_sum_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_sum_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_sum_to_size_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_sum_to_size_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_svd_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_svd_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_svd_lowrank_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_t_cpu_complex128" time="0.021" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_t_cpu_float64" time="0.005" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_take_along_dim_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_take_along_dim_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_take_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_take_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_tan_cpu_complex128" time="0.011" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_tan_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_tanh_cpu_complex128" time="0.010" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_tanh_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_tensor_split_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_tensor_split_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_tensordot_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_tensordot_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_tile_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_tile_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_to_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_to_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_to_sparse_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_to_sparse_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_topk_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_trace_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_trace_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_transpose_cpu_complex128" time="0.057" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_transpose_cpu_float64" time="0.013" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_trapezoid_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_trapezoid_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_trapz_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_trapz_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_triangular_solve_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_triangular_solve_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_tril_cpu_complex128" time="0.081" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_tril_cpu_float64" time="0.014" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_triu_cpu_complex128" time="0.072" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_triu_cpu_float64" time="0.015" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_true_divide_cpu_complex128" time="0.069" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_true_divide_cpu_float64" time="0.014" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_trunc_cpu_float64" time="0.003" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_unbind_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_unbind_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_unflatten_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_unflatten_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_unfold_copy_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_unfold_copy_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_unfold_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_unfold_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_uniform_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_uniform_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_unique_consecutive_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_unique_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_unsafe_split_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_unsafe_split_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_unsqueeze_cpu_complex128" time="0.063" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_unsqueeze_cpu_float64" time="0.014" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_var_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_var_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_var_mean_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_var_mean_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_var_mean_unbiased_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_var_mean_unbiased_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_var_unbiased_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_var_unbiased_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_vdot_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_vdot_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_view_as_complex_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_view_as_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_view_as_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_view_as_real_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_view_copy_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_view_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_view_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_vsplit_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_vsplit_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_vstack_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_vstack_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_where_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_where_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Op has no inplace variant!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Op has no inplace variant!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_xlogy_cpu_float64" time="0.014" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_zero__cpu_complex128" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_zero__cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_zeros_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_zeros_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_zeros_like_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_grad_zeros_like_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:35: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_H_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_H_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_T_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_T_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad___getitem___cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad___getitem___cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad___radd___cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad___radd___cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad___rdiv___cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad___rdiv___cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad___rmatmul___cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad___rmatmul___cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad___rmod___cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad___rmul___cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad___rmul___cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad___rpow___cpu_complex128" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad___rpow___cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad___rsub___cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad___rsub___cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad__native_batch_norm_legit_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad__segment_reduce_lengths_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad__segment_reduce_offsets_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad__softmax_backward_data_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad__upsample_bilinear2d_aa_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_abs_cpu_complex128" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="In-place abs not supported for complex tensors">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: In-place abs not supported for complex tensors</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_abs_cpu_float64" time="0.008" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_acos_cpu_complex128" time="0.125" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_acos_cpu_float64" time="0.021" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_acosh_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_acosh_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_add_cpu_complex128" time="0.552" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_add_cpu_float64" time="0.115" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_addbmm_cpu_complex128" time="0.819" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_addbmm_cpu_float64" time="0.053" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_addcdiv_cpu_complex128" time="0.910" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_addcdiv_cpu_float64" time="0.150" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_addcmul_cpu_complex128" time="0.723" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_addcmul_cpu_float64" time="0.127" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_addmm_cpu_complex128" time="0.061" file="test_ops_gradients.py">
      <failure message="torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 2 with respect to input 1,&#10;numerical:tensor(-0.1847-2.1866j, dtype=torch.complex128)&#10;analytical:tensor(1.2251+1.1312j, dtype=torch.complex128)&#10;&#10;The above quantities relating the numerical and analytical jacobians are computed &#10;in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background &#10;about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:&#10;&#10;Numerical:&#10; tensor([[  1.4334e+00+1.9862e+00j,   1.1185e+00-4.1834e+00j,&#10;         -3.8274e-306+5.8302e-306j,   0.0000e+00+0.0000e+00j,&#10;           0.0000e+00+0.0000e+00j,   0.0000e+00+0.0000e+00j],&#10;        [  0.0000e+00+0.0000e+00j,   0.0000e+00+0.0000e+00j,&#10;           0.0000e+00+0.0000e+00j,   1.4334e+00+1.9862e+00j,&#10;           1.1185e+00-4.1834e+00j, -3.8274e-306+5.8302e-306j],&#10;        [ -2.5487e+00+1.4633e+00j,   4.8637e-02-1.9648e+00j,&#10;         -1.4686e-306+6.7646e-306j,   0.0000e+00+0.0000e+00j,&#10;           0.0000e+00+0.0000e+00j,   0.0000e+00+0.0000e+00j],&#10;        [  0.0000e+00+0.0000e+00j,   0.0000e+00+0.0000e+00j,&#10;           0.0000e+00+0.0000e+00j,  -2.5487e+00+1.4633e+00j,&#10;           4.8637e-02-1.9648e+00j, -1.4686e-306+6.7646e-306j]],&#10; dtype=torch.complex128)&#10;Analytical:&#10;tensor([[ 1.4334-1.9862j, -2.5487-1.4633j,  1.9870-1.7309j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  1.4334-1.9862j,&#10;         -2.5487-1.4633j,  1.9870-1.7309j],&#10;        [ 1.1185+4.1834j,  0.0486+1.9648j,  1.9194+2.2038j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  1.1185+4.1834j,&#10;          0.0486+1.9648j,  1.9194+2.2038j]], dtype=torch.complex128)&#10;&#10;The max per-element difference (slow mode) is: 4.565870490719988.">Traceback (most recent call last):
  File "C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py", line 85, in test_inplace_gradgrad
    self._check_helper(device, dtype, op, self._get_safe_inplace(op.get_inplace()), "bwgrad_bwgrad")
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 4329, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args, **kwargs))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 3879, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1694, in gradgradcheck
    return gradcheck(
           ^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1536, in gradcheck
    return _gradcheck_helper(**args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1550, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1157, in _gradcheck_real_imag
    gradcheck_fn(imag_fn, imag_func_out, tupled_inputs, imag_outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1421, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1389, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 2 with respect to input 1,
numerical:tensor(-0.1847-2.1866j, dtype=torch.complex128)
analytical:tensor(1.2251+1.1312j, dtype=torch.complex128)

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[  1.4334e+00+1.9862e+00j,   1.1185e+00-4.1834e+00j,
         -3.8274e-306+5.8302e-306j,   0.0000e+00+0.0000e+00j,
           0.0000e+00+0.0000e+00j,   0.0000e+00+0.0000e+00j],
        [  0.0000e+00+0.0000e+00j,   0.0000e+00+0.0000e+00j,
           0.0000e+00+0.0000e+00j,   1.4334e+00+1.9862e+00j,
           1.1185e+00-4.1834e+00j, -3.8274e-306+5.8302e-306j],
        [ -2.5487e+00+1.4633e+00j,   4.8637e-02-1.9648e+00j,
         -1.4686e-306+6.7646e-306j,   0.0000e+00+0.0000e+00j,
           0.0000e+00+0.0000e+00j,   0.0000e+00+0.0000e+00j],
        [  0.0000e+00+0.0000e+00j,   0.0000e+00+0.0000e+00j,
           0.0000e+00+0.0000e+00j,  -2.5487e+00+1.4633e+00j,
           4.8637e-02-1.9648e+00j, -1.4686e-306+6.7646e-306j]], dtype=torch.complex128)
Analytical:
tensor([[ 1.4334-1.9862j, -2.5487-1.4633j,  1.9870-1.7309j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  1.4334-1.9862j,
         -2.5487-1.4633j,  1.9870-1.7309j],
        [ 1.1185+4.1834j,  0.0486+1.9648j,  1.9194+2.2038j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  1.1185+4.1834j,
          0.0486+1.9648j,  1.9194+2.2038j]], dtype=torch.complex128)

The max per-element difference (slow mode) is: 4.565870490719988.</failure>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_addmm_cpu_float64" time="0.024" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_addmm_decomposed_cpu_complex128" time="0.056" file="test_ops_gradients.py">
      <failure message="torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 2 with respect to input 1,&#10;numerical:tensor(-0.4064-0.3963j, dtype=torch.complex128)&#10;analytical:tensor(-0.3713+0.3920j, dtype=torch.complex128)&#10;&#10;The above quantities relating the numerical and analytical jacobians are computed &#10;in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background &#10;about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:&#10;&#10;Numerical:&#10; tensor([[ 0.6789-0.0252j, -0.7933-0.9017j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.6789-0.0252j,&#10;         -0.7933-0.9017j,  0.0000+0.0000j],&#10;        [-0.0544+0.8133j, -0.4459-0.3135j,  0.0000+0.0000j,  0.0000+0.0000j,&#10;          0.0000+0.0000j,  0.0000+0.0000j],&#10;        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j, -0.0544+0.8133j,&#10;         -0.4459-0.3135j,  0.0000+0.0000j]], dtype=torch.complex128)&#10;Analytical:&#10;tensor([[ 0.6789+0.0252j, -0.0544-0.8133j,  0.7051+0.1922j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.6789+0.0252j,&#10;         -0.0544-0.8133j,  0.7051+0.1922j],&#10;        [-0.7933+0.9017j, -0.4459+0.3135j, -0.2133+0.7820j,  0.0000-0.0000j,&#10;          0.0000-0.0000j,  0.0000-0.0000j],&#10;        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j, -0.7933+0.9017j,&#10;         -0.4459+0.3135j, -0.2133+0.7820j]], dtype=torch.complex128)&#10;&#10;The max per-element difference (slow mode) is: 0.8105540337898993.">Traceback (most recent call last):
  File "C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py", line 85, in test_inplace_gradgrad
    self._check_helper(device, dtype, op, self._get_safe_inplace(op.get_inplace()), "bwgrad_bwgrad")
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 4329, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args, **kwargs))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 3879, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1694, in gradgradcheck
    return gradcheck(
           ^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1536, in gradcheck
    return _gradcheck_helper(**args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1550, in _gradcheck_helper
    _gradcheck_real_imag(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1157, in _gradcheck_real_imag
    gradcheck_fn(imag_fn, imag_func_out, tupled_inputs, imag_outputs, eps,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1421, in _fast_gradcheck
    _check_analytical_numerical_equal(analytical_vJu, numerical_vJu, complex_indices,
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1389, in _check_analytical_numerical_equal
    raise GradcheckError(_get_notallclose_msg(a, n, j, i, complex_indices, test_imag, is_forward_ad) + jacobians_str)
torch.autograd.gradcheck.GradcheckError: While considering the imaginary part of complex outputs only, Jacobian mismatch for output 2 with respect to input 1,
numerical:tensor(-0.4064-0.3963j, dtype=torch.complex128)
analytical:tensor(-0.3713+0.3920j, dtype=torch.complex128)

The above quantities relating the numerical and analytical jacobians are computed 
in fast mode. See: https://github.com/pytorch/pytorch/issues/53876 for more background 
about fast mode. Below, we recompute numerical and analytical jacobians in slow mode:

Numerical:
 tensor([[ 0.6789-0.0252j, -0.7933-0.9017j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j,  0.6789-0.0252j,
         -0.7933-0.9017j,  0.0000+0.0000j],
        [-0.0544+0.8133j, -0.4459-0.3135j,  0.0000+0.0000j,  0.0000+0.0000j,
          0.0000+0.0000j,  0.0000+0.0000j],
        [ 0.0000+0.0000j,  0.0000+0.0000j,  0.0000+0.0000j, -0.0544+0.8133j,
         -0.4459-0.3135j,  0.0000+0.0000j]], dtype=torch.complex128)
Analytical:
tensor([[ 0.6789+0.0252j, -0.0544-0.8133j,  0.7051+0.1922j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j,  0.6789+0.0252j,
         -0.0544-0.8133j,  0.7051+0.1922j],
        [-0.7933+0.9017j, -0.4459+0.3135j, -0.2133+0.7820j,  0.0000-0.0000j,
          0.0000-0.0000j,  0.0000-0.0000j],
        [ 0.0000-0.0000j,  0.0000-0.0000j,  0.0000-0.0000j, -0.7933+0.9017j,
         -0.4459+0.3135j, -0.2133+0.7820j]], dtype=torch.complex128)

The max per-element difference (slow mode) is: 0.8105540337898993.</failure>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_addmm_decomposed_cpu_float64" time="0.029" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_addmv_cpu_complex128" time="0.270" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_addmv_cpu_float64" time="0.066" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_addr_cpu_complex128" time="0.044" file="test_ops_gradients.py">
      <failure message="torch.autograd.gradcheck.GradcheckError: For output 2 and input 1:&#10;&#10;gradcheck or gradgradcheck failed while testing batched gradient computation.&#10;This could have been invoked in a number of ways (via a test that calls&#10;gradcheck/gradgradcheck directly or via an autogenerated test).&#10;&#10;If you are adding a new operator, please file an issue and then use one of the&#10;workarounds. The workaround depends on how your test invokes gradcheck/gradgradcheck.&#10;If the test&#10;- manually invokes gradcheck/gradgradcheck, then call gradcheck/gradgradcheck&#10;  with `check_batched_grad=False` as a keyword argument.&#10;- is OpInfo-based (e.g., in test_ops_gradients.py), then modify the OpInfo for the test&#10;  to have `check_batched_grad=False` and/or `check_batched_gradgrad=False`.&#10;&#10;If you're modifying an existing operator that supports batched grad computation,&#10;or wish to make a new operator work with batched grad computation, please read&#10;the following.&#10;&#10;To compute batched grads (e.g., jacobians, hessians), we vmap over the backward&#10;computation. The most common failure case is if there is a 'vmap-incompatible&#10;operation' in the backward pass. Please see&#10;NOTE: [How to write vmap-compatible backward formulas]&#10;in the codebase for an explanation of how to fix this.&#10;&#10;Got:&#10;tensor([[ 3.7784e+242+1.5628e+243j, -5.2301e+242+1.6095e+242j,&#10;         -6.3719e+241-1.2342e+243j,  1.6659e+243+1.3393e+242j,&#10;         -2.7914e+242+2.6969e+242j],&#10;        [ 3.3654e+227+1.5045e+228j, -2.3345e+227+8.9260e+227j,&#10;          9.3548e+227+3.4202e+228j, -5.7729e+227+1.2897e+228j,&#10;          2.8531e+228+2.3249e+228j]], dtype=torch.complex128)&#10;&#10;Expected:&#10;tensor([[ 1.0842+0.5543j, -0.3350+1.6438j,  0.3208+0.4515j, -0.1550+1.4121j,&#10;         -1.4954-0.8838j],&#10;        [-0.8455+0.6540j,  1.2100+1.5291j,  0.3054-0.6695j, -3.1500-0.7513j,&#10;         -1.5255+1.3565j]], dtype=torch.complex128)">Traceback (most recent call last):
  File "C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py", line 85, in test_inplace_gradgrad
    self._check_helper(device, dtype, op, self._get_safe_inplace(op.get_inplace()), "bwgrad_bwgrad")
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 4329, in _check_helper
    self.assertTrue(gradgradcheck(fn, gradcheck_args, **kwargs))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\testing\_internal\common_utils.py", line 3879, in gradgradcheck
    return torch.autograd.gradgradcheck(fn, inputs, grad_outputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1694, in gradgradcheck
    return gradcheck(
           ^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1536, in gradcheck
    return _gradcheck_helper(**args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 1564, in _gradcheck_helper
    _test_batched_grad(tupled_inputs, o, i)
  File "c:\users\radekbarton\projects\pytorch\torch\autograd\gradcheck.py", line 939, in _test_batched_grad
    raise GradcheckError(_get_failed_batched_grad_test_msg(output_idx, input_idx, res, exp))
torch.autograd.gradcheck.GradcheckError: For output 2 and input 1:

gradcheck or gradgradcheck failed while testing batched gradient computation.
This could have been invoked in a number of ways (via a test that calls
gradcheck/gradgradcheck directly or via an autogenerated test).

If you are adding a new operator, please file an issue and then use one of the
workarounds. The workaround depends on how your test invokes gradcheck/gradgradcheck.
If the test
- manually invokes gradcheck/gradgradcheck, then call gradcheck/gradgradcheck
  with `check_batched_grad=False` as a keyword argument.
- is OpInfo-based (e.g., in test_ops_gradients.py), then modify the OpInfo for the test
  to have `check_batched_grad=False` and/or `check_batched_gradgrad=False`.

If you're modifying an existing operator that supports batched grad computation,
or wish to make a new operator work with batched grad computation, please read
the following.

To compute batched grads (e.g., jacobians, hessians), we vmap over the backward
computation. The most common failure case is if there is a 'vmap-incompatible
operation' in the backward pass. Please see
NOTE: [How to write vmap-compatible backward formulas]
in the codebase for an explanation of how to fix this.

Got:
tensor([[ 3.7784e+242+1.5628e+243j, -5.2301e+242+1.6095e+242j,
         -6.3719e+241-1.2342e+243j,  1.6659e+243+1.3393e+242j,
         -2.7914e+242+2.6969e+242j],
        [ 3.3654e+227+1.5045e+228j, -2.3345e+227+8.9260e+227j,
          9.3548e+227+3.4202e+228j, -5.7729e+227+1.2897e+228j,
          2.8531e+228+2.3249e+228j]], dtype=torch.complex128)

Expected:
tensor([[ 1.0842+0.5543j, -0.3350+1.6438j,  0.3208+0.4515j, -0.1550+1.4121j,
         -1.4954-0.8838j],
        [-0.8455+0.6540j,  1.2100+1.5291j,  0.3054-0.6695j, -3.1500-0.7513j,
         -1.5255+1.3565j]], dtype=torch.complex128)</failure>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_addr_cpu_float64" time="0.091" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_all_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_all_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_allclose_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_allclose_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_amax_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_amin_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_aminmax_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_angle_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_angle_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_any_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_any_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_arange_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_argmax_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_argmin_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_argsort_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_argwhere_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_argwhere_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_as_strided_cpu_complex128" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Numerous errors">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Numerous errors</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_as_strided_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Numerous errors">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Numerous errors</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_as_strided_partial_views_cpu_complex128" time="0.044" file="test_ops_gradients.py">
      <skipped type="pytest.xfail" message="" />
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_as_strided_partial_views_cpu_float64" time="0.025" file="test_ops_gradients.py">
      <skipped type="pytest.xfail" message="" />
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_as_strided_scatter_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_as_strided_scatter_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_asin_cpu_complex128" time="0.057" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_asin_cpu_float64" time="0.009" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_asinh_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_asinh_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_atan2_cpu_float64" time="0.087" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_atan_cpu_complex128" time="0.047" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_atan_cpu_float64" time="0.009" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_atanh_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_atanh_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_atleast_1d_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_atleast_1d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_atleast_2d_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_atleast_2d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_atleast_3d_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_atleast_3d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_baddbmm_cpu_complex128" time="1.183" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_baddbmm_cpu_float64" time="0.050" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_bernoulli_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_bfloat16_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_bfloat16_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_block_diag_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_block_diag_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_bmm_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_bmm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_bool_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_bool_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_broadcast_tensors_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_broadcast_tensors_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_broadcast_to_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_broadcast_to_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_bucketize_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_byte_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_byte_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_cartesian_prod_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_cartesian_prod_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_cat_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="TODO(whc) fix pre-existing bug with cat for newly added opinfo for empty+nonempty">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: TODO(whc) fix pre-existing bug with cat for newly added opinfo for empty+nonempty</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_cat_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="TODO(whc) fix pre-existing bug with cat for newly added opinfo for empty+nonempty">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: TODO(whc) fix pre-existing bug with cat for newly added opinfo for empty+nonempty</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_cauchy_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_cdist_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_cdouble_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_cdouble_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_ceil_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_cfloat_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_cfloat_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_chalf_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_chalf_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_char_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_char_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_cholesky_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_cholesky_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_cholesky_inverse_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_cholesky_inverse_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_cholesky_solve_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_cholesky_solve_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_chunk_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_chunk_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_clamp_cpu_float64" time="0.137" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_clamp_max_cpu_float64" time="0.078" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_clamp_min_cpu_float64" time="0.071" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_clone_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_clone_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_column_stack_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_column_stack_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_combinations_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_combinations_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_complex_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_conj_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_conj_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_conj_physical_cpu_complex128" time="0.033" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_conj_physical_cpu_float64" time="0.007" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_constant_pad_nd_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_constant_pad_nd_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_contiguous_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_contiguous_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_copysign_cpu_float64" time="0.049" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_corrcoef_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_corrcoef_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_cos_cpu_complex128" time="0.103" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_cos_cpu_float64" time="0.019" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_cosh_cpu_complex128" time="0.099" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_cosh_cpu_float64" time="0.027" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_count_nonzero_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_count_nonzero_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_cov_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_cov_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_cross_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_cross_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_cummax_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_cummin_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_cumprod_cpu_complex128" time="69.258" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_cumprod_cpu_float64" time="11.397" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_cumsum_cpu_complex128" time="0.170" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_cumsum_cpu_float64" time="0.053" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_cumulative_trapezoid_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_cumulative_trapezoid_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_deg2rad_cpu_float64" time="0.010" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_diag_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_diag_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_diag_embed_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_diag_embed_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_diagflat_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_diagflat_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_diagonal_copy_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_diagonal_copy_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_diagonal_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_diagonal_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_diagonal_scatter_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_diagonal_scatter_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_diff_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_diff_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_digamma_cpu_float64" time="0.020" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_dist_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_dist_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_div_floor_rounding_cpu_float64" time="0.045" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_div_no_rounding_mode_cpu_complex128" time="0.536" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_div_no_rounding_mode_cpu_float64" time="0.081" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_div_trunc_rounding_cpu_float64" time="0.044" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_dot_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_dot_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_double_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_double_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_dsplit_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_dsplit_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_dstack_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_dstack_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_einsum_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_einsum_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_empty_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_empty_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_empty_like_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_empty_like_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_empty_permuted_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_empty_permuted_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_eq_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_eq_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_equal_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_equal_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_erf_cpu_float64" time="0.010" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_erfc_cpu_float64" time="0.022" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_erfinv_cpu_float64" time="0.011" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_exp2_cpu_complex128" time="0.107" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_exp2_cpu_float64" time="0.019" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_exp_cpu_complex128" time="0.100" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_exp_cpu_float64" time="0.020" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_expand_as_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_expand_as_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_expand_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_expand_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_expm1_cpu_float64" time="0.016" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_exponential_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_eye_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_eye_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fft_fft2_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fft_fft2_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fft_fft_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fft_fft_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fft_fftn_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fft_fftn_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fft_fftshift_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fft_fftshift_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fft_hfft2_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fft_hfft2_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fft_hfft_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fft_hfft_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fft_hfftn_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fft_hfftn_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fft_ifft2_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fft_ifft2_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fft_ifft_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fft_ifft_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fft_ifftn_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fft_ifftn_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fft_ifftshift_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fft_ifftshift_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fft_ihfft2_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fft_ihfft_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fft_ihfftn_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fft_irfft2_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fft_irfft2_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fft_irfft_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fft_irfft_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fft_irfftn_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fft_irfftn_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fft_rfft2_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fft_rfft_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fft_rfftn_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fill_cpu_complex128" time="0.098" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fill_cpu_float64" time="0.013" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_flatten_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_flatten_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_flip_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_flip_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fliplr_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fliplr_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_flipud_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_flipud_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_float_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_float_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_float_power_cpu_complex128" time="0.636" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_float_power_cpu_float64" time="0.108" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_floor_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_floor_divide_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fmax_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fmin_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_fmod_cpu_float64" time="0.160" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_frac_cpu_float64" time="0.054" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_frexp_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_full_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_full_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_full_like_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_full_like_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_gather_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_gather_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_ge_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_geometric_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_geqrf_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_geqrf_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_gradient_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_gradient_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_grid_sampler_2d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_gt_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_half_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_half_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_heaviside_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_histc_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_histogram_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_histogramdd_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_hsplit_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_hsplit_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_hstack_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_hstack_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_hypot_cpu_float64" time="0.084" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_i0_cpu_float64" time="0.023" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_igamma_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_igammac_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_imag_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_index_add_cpu_complex128" time="0.645" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_index_add_cpu_float64" time="0.122" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_index_copy_cpu_complex128" time="0.212" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_index_copy_cpu_float64" time="0.055" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_index_fill_cpu_complex128" time="0.312" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_index_fill_cpu_float64" time="0.059" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_index_put_cpu_complex128" time="0.308" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_index_put_cpu_float64" time="0.057" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_index_reduce_cpu_float64" time="0.822" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_index_select_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_index_select_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_inner_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_inner_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_int_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_int_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_isclose_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_isclose_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_isfinite_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_isfinite_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_isin_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_isinf_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_isinf_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_isnan_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_isnan_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_isneginf_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_isposinf_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_isreal_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_isreal_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_istft_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_jiterator_2inputs_2outputs_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_jiterator_2inputs_2outputs_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_jiterator_4inputs_with_extra_args_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_jiterator_4inputs_with_extra_args_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_jiterator_binary_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_jiterator_binary_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_jiterator_binary_return_by_ref_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_jiterator_binary_return_by_ref_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_jiterator_unary_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_jiterator_unary_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Only runs on cuda">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Only runs on cuda</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_kron_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_kron_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_kthvalue_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_ldexp_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_ldexp_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_le_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_lerp_cpu_complex128" time="1.383" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_lerp_cpu_float64" time="0.125" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_lgamma_cpu_float64" time="0.020" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_cholesky_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_cholesky_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_cholesky_ex_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_cholesky_ex_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_cond_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_cond_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_cross_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_cross_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_det_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_det_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_det_singular_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_det_singular_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_diagonal_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_diagonal_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_eig_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_eig_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_eigh_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_eigh_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_eigvals_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_eigvals_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_eigvalsh_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_eigvalsh_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_householder_product_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_householder_product_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_inv_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_inv_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_inv_ex_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_inv_ex_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_ldl_factor_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_ldl_factor_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_ldl_factor_ex_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_ldl_factor_ex_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_ldl_solve_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_ldl_solve_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_lstsq_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_lstsq_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_lstsq_grad_oriented_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_lstsq_grad_oriented_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_lu_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_lu_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_lu_factor_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_lu_factor_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_lu_factor_ex_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_lu_factor_ex_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_lu_solve_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_lu_solve_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_matrix_norm_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_matrix_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_matrix_power_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_matrix_power_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_matrix_rank_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_matrix_rank_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_matrix_rank_hermitian_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_matrix_rank_hermitian_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_multi_dot_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_multi_dot_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_norm_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_norm_subgradients_at_zero_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_norm_subgradients_at_zero_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_pinv_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_pinv_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_pinv_hermitian_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_pinv_hermitian_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_pinv_singular_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_pinv_singular_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_qr_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_qr_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_slogdet_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_slogdet_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_solve_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_solve_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_solve_ex_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_solve_ex_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_solve_triangular_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_solve_triangular_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_svd_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_svd_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_svdvals_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_svdvals_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_tensorinv_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_tensorinv_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_tensorsolve_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_tensorsolve_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_vander_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_vander_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_vecdot_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_vecdot_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_vector_norm_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linalg_vector_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linspace_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_linspace_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_log10_cpu_complex128" time="0.162" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_log10_cpu_float64" time="0.020" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_log1p_cpu_complex128" time="0.046" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_log1p_cpu_float64" time="0.009" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_log2_cpu_complex128" time="0.108" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_log2_cpu_float64" time="0.020" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_log_cpu_complex128" time="0.103" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_log_cpu_float64" time="0.019" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_log_normal_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_log_softmax_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_log_softmax_with_dtype_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_log_softmax_with_dtype_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_logaddexp2_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_logaddexp_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_logaddexp_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_logcumsumexp_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_logcumsumexp_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_logdet_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_logdet_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_logical_and_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_logical_and_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_logical_not_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_logical_not_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_logical_or_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_logical_or_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_logical_xor_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_logical_xor_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_logit_cpu_float64" time="0.032" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_logspace_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_logspace_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_logsumexp_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_long_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_long_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_lt_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_lu_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_lu_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_lu_solve_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_lu_solve_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_lu_unpack_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_lu_unpack_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_mH_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_mH_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_mT_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_mT_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_masked_amax_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_masked_amin_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_masked_argmax_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_masked_argmin_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_masked_cumprod_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_masked_cumprod_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_masked_cumsum_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_masked_cumsum_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_masked_fill_cpu_complex128" time="0.307" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_masked_fill_cpu_float64" time="0.053" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_masked_log_softmax_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_masked_logaddexp_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_masked_logsumexp_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_masked_mean_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_masked_mean_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_masked_median_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_masked_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_masked_normalize_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_masked_normalize_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_masked_prod_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_masked_prod_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_masked_scatter_cpu_complex128" time="0.361" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_masked_scatter_cpu_float64" time="0.046" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_masked_select_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_masked_select_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_masked_softmax_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_masked_softmin_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_masked_std_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_masked_std_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_masked_sum_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_masked_sum_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_masked_var_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_masked_var_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_matmul_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_matmul_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_matrix_exp_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_matrix_exp_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_max_binary_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_max_pool2d_with_indices_backward_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_max_reduction_no_dim_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_max_reduction_with_dim_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_maximum_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_mean_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_mean_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_median_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_meshgrid_list_of_tensors_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_meshgrid_list_of_tensors_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_meshgrid_variadic_tensors_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_meshgrid_variadic_tensors_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_min_binary_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_min_reduction_no_dim_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_min_reduction_with_dim_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_minimum_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_mm_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_mm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_mode_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_movedim_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_movedim_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_msort_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_mul_cpu_complex128" time="0.407" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_mul_cpu_float64" time="0.076" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_multinomial_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_mv_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_mv_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_mvlgamma_mvlgamma_p_1_cpu_float64" time="0.083" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_mvlgamma_mvlgamma_p_3_cpu_float64" time="0.083" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_mvlgamma_mvlgamma_p_5_cpu_float64" time="0.082" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nan_to_num_cpu_float64" time="0.019" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nanmean_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nanmedian_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nanquantile_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nansum_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_narrow_copy_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_narrow_copy_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_narrow_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_narrow_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_native_batch_norm_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_native_dropout_backward_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_native_layer_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_ne_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_ne_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_neg_cpu_complex128" time="0.043" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_neg_cpu_float64" time="0.007" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_new_empty_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_new_empty_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_new_empty_strided_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_new_empty_strided_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_new_full_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_new_full_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_new_ones_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_new_ones_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_new_zeros_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_new_zeros_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nextafter_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_adaptive_avg_pool1d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_adaptive_avg_pool2d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_adaptive_avg_pool3d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_adaptive_max_pool1d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_adaptive_max_pool2d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_adaptive_max_pool3d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_alpha_dropout_cpu_float64" time="0.558" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_avg_pool1d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_avg_pool2d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_avg_pool3d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_batch_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_bilinear_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_binary_cross_entropy_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_binary_cross_entropy_with_logits_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_celu_cpu_float64" time="0.021" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_conv1d_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_conv1d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_conv2d_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_conv2d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_conv_transpose1d_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_conv_transpose1d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_conv_transpose2d_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_conv_transpose2d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_conv_transpose3d_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_conv_transpose3d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_cosine_embedding_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_cosine_similarity_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_cross_entropy_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_ctc_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_dropout2d_cpu_float64" time="0.287" file="test_ops_gradients.py">
      <system-err>c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
c:\users\radekbarton\projects\pytorch\torch\nn\functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.
  warnings.warn("dropout2d: Received a 3D input to dropout2d and assuming that channel-wise "
      </system-err>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_dropout3d_cpu_float64" time="0.286" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_dropout_cpu_float64" time="0.371" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_elu_cpu_float64" time="0.021" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_embedding_bag_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_embedding_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_feature_alpha_dropout_with_train_cpu_float64" time="0.144" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_feature_alpha_dropout_without_train_cpu_complex128" time="1.688" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_feature_alpha_dropout_without_train_cpu_float64" time="0.339" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_fractional_max_pool2d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_fractional_max_pool3d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_gaussian_nll_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_gelu_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_glu_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_grid_sample_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_group_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_hardshrink_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_hardsigmoid_cpu_float64" time="0.003" file="test_ops_gradients.py">
      <skipped type="pytest.xfail" message="" />
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_hardswish_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_hardtanh_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_hinge_embedding_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_huber_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_instance_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_interpolate_area_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_interpolate_bicubic_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_interpolate_bilinear_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_interpolate_linear_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_interpolate_nearest_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_interpolate_trilinear_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_kl_div_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_l1_loss_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_l1_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_layer_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_leaky_relu_cpu_float64" time="0.061" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_linear_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_linear_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_local_response_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_logsigmoid_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_margin_ranking_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_max_pool1d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_max_pool2d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_max_pool3d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_max_unpool1d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_max_unpool1d_grad_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_max_unpool2d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_max_unpool2d_grad_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_max_unpool3d_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_max_unpool3d_grad_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_mish_cpu_float64" time="0.027" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_mse_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_multi_margin_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_multilabel_margin_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_multilabel_soft_margin_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_nll_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_normalize_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_normalize_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_pad_circular_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_pad_circular_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_pad_constant_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_pad_constant_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_pad_reflect_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_pad_reflect_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_pad_replicate_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_pad_replicate_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_pairwise_distance_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_pairwise_distance_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_pdist_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_pixel_shuffle_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_pixel_shuffle_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_pixel_unshuffle_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_pixel_unshuffle_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_poisson_nll_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_prelu_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_relu6_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_relu_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_rrelu_cpu_float64" time="0.091" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_scaled_dot_product_attention_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_selu_cpu_float64" time="0.021" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_silu_complex_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_silu_cpu_float64" time="0.036" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_smooth_l1_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_soft_margin_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_softmin_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_softmin_with_dtype_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_softmin_with_dtype_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_softplus_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_softshrink_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_softsign_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_softsign_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_tanhshrink_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_tanhshrink_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_threshold_cpu_float64" time="0.027" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_triplet_margin_loss_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_triplet_margin_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_triplet_margin_with_distance_loss_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_triplet_margin_with_distance_loss_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_unfold_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_unfold_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_upsample_bilinear_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nn_functional_upsample_nearest_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nonzero_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_nonzero_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_norm_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_norm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_norm_fro_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_norm_fro_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_norm_inf_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_norm_inf_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_norm_nuc_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_norm_nuc_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_normal_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Gradients are incorrect!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Gradients are incorrect!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_normal_in_place_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_normal_in_place_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_normal_number_mean_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Gradients are incorrect!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Gradients are incorrect!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_ones_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_ones_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_ones_like_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_ones_like_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_ormqr_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_ormqr_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_outer_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_outer_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_pca_lowrank_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_permute_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_permute_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_pinverse_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_pinverse_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_polar_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_polygamma_polygamma_n_0_cpu_float64" time="0.081" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_polygamma_polygamma_n_1_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_polygamma_polygamma_n_2_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_polygamma_polygamma_n_3_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_polygamma_polygamma_n_4_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_positive_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_positive_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_pow_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_pow_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_prod_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_prod_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_put_cpu_complex128" time="1.367" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_put_cpu_float64" time="0.245" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_qr_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_qr_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_quantile_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_rad2deg_cpu_float64" time="0.016" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_rand_like_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_rand_like_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_randint_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_randint_like_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_randn_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_randn_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_randn_like_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_randn_like_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_ravel_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_ravel_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_real_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_real_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_reciprocal_cpu_complex128" time="0.114" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_reciprocal_cpu_float64" time="0.021" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_remainder_cpu_float64" time="0.077" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_renorm_cpu_complex128" time="0.289" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_renorm_cpu_float64" time="0.080" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_repeat_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_repeat_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_repeat_interleave_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_repeat_interleave_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_reshape_as_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_reshape_as_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_reshape_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_reshape_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_resize__cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_resize__cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_resize_as__cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_resize_as__cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_resolve_conj_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_resolve_conj_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_resolve_neg_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_resolve_neg_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_roll_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_roll_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_rot90_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_rot90_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_round_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_round_decimals_0_cpu_float64" time="0.014" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_round_decimals_3_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_round_decimals_neg_3_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped!">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped!</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_rsqrt_cpu_complex128" time="0.114" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_rsqrt_cpu_float64" time="0.021" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_rsub_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_rsub_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_scalar_tensor_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_scalar_tensor_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_scatter_add_cpu_complex128" time="0.509" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_scatter_add_cpu_float64" time="0.088" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_scatter_cpu_complex128" time="0.563" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_scatter_cpu_float64" time="0.099" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_scatter_reduce_amax_cpu_float64" time="0.423" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_scatter_reduce_amin_cpu_float64" time="0.337" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_scatter_reduce_mean_cpu_float64" time="0.392" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_scatter_reduce_prod_cpu_float64" time="0.676" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_scatter_reduce_sum_cpu_float64" time="0.443" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_searchsorted_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_select_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_select_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_select_scatter_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_sgn_cpu_complex128" time="0.097" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_sgn_cpu_float64" time="0.010" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_short_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_short_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_sigmoid_cpu_complex128" time="0.159" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_sigmoid_cpu_float64" time="0.091" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_sign_cpu_float64" time="0.026" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_signal_windows_bartlett_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_signal_windows_blackman_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_signal_windows_cosine_cpu_float64" time="0.003" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_signal_windows_exponential_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_signal_windows_gaussian_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_signal_windows_general_cosine_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_signal_windows_general_hamming_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_signal_windows_hamming_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_signal_windows_hann_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_signal_windows_kaiser_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_signal_windows_nuttall_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_signbit_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_sin_cpu_complex128" time="0.054" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_sin_cpu_float64" time="0.009" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_sinc_cpu_complex128" time="0.149" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_sinc_cpu_float64" time="0.026" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_sinh_cpu_complex128" time="0.053" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_sinh_cpu_float64" time="0.009" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_slice_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_slice_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_slice_scatter_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_softmax_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_softmax_with_dtype_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_softmax_with_dtype_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_sort_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_sparse_mm_reduce_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_sparse_sampled_addmm_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch is built without MKL support">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch is built without MKL support</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_sparse_sampled_addmm_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch is built without MKL support">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch is built without MKL support</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_special_airy_ai_cpu_float64" time="0.006" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_special_bessel_j0_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_special_bessel_j1_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_special_bessel_y0_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_special_bessel_y1_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_special_chebyshev_polynomial_t_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_special_chebyshev_polynomial_u_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_special_chebyshev_polynomial_v_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipping - testing takes an unreasonably long time, #79528">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipping - testing takes an unreasonably long time, #79528</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_special_chebyshev_polynomial_w_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipping - testing takes an unreasonably long time, #79528">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipping - testing takes an unreasonably long time, #79528</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_special_entr_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_special_erfcx_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_special_hermite_polynomial_h_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_special_hermite_polynomial_he_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_special_i0e_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_special_i1_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_special_i1e_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_special_laguerre_polynomial_l_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_special_legendre_polynomial_p_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipping - testing takes an unreasonably long time, #79528">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipping - testing takes an unreasonably long time, #79528</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_special_log_ndtr_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_special_modified_bessel_i0_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_special_modified_bessel_i1_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_special_modified_bessel_k0_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_special_modified_bessel_k1_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_special_ndtr_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_special_ndtri_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_special_polygamma_special_polygamma_n_0_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_special_scaled_modified_bessel_k0_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_special_scaled_modified_bessel_k1_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_special_shifted_chebyshev_polynomial_t_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipping - testing takes an unreasonably long time, #79528">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipping - testing takes an unreasonably long time, #79528</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_special_shifted_chebyshev_polynomial_u_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipping - testing takes an unreasonably long time, #79528">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipping - testing takes an unreasonably long time, #79528</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_special_shifted_chebyshev_polynomial_v_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipping - testing takes an unreasonably long time, #79528">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipping - testing takes an unreasonably long time, #79528</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_special_shifted_chebyshev_polynomial_w_cpu_float64" time="0.000" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipping - testing takes an unreasonably long time, #79528">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipping - testing takes an unreasonably long time, #79528</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_special_spherical_bessel_j0_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_special_xlog1py_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_special_zeta_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_split_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_split_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_split_list_args_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_split_list_args_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_split_with_sizes_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_split_with_sizes_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_sqrt_cpu_complex128" time="0.048" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_sqrt_cpu_float64" time="0.026" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_square_cpu_complex128" time="0.141" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_square_cpu_float64" time="0.021" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_squeeze_cpu_complex128" time="0.246" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_squeeze_cpu_float64" time="0.045" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_squeeze_multiple_cpu_complex128" time="0.201" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_squeeze_multiple_cpu_float64" time="0.044" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_stack_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_stack_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_std_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_std_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_std_mean_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_std_mean_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_std_mean_unbiased_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_std_mean_unbiased_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_std_unbiased_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_std_unbiased_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_stft_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_stft_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_sub_cpu_complex128" time="0.571" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_sub_cpu_float64" time="0.100" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_sum_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_sum_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_sum_to_size_cpu_complex128" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_sum_to_size_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_svd_cpu_complex128" time="0.003" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_svd_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_svd_lowrank_cpu_float64" time="0.002" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_t_cpu_complex128" time="0.093" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_t_cpu_float64" time="0.017" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_take_along_dim_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_take_along_dim_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_take_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_take_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_tan_cpu_complex128" time="0.051" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_tan_cpu_float64" time="0.010" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_tanh_cpu_complex128" time="0.044" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_tanh_cpu_float64" time="0.009" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_tensor_split_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_tensor_split_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_tensordot_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_tensordot_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_tile_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_tile_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_to_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_to_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_to_sparse_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_to_sparse_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_topk_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_trace_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_trace_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_transpose_cpu_complex128" time="0.278" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_transpose_cpu_float64" time="0.046" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_trapezoid_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_trapezoid_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_trapz_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_trapz_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_triangular_solve_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_triangular_solve_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="PyTorch compiled without Lapack">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: PyTorch compiled without Lapack</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_tril_cpu_complex128" time="0.526" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_tril_cpu_float64" time="0.077" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_triu_cpu_complex128" time="0.505" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_triu_cpu_float64" time="0.057" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_true_divide_cpu_complex128" time="0.495" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_true_divide_cpu_float64" time="0.083" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_trunc_cpu_float64" time="0.006" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_unbind_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_unbind_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_unflatten_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_unflatten_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_unfold_copy_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_unfold_copy_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_unfold_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_unfold_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_uniform_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_uniform_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_unique_consecutive_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_unique_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_unsafe_split_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_unsafe_split_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_unsqueeze_cpu_complex128" time="0.293" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_unsqueeze_cpu_float64" time="0.072" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_var_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_var_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_var_mean_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_var_mean_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_var_mean_unbiased_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_var_mean_unbiased_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_var_unbiased_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_var_unbiased_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_vdot_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_vdot_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_view_as_complex_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_view_as_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_view_as_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_view_as_real_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_view_copy_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_view_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_view_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_vsplit_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_vsplit_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_vstack_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_vstack_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_where_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_where_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Operation does not support inplace autograd.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Operation does not support inplace autograd.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_xlogy_cpu_float64" time="0.086" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_zero__cpu_complex128" time="0.098" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_zero__cpu_float64" time="0.015" file="test_ops_gradients.py" />
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_zeros_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_zeros_cpu_float64" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_zeros_like_cpu_complex128" time="0.001" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
    <testcase classname="TestBwdGradientsCPU" name="test_inplace_gradgrad_zeros_like_cpu_float64" time="0.053" file="test_ops_gradients.py">
      <skipped type="pytest.skip" message="Skipped! Op doesn't support autograd for this dtype.">C:\Users\radekbarton\Projects\pytorch\test\test_ops_gradients.py:80: Skipped! Op doesn't support autograd for this dtype.</skipped>
    </testcase>
  </testsuite>
</testsuites>